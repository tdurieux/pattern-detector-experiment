diff --git a/doc/developers/utilities.rst b/doc/developers/utilities.rst
index 9ef9f6cd3a..8dbe460635 100755
--- a/doc/developers/utilities.rst
+++ b/doc/developers/utilities.rst
@@ -188,11 +188,6 @@ Backports
   Used in ``sklearn.cluster.hierarchical``, as well as in tests for
   :mod:`sklearn.feature_extraction`.
 
-- :func:`fixes.isclose`
-  (backported from ``numpy.isclose`` in numpy 1.8.1).
-  In versions before 1.7, this function was not available in
-  numpy. Used in ``sklearn.metrics``.
-
 
 ARPACK
 ------
diff --git a/doc/modules/classes.rst b/doc/modules/classes.rst
index d1d8eec4dd..cc2ebd6dfa 100755
--- a/doc/modules/classes.rst
+++ b/doc/modules/classes.rst
@@ -164,14 +164,14 @@ Splitter Classes
    :template: class.rst
 
    model_selection.KFold
-   model_selection.LabelKFold
+   model_selection.GroupKFold
    model_selection.StratifiedKFold
-   model_selection.LeaveOneLabelOut
-   model_selection.LeavePLabelOut
+   model_selection.LeaveOneGroupOut
+   model_selection.LeavePGroupsOut
    model_selection.LeaveOneOut
    model_selection.LeavePOut
    model_selection.ShuffleSplit
-   model_selection.LabelShuffleSplit
+   model_selection.GroupShuffleSplit
    model_selection.StratifiedShuffleSplit
    model_selection.PredefinedSplit
    model_selection.TimeSeriesSplit
diff --git a/doc/modules/cross_validation.rst b/doc/modules/cross_validation.rst
index 872844cd0c..cb81b2d340 100755
--- a/doc/modules/cross_validation.rst
+++ b/doc/modules/cross_validation.rst
@@ -1,3 +1,4 @@
+
 .. _cross_validation:
 
 ===================================================
@@ -209,6 +210,28 @@ The following sections list utilities to generate indices
 that can be used to generate dataset splits according to different cross
 validation strategies.
 
+.. _iid_cv
+
+Cross-validation iterators for i.i.d. data
+==========================================
+
+Assuming that some data is Independent Identically Distributed (i.i.d.) is
+making the assumption that all samples stem from the same generative process
+and that the generative process is assumed to have no memory of past generated
+samples.
+
+The following cross-validators can be used in such cases.
+
+**NOTE**
+
+While i.i.d. data is a common assumption in machine learning theory, it rarely
+holds in practice. If one knows that the samples have been generated using a
+time-dependent process, it's safer to
+use a `time-series aware cross-validation scheme <time_series_cv>`
+Similarly if we know that the generative process has a group structure
+(samples from collected from different subjects, experiments, measurement
+devices) it safer to use `group-wise cross-validation <group_cv>`.
+
 
 K-fold
 ------
@@ -239,58 +262,7 @@ Thus, one can create the training/test sets using numpy indexing::
   >>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]
 
 
-Stratified k-fold
------------------
-
-:class:`StratifiedKFold` is a variation of *k-fold* which returns *stratified*
-folds: each set contains approximately the same percentage of samples of each
-target class as the complete set.
-
-Example of stratified 3-fold cross-validation on a dataset with 10 samples from
-two slightly unbalanced classes::
-
-  >>> from sklearn.model_selection import StratifiedKFold
-
-  >>> X = np.ones(10)
-  >>> y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
-  >>> skf = StratifiedKFold(n_splits=3)
-  >>> for train, test in skf.split(X, y):
-  ...     print("%s %s" % (train, test))
-  [2 3 6 7 8 9] [0 1 4 5]
-  [0 1 3 4 5 8 9] [2 6 7]
-  [0 1 2 4 5 6 7] [3 8 9]
-
-
-Label k-fold
-------------
-
-:class:`LabelKFold` is a variation of *k-fold* which ensures that the same
-label is not in both testing and training sets. This is necessary for example
-if you obtained data from different subjects and you want to avoid over-fitting
-(i.e., learning person specific features) by testing and training on different
-subjects.
-
-Imagine you have three subjects, each with an associated number from 1 to 3::
-
-  >>> from sklearn.model_selection import LabelKFold
-
-  >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]
-  >>> y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
-  >>> labels = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]
-
-  >>> lkf = LabelKFold(n_splits=3)
-  >>> for train, test in lkf.split(X, y, labels):
-  ...     print("%s %s" % (train, test))
-  [0 1 2 3 4 5] [6 7 8 9]
-  [0 1 2 6 7 8 9] [3 4 5]
-  [3 4 5 6 7 8 9] [0 1 2]
-
-Each subject is in a different testing fold, and the same subject is never in
-both testing and training. Notice that the folds do not have exactly the same
-size due to the imbalance in the data.
-
-
-Leave-One-Out - LOO
+Leave One Out (LOO)
 -------------------
 
 :class:`LeaveOneOut` (or LOO) is a simple cross-validation. Each learning
@@ -348,7 +320,7 @@ fold cross validation should be preferred to LOO.
    Statistical Learning <http://www-bcf.usc.edu/~gareth/ISL>`_, Springer 2013.
 
 
-Leave-P-Out - LPO
+Leave P Out (LPO)
 -----------------
 
 :class:`LeavePOut` is very similar to :class:`LeaveOneOut` as it creates all
@@ -373,68 +345,6 @@ Example of Leave-2-Out on a dataset with 4 samples::
   [0 1] [2 3]
 
 
-Leave-One-Label-Out - LOLO
---------------------------
-
-:class:`LeaveOneLabelOut` (LOLO) is a cross-validation scheme which holds out
-the samples according to a third-party provided array of integer labels. This
-label information can be used to encode arbitrary domain specific pre-defined
-cross-validation folds.
-
-Each training set is thus constituted by all the samples except the ones
-related to a specific label.
-
-For example, in the cases of multiple experiments, LOLO can be used to
-create a cross-validation based on the different experiments: we create
-a training set using the samples of all the experiments except one::
-
-  >>> from sklearn.model_selection import LeaveOneLabelOut
-
-  >>> X = [1, 5, 10, 50]
-  >>> y = [0, 1, 1, 2]
-  >>> labels = [1, 1, 2, 2]
-  >>> lolo = LeaveOneLabelOut()
-  >>> for train, test in lolo.split(X, y, labels):
-  ...     print("%s %s" % (train, test))
-  [2 3] [0 1]
-  [0 1] [2 3]
-
-Another common application is to use time information: for instance the
-labels could be the year of collection of the samples and thus allow
-for cross-validation against time-based splits.
-
-.. warning::
-
-  Contrary to :class:`StratifiedKFold`,
-  the ``labels`` of :class:`LeaveOneLabelOut` should not encode
-  the target class to predict: the goal of :class:`StratifiedKFold`
-  is to rebalance dataset classes across
-  the train / test split to ensure that the train and test folds have
-  approximately the same percentage of samples of each class while
-  :class:`LeaveOneLabelOut` will do the opposite by ensuring that the samples
-  of the train and test fold will not share the same label value.
-
-
-Leave-P-Label-Out
------------------
-
-:class:`LeavePLabelOut` is similar as *Leave-One-Label-Out*, but removes
-samples related to :math:`P` labels for each training/test set.
-
-Example of Leave-2-Label Out::
-
-  >>> from sklearn.model_selection import LeavePLabelOut
-
-  >>> X = np.arange(6)
-  >>> y = [1, 1, 1, 2, 2, 2]
-  >>> labels = [1, 1, 2, 2, 3, 3]
-  >>> lplo = LeavePLabelOut(n_labels=2)
-  >>> for train, test in lplo.split(X, y, labels):
-  ...     print("%s %s" % (train, test))
-  [4 5] [0 1 2 3]
-  [2 3] [0 1 4 5]
-  [0 1] [2 3 4 5]
-
 .. _ShuffleSplit:
 
 Random permutations cross-validation a.k.a. Shuffle & Split
@@ -467,26 +377,167 @@ Here is a usage example::
 validation that allows a finer control on the number of iterations and
 the proportion of samples on each side of the train / test split.
 
+Cross-validation iterators with stratification based on class labels.
+=====================================================================
+
+Some classification problems can exhibit a large imbalance in the distribution
+of the target classes: for instance there could be several times more negative
+samples than positive samples. In such cases it is recommended to use
+stratified sampling as implemented in :class:`StratifiedKFold` and
+:class:`StratifiedShuffleSplit` to ensure that relative class frequencies is
+approximately preserved in each train and validation fold.
 
-Label-Shuffle-Split
+Stratified k-fold
+-----------------
+
+:class:`StratifiedKFold` is a variation of *k-fold* which returns *stratified*
+folds: each set contains approximately the same percentage of samples of each
+target class as the complete set.
+
+Example of stratified 3-fold cross-validation on a dataset with 10 samples from
+two slightly unbalanced classes::
+
+  >>> from sklearn.model_selection import StratifiedKFold
+
+  >>> X = np.ones(10)
+  >>> y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
+  >>> skf = StratifiedKFold(n_splits=3)
+  >>> for train, test in skf.split(X, y):
+  ...     print("%s %s" % (train, test))
+  [2 3 6 7 8 9] [0 1 4 5]
+  [0 1 3 4 5 8 9] [2 6 7]
+  [0 1 2 4 5 6 7] [3 8 9]
+
+Stratified Shuffle Split
+------------------------
+
+:class:`StratifiedShuffleSplit` is a variation of *ShuffleSplit*, which returns
+stratified splits, *i.e* which creates splits by preserving the same
+percentage for each target class as in the complete set.
+
+.. _group_cv
+
+Cross-validation iterators for grouped data.
+============================================
+
+The i.i.d. assumption is broken if the underlying generative process yield
+groups of dependent samples.
+
+Such a grouping of data is domain specific. An example would be when there is
+medical data collected from multiple patients, with multiple samples taken from
+each patient. And such data is likely to be dependent on the individual group.
+In our example, the patient id for each sample will be its group identifier.
+
+In this case we would like to know if a model trained on a particular set of
+groups generalizes well to the unseen groups. To measure this, we need to
+ensure that all the samples in the validation fold come from groups that are
+not represented at all in the paired training fold.
+ 
+The following cross-validation splitters can be used to do that.
+The grouping identifier for the samples is specified via the ``groups``
+parameter.
+
+
+Group k-fold
+------------
+
+class:GroupKFold is a variation of k-fold which ensures that the same group is
+not represented in both testing and training sets. For example if the data is
+obtained from different subjects with several samples per-subject and if the
+model is flexible enough to learn from highly person specific features it
+could fail to generalize to new subjects. class:GroupKFold makes it possible
+to detect this kind of overfitting situations.
+
+Imagine you have three subjects, each with an associated number from 1 to 3::
+
+  >>> from sklearn.model_selection import GroupKFold
+
+  >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]
+  >>> y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
+  >>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]
+
+  >>> gkf = GroupKFold(n_splits=3)
+  >>> for train, test in gkf.split(X, y, groups=groups):
+  ...     print("%s %s" % (train, test))
+  [0 1 2 3 4 5] [6 7 8 9]
+  [0 1 2 6 7 8 9] [3 4 5]
+  [3 4 5 6 7 8 9] [0 1 2]
+
+Each subject is in a different testing fold, and the same subject is never in
+both testing and training. Notice that the folds do not have exactly the same
+size due to the imbalance in the data.
+
+
+Leave One Group Out
 -------------------
 
-:class:`LabelShuffleSplit`
+:class:`LeaveOneGroupOut` is a cross-validation scheme which holds out
+the samples according to a third-party provided array of integer groups. This
+group information can be used to encode arbitrary domain specific pre-defined
+cross-validation folds.
+
+Each training set is thus constituted by all the samples except the ones
+related to a specific group.
+
+For example, in the cases of multiple experiments, :class:`LeaveOneGroupOut`
+can be used to create a cross-validation based on the different experiments:
+we create a training set using the samples of all the experiments except one::
+
+  >>> from sklearn.model_selection import LeaveOneGroupOut
+
+  >>> X = [1, 5, 10, 50, 60, 70, 80]
+  >>> y = [0, 1, 1, 2, 2, 2, 2]
+  >>> groups = [1, 1, 2, 2, 3, 3, 3]
+  >>> logo = LeaveOneGroupOut()
+  >>> for train, test in logo.split(X, y, groups=groups):
+  ...     print("%s %s" % (train, test))
+  [2 3 4 5 6] [0 1]
+  [0 1 4 5 6] [2 3]
+  [0 1 2 3] [4 5 6]
+
+Another common application is to use time information: for instance the
+groups could be the year of collection of the samples and thus allow
+for cross-validation against time-based splits.
+
+Leave P Groups Out
+------------------
 
-The :class:`LabelShuffleSplit` iterator behaves as a combination of
-:class:`ShuffleSplit` and :class:`LeavePLabelsOut`, and generates a
-sequence of randomized partitions in which a subset of labels are held
+:class:`LeavePGroupsOut` is similar as :class:`LeaveOneGroupOut`, but removes
+samples related to :math:`P` groups for each training/test set.
+
+Example of Leave-2-Group Out::
+
+  >>> from sklearn.model_selection import LeavePGroupsOut
+
+  >>> X = np.arange(6)
+  >>> y = [1, 1, 1, 2, 2, 2]
+  >>> groups = [1, 1, 2, 2, 3, 3]
+  >>> lpgo = LeavePGroupsOut(n_groups=2)
+  >>> for train, test in lpgo.split(X, y, groups=groups):
+  ...     print("%s %s" % (train, test))
+  [4 5] [0 1 2 3]
+  [2 3] [0 1 4 5]
+  [0 1] [2 3 4 5]
+
+Group Shuffle Split
+-------------------
+
+:class:`GroupShuffleSplit`
+
+The :class:`GroupShuffleSplit` iterator behaves as a combination of
+:class:`ShuffleSplit` and :class:`LeavePGroupsOut`, and generates a
+sequence of randomized partitions in which a subset of groups are held
 out for each split.
 
 Here is a usage example::
 
-  >>> from sklearn.model_selection import LabelShuffleSplit
+  >>> from sklearn.model_selection import GroupShuffleSplit
 
   >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]
   >>> y = ["a", "b", "b", "b", "c", "c", "c", "a"]
-  >>> labels = [1, 1, 2, 2, 3, 3, 4, 4]
-  >>> lss = LabelShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
-  >>> for train, test in lss.split(X, y, labels):
+  >>> groups = [1, 1, 2, 2, 3, 3, 4, 4]
+  >>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
+  >>> for train, test in gss.split(X, y, groups=groups):
   ...     print("%s %s" % (train, test))
   ...
   [0 1 2 3] [4 5 6 7]
@@ -494,17 +545,16 @@ Here is a usage example::
   [2 3 4 5] [0 1 6 7]
   [4 5 6 7] [0 1 2 3]
 
-This class is useful when the behavior of :class:`LeavePLabelsOut` is
-desired, but the number of labels is large enough that generating all
-possible partitions with :math:`P` labels withheld would be prohibitively
-expensive.  In such a scenario, :class:`LabelShuffleSplit` provides
+This class is useful when the behavior of :class:`LeavePGroupsOut` is
+desired, but the number of groups is large enough that generating all
+possible partitions with :math:`P` groups withheld would be prohibitively
+expensive.  In such a scenario, :class:`GroupShuffleSplit` provides
 a random sample (with replacement) of the train / test splits
-generated by :class:`LeavePLabelsOut`.
-
+generated by :class:`LeavePGroupsOut`.
 
 
 Predefined Fold-Splits / Validation-Sets
-----------------------------------------
+========================================
 
 For some datasets, a pre-defined split of the data into training- and
 validation fold or into several cross-validation folds already
@@ -514,12 +564,7 @@ e.g. when searching for hyperparameters.
 For example, when using a validation set, set the ``test_fold`` to 0 for all
 samples that are part of the validation set, and to -1 for all other samples.
 
-
-See also
---------
-:class:`StratifiedShuffleSplit` is a variation of *ShuffleSplit*, which returns
-stratified splits, *i.e* which creates splits by preserving the same
-percentage for each target class as in the complete set.
+.. _timeseries_cv
 
 Cross validation of time series data
 ====================================
@@ -536,8 +581,8 @@ least like those that are used to train the model. To achieve this, one
 solution is provided by :class:`TimeSeriesSplit`.
 
 
-TimeSeriesSplit
------------------------
+Time Series Split
+-----------------
 
 :class:`TimeSeriesSplit` is a variation of *k-fold* which 
 returns first :math:`k` folds as train set and the :math:`(k+1)` th 
@@ -568,8 +613,8 @@ Example of 3-split time series cross-validation on a dataset with 6 samples::
 A note on shuffling
 ===================
 
-If the data ordering is not arbitrary (e.g. samples with the same label are
-contiguous), shuffling it first may be essential to get a meaningful cross-
+If the data ordering is not arbitrary (e.g. samples with the same class label
+are contiguous), shuffling it first may be essential to get a meaningful cross-
 validation result. However, the opposite may be true if the samples are not
 independently and identically distributed. For example, if samples correspond
 to news articles, and are ordered by their time of publication, then shuffling
diff --git a/doc/modules/mixture.rst b/doc/modules/mixture.rst
index cf9c3ea7e7..2449901e90 100755
--- a/doc/modules/mixture.rst
+++ b/doc/modules/mixture.rst
@@ -8,7 +8,7 @@ Gaussian mixture models
 
 .. currentmodule:: sklearn.mixture
 
-`sklearn.mixture` is a package which enables one to learn
+``sklearn.mixture`` is a package which enables one to learn
 Gaussian Mixture Models (diagonal, spherical, tied and full covariance
 matrices supported), sample them, and estimate them from
 data. Facilities to help determine the appropriate number of
@@ -93,14 +93,16 @@ Cons
    or information theoretical criteria to decide how many components to use
    in the absence of external cues.
 
-Selecting the number of components in a classical GMM
-------------------------------------------------------
+Selecting the number of components in a classical Gaussian Mixture Model
+------------------------------------------------------------------------
 
-The BIC criterion can be used to select the number of components in a GMM
-in an efficient way. In theory, it recovers the true number of components
-only in the asymptotic regime (i.e. if much data is available).
-Note that using a :ref:`DPGMM <dpgmm>` avoids the specification of the
-number of components for a Gaussian mixture model.
+The BIC criterion can be used to select the number of components in a Gaussian
+Mixture in an efficient way. In theory, it recovers the true number of
+components only in the asymptotic regime (i.e. if much data is available and
+assuming that the data was actually generated i.i.d. from a mixture of Gaussian
+distribution). Note that using a :ref:`Variational Bayesian Gaussian mixture <bgmm>`
+avoids the specification of the number of components for a Gaussian mixture
+model.
 
 .. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_selection_001.png
    :target: ../auto_examples/mixture/plot_gmm_selection.html
@@ -135,11 +137,12 @@ to a local optimum.
 
 .. _bgmm:
 
-Bayesian Gaussian Mixture
-=========================
+Variational Bayesian Gaussian Mixture
+=====================================
 
-The :class:`BayesianGaussianMixture` object implements a variant of the Gaussian
-mixture model with variational inference algorithms.
+The :class:`BayesianGaussianMixture` object implements a variant of the
+Gaussian mixture model with variational inference algorithms. The API is
+similar as the one defined by :class:`GaussianMixture`.
 
 .. _variational_inference:
 
@@ -152,171 +155,167 @@ priors) instead of data likelihood. The principle behind
 variational methods is the same as expectation-maximization (that is
 both are iterative algorithms that alternate between finding the
 probabilities for each point to be generated by each mixture and
-fitting the mixtures to these assigned points), but variational
+fitting the mixture to these assigned points), but variational
 methods add regularization by integrating information from prior
 distributions. This avoids the singularities often found in
 expectation-maximization solutions but introduces some subtle biases
 to the model. Inference is often notably slower, but not usually as
 much so as to render usage unpractical.
 
-Due to its Bayesian nature, the variational algorithm needs more
-hyper-parameters than expectation-maximization, the most
-important of these being the concentration parameter ``dirichlet_concentration_prior``. Specifying
-a high value of prior of the dirichlet concentration leads more often to uniformly-sized mixture
-components, while specifying small (between 0 and 1) values will lead
-to some mixture components getting almost all the points while most
-mixture components will be centered on just a few of the remaining
-points.
-
-.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_bayesian_gaussian_mixture_001.png
-   :target: ../auto_examples/mixture/plot_bayesian_gaussian_mixture.html
-   :align: center
-   :scale: 50%
-
-.. topic:: Examples:
-
-    * See :ref:`sphx_glr_auto_examples_plot_bayesian_gaussian_mixture.py` for a comparaison of
-      the results of the ``BayesianGaussianMixture`` for different values
-      of the parameter ``dirichlet_concentration_prior``.
-
-Pros and cons of variational inference with :class:BayesianGaussianMixture
---------------------------------------------------------------------------
-
-Pros
-.....
-
-:Regularization: due to the incorporation of prior information,
-   variational solutions have less pathological special cases than
-   expectation-maximization solutions.
-
-:Automatic selection: when `dirichlet_concentration_prior` is small enough and
-   `n_components` is larger than what is found necessary by the model, the
-   Variational Bayesian mixture model has a natural tendency to set some mixture
-   weights values close to zero. This makes it possible to let the model choose a
-   suitable number of effective components automatically.
-
-Cons
-.....
-
-:Bias: to regularize a model one has to add biases. The
-   variational algorithm will bias all the means towards the origin
-   (part of the prior information adds a "ghost point" in the origin
-   to every mixture component) and it will bias the covariances to
-   be more spherical. It will also, depending on the concentration
-   parameter, bias the cluster structure either towards uniformity
-   or towards a rich-get-richer scenario.
-
-:Hyperparameters: this algorithm needs an extra hyperparameter
-   that might need experimental tuning via cross-validation.
+Due to its Bayesian nature, the variational algorithm needs more hyper-
+parameters than expectation-maximization, the most important of these being the
+concentration parameter ``weight_concentration_prior``. Specifying a low value
+for the concentration prior will make the model put most of the weight on few
+components set the remaining components weights very close to zero. High values
+of the concentration prior will allow a larger number of components to be active
+in the mixture.
+
+The parameters implementation of the :class:`BayesianGaussianMixture` class
+proposes two types of prior for the weights distribution: a finite mixture model
+with Dirichlet distribution and an infinite mixture model with the Dirichlet
+Process. In practice Dirichlet Process inference algorithm is approximated and
+uses a truncated distribution with a fixed maximum number of components (called
+the Stick-breaking representation). The number of components actually used
+almost always depends on the data.
+
+The next figure compares the results obtained for the different type of the
+weight concentration prior (parameter ``weight_concentration_prior_type``)
+for different values of ``weight_concentration_prior``.
+Here, we can see the the value of the ``weight_concentration_prior`` parameter
+has a strong impact on the effective number of active components obtained. We
+can also notice that large values for the concentration weight prior lead to
+more uniform weights when the type of prior is 'dirichlet_distribution' while
+this is not necessarily the case for the 'dirichlet_process' type (used by
+default).
+
+.. |plot_bgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_002.png
+   :target: ../auto_examples/mixture/plot_gmm.html
+   :scale: 48%
 
-.. _dpgmm:
+.. |plot_dpgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_002.png
+   :target: ../auto_examples/mixture/plot_concentration_prior.html
+   :scale: 48%
 
-DPGMM: Infinite Gaussian mixtures
-=================================
+.. centered:: |plot_bgmm| |plot_dpgmm|
+
+The examples below compare Gaussian mixture models with a fixed number of
+components, to the variational Gaussian mixture models with a Dirichlet process
+prior. Here, a classical Gaussian mixture is fitted with 5 components on a
+dataset composed of 2 clusters. We can see that the variational Gaussian mixture
+with a Dirichlet process prior is able to limit itself to only 2 components
+whereas the Gaussian mixture fits the data with a fixed number of components
+that has to be set a priori by the user. In this case the user has selected
+``n_components=5`` which does not match the true generative distribution of this
+toy dataset. Note that with very little observations, the variational Gaussian
+mixture models with a Dirichlet process prior can take a conservative stand, and
+fit only one component.
+
+.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_001.png
+   :target: ../auto_examples/mixture/plot_gmm.html
+   :align: center
+   :scale: 70%
 
-The :class:`DPGMM` object implements a variant of the Gaussian mixture
-model with a variable (but bounded) number of components using the
-Dirichlet Process.
-This class doesn't require the user to choose the number of
-components, and at the expense of extra computational time the user
-only needs to specify a loose upper bound on this number and a
-concentration parameter.
 
-.. |plot_gmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_001.png
-   :target: ../auto_examples/mixture/plot_gmm.html
-   :scale: 48%
+On the following figure we are fitting a dataset not well-depicted by a
+Gaussian mixture. Adjusting the ``weight_concentration_prior``, parameter of the
+class:`BayesianGaussianMixture` controls the number of components used to fit
+this data. We also present on the last two plots a random sampling generated
+from the two resulting mixtures.
 
-.. |plot_gmm_sin| image:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_sin_001.png
+.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_sin_001.png
    :target: ../auto_examples/mixture/plot_gmm_sin.html
-   :scale: 48%
-
-.. centered:: |plot_gmm| |plot_gmm_sin|
+   :align: center
+   :scale: 65%
 
 
-The examples above compare Gaussian mixture models with fixed number of
-components, to DPGMM models. **On the left** the GMM is fitted with 5
-components on a dataset composed of 2 clusters. We can see that the DPGMM is
-able to limit itself to only 2 components whereas the GMM fits the data fit too
-many components. Note that with very little observations, the DPGMM can take a
-conservative stand, and fit only one component. **On the right** we are fitting
-a dataset not well-depicted by a Gaussian mixture. Adjusting the `alpha`
-parameter of the DPGMM controls the number of components used to fit this
-data.
 
 .. topic:: Examples:
 
-    * See :ref:`sphx_glr_auto_examples_mixture_plot_gmm.py` for an example on plotting the
-      confidence ellipsoids for both :class:`GaussianMixture`
-      and :class:`DPGMM`.
+    * See :ref:`sphx_glr_auto_examples_mixture_plot_gmm.py` for an example on
+      plotting the confidence ellipsoids for both :class:`GaussianMixture`
+      and :class:`BayesianGaussianMixture`.
 
     * :ref:`sphx_glr_auto_examples_mixture_plot_gmm_sin.py` shows using
-      :class:`GaussianMixture` and :class:`DPGMM` to fit a sine wave
+      :class:`GaussianMixture` and :class:`BayesianGaussianMixture` to fit a
+      sine wave.
+
+    * See :ref:`sphx_glr_auto_examples_mixture_plot_concentration_prior.py`
+      for an example plotting the confidence ellipsoids for the
+      :class:`BayesianGaussianMixture` with different
+      ``weight_concentration_prior_type`` for different values of the parameter
+      ``weight_concentration_prior``.
 
-Pros and cons of class :class:`DPGMM`: Dirichlet process mixture model
-----------------------------------------------------------------------
+
+Pros and cons of variational inference with :class:`BayesianGaussianMixture`
+----------------------------------------------------------------------------
 
 Pros
 .....
 
-:Less sensitivity to the number of parameters: unlike finite
-   models, which will almost always use all components as much as
-   they can, and hence will produce wildly different solutions for
-   different numbers of components, the Dirichlet process solution
-   won't change much with changes to the parameters, leading to more
-   stability and less tuning.
+:Automatic selection: when ``weight_concentration_prior`` is small enough and
+   ``n_components`` is larger than what is found necessary by the model, the
+   Variational Bayesian mixture model has a natural tendency to set some mixture
+   weights values close to zero. This makes it possible to let the model choose
+   a suitable number of effective components automatically. Only an upper bound
+   of this number needs to be provided. Note however that the "ideal" number of
+   active components is very application specific and is typically ill-defined
+   in a data exploration setting.
+
+:Less sensitivity to the number of parameters: unlike finite models, which will
+   almost always use all components as much as they can, and hence will produce
+   wildly different solutions for different numbers of components, the
+   variantional inference with a Dirichlet process prior
+   (``weight_concentration_prior_type='dirichlet_process'``) won't change much
+   with changes to the parameters, leading to more stability and less tuning.
+
+:Regularization: due to the incorporation of prior information,
+   variational solutions have less pathological special cases than
+   expectation-maximization solutions.
 
-:No need to specify the number of components: only an upper bound of
-   this number needs to be provided. Note however that the DPMM is not
-   a formal model selection procedure, and thus provides no guarantee
-   on the result.
 
 Cons
 .....
 
-:Speed: the extra parametrization necessary for variational
-   inference and for the structure of the Dirichlet process can and
-   will make inference slower, although not by much.
+:Speed: the extra parametrization necessary for variational inference make
+   inference slower, although not by much.
+
+:Hyperparameters: this algorithm needs an extra hyperparameter
+   that might need experimental tuning via cross-validation.
 
-:Bias: as in variational techniques, but only more so, there are
-   many implicit biases in the Dirichlet process and the inference
-   algorithms, and whenever there is a mismatch between these biases
-   and the data it might be possible to fit better models using a
+:Bias: there are many implicit biases in the inference algorithms (and also in
+   the Dirichlet process if used), and whenever there is a mismatch between
+   these biases and the data it might be possible to fit better models using a
    finite mixture.
 
+
 .. _dirichlet_process:
 
 The Dirichlet Process
 ---------------------
 
 Here we describe variational inference algorithms on Dirichlet process
-mixtures. The Dirichlet process is a prior probability distribution on
+mixture. The Dirichlet process is a prior probability distribution on
 *clusterings with an infinite, unbounded, number of partitions*.
 Variational techniques let us incorporate this prior structure on
 Gaussian mixture models at almost no penalty in inference time, comparing
 with a finite Gaussian mixture model.
 
-An important question is how can the Dirichlet process use an
-infinite, unbounded number of clusters and still be consistent. While
-a full explanation doesn't fit this manual, one can think of its
-`chinese restaurant process
-<https://en.wikipedia.org/wiki/Chinese_restaurant_process>`_
-analogy to help understanding it. The
-chinese restaurant process is a generative story for the Dirichlet
-process. Imagine a chinese restaurant with an infinite number of
-tables, at first all empty. When the first customer of the day
-arrives, he sits at the first table. Every following customer will
-then either sit on an occupied table with probability proportional to
-the number of customers in that table or sit in an entirely new table
-with probability proportional to the concentration parameter
-`alpha`. After a finite number of customers has sat, it is easy to see
-that only finitely many of the infinite tables will ever be used, and
-the higher the value of alpha the more total tables will be used. So
-the Dirichlet process does clustering with an unbounded number of
-mixture components by assuming a very asymmetrical prior structure
-over the assignments of points to components that is very concentrated
-(this property is known as rich-get-richer, as the full tables in the
-Chinese restaurant process only tend to get fuller as the simulation
-progresses).
+An important question is how can the Dirichlet process use an infinite,
+unbounded number of clusters and still be consistent. While a full explanation
+doesn't fit this manual, one can think of its `stick breaking process
+<https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process>`
+analogy to help understanding it. The stick breaking process is a generative
+story for the Dirichlet process. We start with a unit-length stick and in each
+step we break off a portion of the remaining stick. Each time, we associate the
+length of the piece of the stick to the proportion of points that falls into a
+group of the mixture. At the end, to represent the infinite mixture, we
+associate the last remaining piece of the stick to the proportion of points
+that don't fall into all the other groups. The length of each piece is random
+variable with probability proportional to the concentration parameter. Smaller
+value of the concentration will divide the unit-length into larger pieces of
+the stick (defining more concentrated distribution). Larger concentration
+values will create smaller pieces of the stick (increasing the number of
+components with non zero weights).
 
 Variational inference techniques for the Dirichlet process still work
 with a finite approximation to this infinite mixture model, but
@@ -325,13 +324,3 @@ use, one just specifies the concentration parameter and an upper bound
 on the number of mixture components (this upper bound, assuming it is
 higher than the "true" number of components, affects only algorithmic
 complexity, not the actual number of components used).
-
-.. topic:: Derivation:
-
-   * See `here <dp-derivation.html>`_ the full derivation of this
-     algorithm.
-
-.. toctree::
-    :hidden:
-
-    dp-derivation.rst
diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst
index 690e85f915..a44e753fb2 100755
--- a/doc/modules/model_evaluation.rst
+++ b/doc/modules/model_evaluation.rst
@@ -49,36 +49,37 @@ Common cases: predefined values
 For the most common use cases, you can designate a scorer object with the
 ``scoring`` parameter; the table below shows all possible values.
 All scorer objects follow the convention that **higher return values are better
-than lower return values**.  Thus the returns from mean_absolute_error
-and mean_squared_error, which measure the distance between the model
-and the data, are negated.
+than lower return values**.  Thus metrics which measure the distance between
+the model and the data, like :func:`metrics.mean_squared_error`, are
+available as neg_mean_squared_error which return the negated value
+of the metric.
 
 
-========================     =======================================     ==================================
-Scoring                      Function                                    Comment
-========================     =======================================     ==================================
+==========================      =========================================     ==================================
+Scoring                         Function                                      Comment
+==========================      =========================================     ==================================
 **Classification**
-'accuracy'                   :func:`metrics.accuracy_score`
-'average_precision'          :func:`metrics.average_precision_score`
-'f1'                         :func:`metrics.f1_score`                    for binary targets
-'f1_micro'                   :func:`metrics.f1_score`                    micro-averaged
-'f1_macro'                   :func:`metrics.f1_score`                    macro-averaged
-'f1_weighted'                :func:`metrics.f1_score`                    weighted average
-'f1_samples'                 :func:`metrics.f1_score`                    by multilabel sample
-'log_loss'                   :func:`metrics.log_loss`                    requires ``predict_proba`` support
-'precision' etc.             :func:`metrics.precision_score`             suffixes apply as with 'f1'
-'recall' etc.                :func:`metrics.recall_score`                suffixes apply as with 'f1'
-'roc_auc'                    :func:`metrics.roc_auc_score`
+'accuracy'                      :func:`metrics.accuracy_score`
+'average_precision'             :func:`metrics.average_precision_score`
+'f1'                            :func:`metrics.f1_score`                      for binary targets
+'f1_micro'                      :func:`metrics.f1_score`                      micro-averaged
+'f1_macro'                      :func:`metrics.f1_score`                      macro-averaged
+'f1_weighted'                   :func:`metrics.f1_score`                      weighted average
+'f1_samples'                    :func:`metrics.f1_score`                      by multilabel sample
+'neg_log_loss'                  :func:`metrics.log_loss`                      requires ``predict_proba`` support
+'precision' etc.                :func:`metrics.precision_score`               suffixes apply as with 'f1'
+'recall' etc.                   :func:`metrics.recall_score`                  suffixes apply as with 'f1'
+'roc_auc'                       :func:`metrics.roc_auc_score`
 
 **Clustering**
-'adjusted_rand_score'        :func:`metrics.adjusted_rand_score`
+'adjusted_rand_score'           :func:`metrics.adjusted_rand_score`
 
 **Regression**
-'mean_absolute_error'        :func:`metrics.mean_absolute_error`
-'mean_squared_error'         :func:`metrics.mean_squared_error`
-'median_absolute_error'      :func:`metrics.median_absolute_error`
-'r2'                         :func:`metrics.r2_score`
-========================     =======================================     ==================================
+'neg_mean_absolute_error'       :func:`metrics.mean_absolute_error`
+'neg_mean_squared_error'        :func:`metrics.mean_squared_error`
+'neg_median_absolute_error'     :func:`metrics.median_absolute_error`
+'r2'                            :func:`metrics.r2_score`
+===========================     =========================================     ==================================
 
 Usage examples:
 
@@ -87,12 +88,12 @@ Usage examples:
     >>> iris = datasets.load_iris()
     >>> X, y = iris.data, iris.target
     >>> clf = svm.SVC(probability=True, random_state=0)
-    >>> cross_val_score(clf, X, y, scoring='log_loss') # doctest: +ELLIPSIS
+    >>> cross_val_score(clf, X, y, scoring='neg_log_loss') # doctest: +ELLIPSIS
     array([-0.07..., -0.16..., -0.06...])
     >>> model = svm.SVC()
     >>> cross_val_score(model, X, y, scoring='wrong_choice')
     Traceback (most recent call last):
-    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']
+    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']
 
 .. note::
 
@@ -231,6 +232,7 @@ Others also work in the multiclass case:
 .. autosummary::
    :template: function.rst
 
+   cohen_kappa_score
    confusion_matrix
    hinge_loss
 
@@ -359,7 +361,8 @@ In the multilabel case with binary label indicators: ::
 Cohen's kappa
 -------------
 
-The function :func:`cohen_kappa_score` computes Cohen's kappa statistic.
+The function :func:`cohen_kappa_score` computes `Cohen's kappa
+<https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_ statistic.
 This measure is intended to compare labelings by different human annotators,
 not a classifier versus a ground truth.
 
@@ -371,6 +374,12 @@ Kappa scores can be computed for binary or multiclass problems,
 but not for multilabel problems (except by manually computing a per-label score)
 and not for more than two annotators.
 
+  >>> from sklearn.metrics import cohen_kappa_score
+  >>> y_true = [2, 0, 2, 2, 0, 1]
+  >>> y_pred = [0, 0, 2, 2, 0, 2]
+  >>> cohen_kappa_score(y_true, y_pred)
+  0.4285714285714286
+
 .. _confusion_matrix:
 
 Confusion matrix
diff --git a/doc/modules/neural_networks_supervised.rst b/doc/modules/neural_networks_supervised.rst
index 036ea71cce..6f083ab4a8 100755
--- a/doc/modules/neural_networks_supervised.rst
+++ b/doc/modules/neural_networks_supervised.rst
@@ -107,14 +107,6 @@ contains the weight matrices that constitute the model parameters::
     >>> [coef.shape for coef in clf.coefs_]
     [(2, 5), (5, 2), (2, 1)]
 
-To get the raw values before applying the output activation function, run the
-following command,
-
-use :meth:`MLPClassifier.decision_function`::
-
-    >>> clf.decision_function([[2., 2.], [1., 2.]])  # doctest: +ELLIPSIS
-    array([ 47.6...,  47.6...])
-
 Currently, :class:`MLPClassifier` supports only the
 Cross-Entropy loss function, which allows probability estimates by running the
 ``predict_proba`` method.
@@ -125,23 +117,23 @@ classification, it minimizes the Cross-Entropy loss function, giving a vector
 of probability estimates :math:`P(y|x)` per sample :math:`x`::
 
     >>> clf.predict_proba([[2., 2.], [1., 2.]])  # doctest: +ELLIPSIS
-    array([[ 0.,  1.],
-           [ 0.,  1.]])
+    array([[  1.967...e-04,   9.998...-01],
+           [  1.967...e-04,   9.998...-01]])
 
 :class:`MLPClassifier` supports multi-class classification by
 applying `Softmax <https://en.wikipedia.org/wiki/Softmax_activation_function>`_
 as the output function.
 
 Further, the algorithm supports :ref:`multi-label classification <multiclass>`
-in which a sample can belong to more than one class. For each class, the output
-of :meth:`MLPClassifier.decision_function` passes through the
-logistic function. Values larger or equal to `0.5` are rounded to `1`,
-otherwise to `0`. For a predicted output of a sample, the indices where the
-value is `1` represents the assigned classes of that sample::
+in which a sample can belong to more than one class. For each class, the raw
+output passes through the logistic function. Values larger or equal to `0.5`
+are rounded to `1`, otherwise to `0`. For a predicted output of a sample, the
+indices where the value is `1` represents the assigned classes of that sample::
 
     >>> X = [[0., 0.], [1., 1.]]
     >>> y = [[0, 1], [1, 1]]
-    >>> clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)
+    >>> clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5,
+    ...                     hidden_layer_sizes=(15,), random_state=1)
     >>> clf.fit(X, y)
     MLPClassifier(activation='relu', algorithm='l-bfgs', alpha=1e-05,
            batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
diff --git a/doc/tutorial/statistical_inference/model_selection.rst b/doc/tutorial/statistical_inference/model_selection.rst
index ef3568ffb0..6158846b27 100755
--- a/doc/tutorial/statistical_inference/model_selection.rst
+++ b/doc/tutorial/statistical_inference/model_selection.rst
@@ -110,7 +110,7 @@ scoring method.
 
     - :class:`StratifiedKFold` **(n_iter, test_size, train_size, random_state)**
 
-    - :class:`LabelKFold` **(n_splits, shuffle, random_state)**
+    - :class:`GroupKFold` **(n_splits, shuffle, random_state)**
 
 
    *
@@ -119,7 +119,7 @@ scoring method.
 
     - Same as K-Fold but preserves the class distribution within each fold.
 
-    - Ensures that the same label is not in both testing and training sets.
+    - Ensures that the same group is not in both testing and training sets.
 
 
 .. list-table::
@@ -130,7 +130,7 @@ scoring method.
 
     - :class:`StratifiedShuffleSplit`
 
-    - :class:`LabelShuffleSplit`
+    - :class:`GroupShuffleSplit`
 
    *
 
@@ -138,16 +138,16 @@ scoring method.
 
     - Same as shuffle split but preserves the class distribution within each iteration.
 
-    - Ensures that the same label is not in both testing and training sets.
+    - Ensures that the same group is not in both testing and training sets.
 
 
 .. list-table::
 
    *
 
-    - :class:`LeaveOneLabelOut` **()**
+    - :class:`LeaveOneGroupOut` **()**
 
-    - :class:`LeavePLabelOut`  **(p)**
+    - :class:`LeavePGroupsOut`  **(p)**
 
     - :class:`LeaveOneOut` **()**
 
@@ -155,9 +155,9 @@ scoring method.
 
    *
 
-    - Takes a label array to group observations.
+    - Takes a group array to group observations.
 
-    - Leave P labels out.
+    - Leave P groups out.
 
     - Leave one observation out.
 
diff --git a/doc/whats_new.rst b/doc/whats_new.rst
index 7ac7c5fcc8..9e4e9bfebd 100755
--- a/doc/whats_new.rst
+++ b/doc/whats_new.rst
@@ -64,16 +64,41 @@ Model Selection Enhancements and API Changes
   - **Parameters ``n_folds`` and ``n_iter`` renamed to ``n_splits``**
 
     Some parameter names have changed:
-    The ``n_folds`` parameter in :class:`model_selection.KFold`,
-    :class:`model_selection.LabelKFold`, and
-    :class:`model_selection.StratifiedKFold` is now renamed to ``n_splits``.
-    The ``n_iter`` parameter in :class:`model_selection.ShuffleSplit`,
-    :class:`model_selection.LabelShuffleSplit`,
-    and :class:`model_selection.StratifiedShuffleSplit` is now renamed
-    to ``n_splits``.
+    The ``n_folds`` parameter in new :class:`model_selection.KFold`,
+    :class:`model_selection.GroupKFold` (see below for the name change),
+    and :class:`model_selection.StratifiedKFold` is now renamed to
+    ``n_splits``. The ``n_iter`` parameter in
+    :class:`model_selection.ShuffleSplit`, the new class
+    :class:`model_selection.GroupShuffleSplit` and
+    :class:`model_selection.StratifiedShuffleSplit` is now renamed to
+    ``n_splits``.
+
+  - **Rename of splitter classes which accepts group labels along with data**
+
+    The cross-validation splitters ``LabelKFold``,
+    ``LabelShuffleSplit``, ``LeaveOneLabelOut`` and ``LeavePLabelOut`` have
+    been renamed to :class:`model_selection.GroupKFold`,
+    :class:`model_selection.GroupShuffleSplit`,
+    :class:`model_selection.LeaveOneGroupOut` and
+    :class:`model_selection.LeavePGroupsOut` respectively.
+
+    NOTE the change from singular to plural form in
+    :class:`model_selection.LeavePGroupsOut`.
+
+  - **Fit parameter ``labels`` renamed to ``groups``**
+
+    The ``labels`` parameter in the :func:`split` method of the newly renamed
+    splitters :class:`model_selection.GroupKFold`,
+    :class:`model_selection.LeaveOneGroupOut`,
+    :class:`model_selection.LeavePGroupsOut`,
+    :class:`model_selection.GroupShuffleSplit` is renamed to ``groups``
+    following the new nomenclature of their class names.
+
+  - **Parameter ``n_labels`` renamed to ``n_groups``**
+
+    The parameter ``n_labels`` in the newly renamed
+    :class:`model_selection.LeavePGroupsOut` is changed to ``n_groups``.
 
-Changelog
----------
 
 New features
 ............
@@ -397,6 +422,16 @@ Bug fixes
     - Fix :class:`linear_model.ElasticNet` sparse decision function to match
       output with dense in the multioutput case.
 
+    - Fix in :class:`sklearn.model_selection.StratifiedShuffleSplit` to
+      return splits of size ``train_size`` and ``test_size`` in all cases
+      (`#6472 <https://github.com/scikit-learn/scikit-learn/pull/6472>`).
+      By `Andreas MÃ¼ller`_.
+
+    - :func:`metrics.roc_curve` and :func:`metrics.precision_recall_curve` no
+      longer round ``y_score`` values when creating ROC curves; this was causing
+      problems for users with very small differences in scores (`#7353
+      <https://github.com/scikit-learn/scikit-learn/pull/7353>`_).
+
 API changes summary
 -------------------
 
@@ -414,10 +449,20 @@ API changes summary
      :class:`isotonic.IsotonicRegression`. By `Jonathan Arfa`_.
 
    - The old :class:`VBGMM` is deprecated in favor of the new
-     :class:`BayesianGaussianMixture`. The new class solves the computational
+     :class:`BayesianGaussianMixture` (with the parameter
+     ``weight_concentration_prior_type='dirichlet_distribution'``).
+     The new class solves the computational
+     problems of the old class and computes the Gaussian mixture with a
+     Dirichlet process prior faster than before.
+     (`#7295 <https://github.com/scikit-learn/scikit-learn/pull/7295>`_) by
+     `Wei Xue`_ and `Thierry Guillemot`_.
+
+   - The old :class:`VBGMM` is deprecated in favor of the new
+     :class:`BayesianGaussianMixture` (with the parameter
+     ``weight_concentration_prior_type='dirichlet_distribution'``).
+     The new class solves the computational
      problems of the old class and computes the Variational Bayesian Gaussian
      mixture faster than before.
-     Ref :ref:`b` for more information.
      (`#6651 <https://github.com/scikit-learn/scikit-learn/pull/6651>`_) by
      `Wei Xue`_ and `Thierry Guillemot`_.
 
@@ -444,6 +489,20 @@ API changes summary
       :func:`metrics.classification.hamming_loss`.
       (`#7260 <https://github.com/scikit-learn/scikit-learn/pull/7260>`_) by
       `SebastiÃ¡n Vanrell`_.
+   
+    - The splitter classes ``LabelKFold``, ``LabelShuffleSplit``,
+     ``LeaveOneLabelOut`` and ``LeavePLabelsOut`` are renamed to
+     :class:`model_selection.GroupKFold`,
+     :class:`model_selection.GroupShuffleSplit`,
+     :class:`model_selection.LeaveOneGroupOut`
+     and :class:`model_selection.LeavePGroupsOut` respectively.
+     Also the parameter ``labels`` in the :func:`split` method of the newly
+     renamed splitters :class:`model_selection.LeaveOneGroupOut` and
+     :class:`model_selection.LeavePGroupsOut` is renamed to
+     ``groups``. Additionally in :class:`model_selection.LeavePGroupsOut`,
+     the parameter ``n_labels``is renamed to ``n_groups``.
+     (`#6660 <https://github.com/scikit-learn/scikit-learn/pull/6660>`_)
+     by `Raghav RV`_.
 
 
 .. currentmodule:: sklearn
diff --git a/examples/mixture/plot_bayesian_gaussian_mixture.py b/examples/mixture/plot_bayesian_gaussian_mixture.py
deleted file mode 100755
index 9efea3f04c..0000000000
--- a/examples/mixture/plot_bayesian_gaussian_mixture.py
+++ /dev/null
@@ -1,114 +0,0 @@
-"""
-======================================================
-Bayesian Gaussian Mixture Concentration Prior Analysis
-======================================================
-
-Plot the resulting ellipsoids of a mixture of three Gaussians with
-variational Bayesian Gaussian Mixture for three different values on the
-prior the dirichlet concentration.
-
-For all models, the Variationnal Bayesian Gaussian Mixture adapts its number of
-mixture automatically. The parameter `dirichlet_concentration_prior` has a
-direct link with the resulting number of components. Specifying a high value of
-`dirichlet_concentration_prior` leads more often to uniformly-sized mixture
-components, while specifying small (under 0.1) values will lead to some mixture
-components getting almost all the points while most mixture components will be
-centered on just a few of the remaining points.
-"""
-# Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>
-# License: BSD 3 clause
-
-import numpy as np
-import matplotlib as mpl
-import matplotlib.pyplot as plt
-import matplotlib.gridspec as gridspec
-
-from sklearn.mixture import BayesianGaussianMixture
-
-print(__doc__)
-
-
-def plot_ellipses(ax, weights, means, covars):
-    for n in range(means.shape[0]):
-        v, w = np.linalg.eigh(covars[n][:2, :2])
-        u = w[0] / np.linalg.norm(w[0])
-        angle = np.arctan2(u[1], u[0])
-        angle = 180 * angle / np.pi  # convert to degrees
-        v = 2 * np.sqrt(2) * np.sqrt(v)
-        ell = mpl.patches.Ellipse(means[n, :2], v[0], v[1], 180 + angle)
-        ell.set_clip_box(ax.bbox)
-        ell.set_alpha(weights[n])
-        ax.add_artist(ell)
-
-
-def plot_results(ax1, ax2, estimator, dirichlet_concentration_prior, X, y, plot_title=False):
-    estimator.dirichlet_concentration_prior = dirichlet_concentration_prior
-    estimator.fit(X)
-    ax1.set_title("Bayesian Gaussian Mixture for "
-                  r"$dc_0=%.1e$" % dirichlet_concentration_prior)
-    # ax1.axis('equal')
-    ax1.scatter(X[:, 0], X[:, 1], s=5, marker='o', color=colors[y], alpha=0.8)
-    ax1.set_xlim(-2., 2.)
-    ax1.set_ylim(-3., 3.)
-    ax1.set_xticks(())
-    ax1.set_yticks(())
-    plot_ellipses(ax1, estimator.weights_, estimator.means_,
-                  estimator.covariances_)
-
-    ax2.get_xaxis().set_tick_params(direction='out')
-    ax2.yaxis.grid(True, alpha=0.7)
-    for k, w in enumerate(estimator.weights_):
-        ax2.bar(k - .45, w, width=0.9, color='royalblue', zorder=3)
-        ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.),
-                 horizontalalignment='center')
-    ax2.set_xlim(-.6, 2 * n_components - .4)
-    ax2.set_ylim(0., 1.1)
-    ax2.tick_params(axis='y', which='both', left='off',
-                    right='off', labelleft='off')
-    ax2.tick_params(axis='x', which='both', top='off')
-
-    if plot_title:
-        ax1.set_ylabel('Estimated Mixtures')
-        ax2.set_ylabel('Weight of each component')
-
-# Parameters
-random_state = 2
-n_components, n_features = 3, 2
-colors = np.array(['mediumseagreen', 'royalblue', 'r', 'gold',
-                   'orchid', 'indigo', 'darkcyan', 'tomato'])
-dirichlet_concentration_prior = np.logspace(-3, 3, 3)
-covars = np.array([[[.7, .0], [.0, .1]],
-                   [[.5, .0], [.0, .1]],
-                   [[.5, .0], [.0, .1]]])
-samples = np.array([200, 500, 200])
-means = np.array([[.0, -.70],
-                  [.0, .0],
-                  [.0, .70]])
-
-
-# Here we put beta_prior to 0.8 to minimize the influence of the prior for this
-# dataset
-estimator = BayesianGaussianMixture(n_components=2 * n_components,
-                                    init_params='random', max_iter=1500,
-                                    mean_precision_prior=.8, tol=1e-9,
-                                    random_state=random_state)
-
-# Generate data
-rng = np.random.RandomState(random_state)
-X = np.vstack([
-    rng.multivariate_normal(means[j], covars[j], samples[j])
-    for j in range(n_components)])
-y = np.concatenate([j * np.ones(samples[j], dtype=int)
-                    for j in range(n_components)])
-
-# Plot Results
-plt.figure(figsize=(4.7 * 3, 8))
-plt.subplots_adjust(bottom=.04, top=0.95, hspace=.05, wspace=.05,
-                    left=.03, right=.97)
-
-gs = gridspec.GridSpec(3, len(dirichlet_concentration_prior))
-for k, dc in enumerate(dirichlet_concentration_prior):
-    plot_results(plt.subplot(gs[0:2, k]), plt.subplot(gs[2, k]),
-                 estimator, dc, X, y, plot_title=k == 0)
-
-plt.show()
diff --git a/examples/mixture/plot_concentration_prior.py b/examples/mixture/plot_concentration_prior.py
new file mode 100755
index 0000000000..49d9e6211b
--- /dev/null
+++ b/examples/mixture/plot_concentration_prior.py
@@ -0,0 +1,135 @@
+"""
+========================================================================
+Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture
+========================================================================
+
+This example plots the ellipsoids obtained from a toy dataset (mixture of three
+Gaussians) fitted by the ``BayesianGaussianMixture`` class models with a
+Dirichlet distribution prior
+(``weight_concentration_prior_type='dirichlet_distribution'``) and a Dirichlet
+process prior (``weight_concentration_prior_type='dirichlet_process'``). On
+each figure, we plot the results for three different values of the weight
+concentration prior.
+
+The ``BayesianGaussianMixture`` class can adapt its number of mixture
+componentsautomatically. The parameter ``weight_concentration_prior`` has a
+direct link with the resulting number of components with non-zero weights.
+Specifying a low value for the concentration prior will make the model put most
+of the weight on few components set the remaining components weights very close
+to zero. High values of the concentration prior will allow a larger number of
+components to be active in the mixture.
+
+The Dirichlet process prior allows to define an infinite number of components
+and automatically selects the correct number of components: it activates a
+component only if it is necessary.
+
+On the contrary the classical finite mixture model with a Dirichlet
+distribution prior will favor more uniformly weighted components and therefore
+tends to divide natural clusters into unnecessary sub-components.
+"""
+# Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>
+# License: BSD 3 clause
+
+import numpy as np
+import matplotlib as mpl
+import matplotlib.pyplot as plt
+import matplotlib.gridspec as gridspec
+
+from sklearn.mixture import BayesianGaussianMixture
+
+print(__doc__)
+
+
+def plot_ellipses(ax, weights, means, covars):
+    for n in range(means.shape[0]):
+        eig_vals, eig_vecs = np.linalg.eigh(covars[n])
+        unit_eig_vec = eig_vecs[0] / np.linalg.norm(eig_vecs[0])
+        angle = np.arctan2(unit_eig_vec[1], unit_eig_vec[0])
+        # Ellipse needs degrees
+        angle = 180 * angle / np.pi
+        # eigenvector normalization
+        eig_vals = 2 * np.sqrt(2) * np.sqrt(eig_vals)
+        ell = mpl.patches.Ellipse(means[n], eig_vals[0], eig_vals[1],
+                                  180 + angle)
+        ell.set_clip_box(ax.bbox)
+        ell.set_alpha(weights[n])
+        ell.set_facecolor('#56B4E9')
+        ax.add_artist(ell)
+
+
+def plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):
+    ax1.set_title(title)
+    ax1.scatter(X[:, 0], X[:, 1], s=5, marker='o', color=colors[y], alpha=0.8)
+    ax1.set_xlim(-2., 2.)
+    ax1.set_ylim(-3., 3.)
+    ax1.set_xticks(())
+    ax1.set_yticks(())
+    plot_ellipses(ax1, estimator.weights_, estimator.means_,
+                  estimator.covariances_)
+
+    ax2.get_xaxis().set_tick_params(direction='out')
+    ax2.yaxis.grid(True, alpha=0.7)
+    for k, w in enumerate(estimator.weights_):
+        ax2.bar(k - .45, w, width=0.9, color='#56B4E9', zorder=3)
+        ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.),
+                 horizontalalignment='center')
+    ax2.set_xlim(-.6, 2 * n_components - .4)
+    ax2.set_ylim(0., 1.1)
+    ax2.tick_params(axis='y', which='both', left='off',
+                    right='off', labelleft='off')
+    ax2.tick_params(axis='x', which='both', top='off')
+
+    if plot_title:
+        ax1.set_ylabel('Estimated Mixtures')
+        ax2.set_ylabel('Weight of each component')
+
+# Parameters of the dataset
+random_state, n_components, n_features = 2, 3, 2
+colors = np.array(['#0072B2', '#F0E442', '#D55E00'])
+
+covars = np.array([[[.7, .0], [.0, .1]],
+                   [[.5, .0], [.0, .1]],
+                   [[.5, .0], [.0, .1]]])
+samples = np.array([200, 500, 200])
+means = np.array([[.0, -.70],
+                  [.0, .0],
+                  [.0, .70]])
+
+# mean_precision_prior= 0.8 to minimize the influence of the prior
+estimators = [
+    ("Finite mixture with a Dirichlet distribution\nprior and "
+     r"$\gamma_0=$", BayesianGaussianMixture(
+        weight_concentration_prior_type="dirichlet_distribution",
+        n_components=2 * n_components, reg_covar=0, init_params='random',
+        max_iter=1500, mean_precision_prior=.8,
+        random_state=random_state), [0.001, 1, 1000]),
+    ("Infinite mixture with a Dirichlet process\n prior and" r"$\gamma_0=$",
+     BayesianGaussianMixture(
+        weight_concentration_prior_type="dirichlet_process",
+        n_components=2 * n_components, reg_covar=0, init_params='random',
+        max_iter=1500, mean_precision_prior=.8,
+        random_state=random_state), [1, 1000, 100000])]
+
+# Generate data
+rng = np.random.RandomState(random_state)
+X = np.vstack([
+    rng.multivariate_normal(means[j], covars[j], samples[j])
+    for j in range(n_components)])
+y = np.concatenate([j * np.ones(samples[j], dtype=int)
+                    for j in range(n_components)])
+
+# Plot results in two different figures
+for (title, estimator, concentrations_prior) in estimators:
+    plt.figure(figsize=(4.7 * 3, 8))
+    plt.subplots_adjust(bottom=.04, top=0.90, hspace=.05, wspace=.05,
+                        left=.03, right=.99)
+
+    gs = gridspec.GridSpec(3, len(concentrations_prior))
+    for k, concentration in enumerate(concentrations_prior):
+        estimator.weight_concentration_prior = concentration
+        estimator.fit(X)
+        plot_results(plt.subplot(gs[0:2, k]), plt.subplot(gs[2, k]), estimator,
+                     X, y, r"%s$%.1e$" % (title, concentration),
+                     plot_title=k == 0)
+
+plt.show()
diff --git a/examples/mixture/plot_gmm.py b/examples/mixture/plot_gmm.py
index d6b0839c19..5f2f8596d4 100755
--- a/examples/mixture/plot_gmm.py
+++ b/examples/mixture/plot_gmm.py
@@ -3,16 +3,18 @@
 Gaussian Mixture Model Ellipsoids
 =================================
 
-Plot the confidence ellipsoids of a mixture of two Gaussians with EM
-and variational Dirichlet process.
-
-Both models have access to five components with which to fit the
-data. Note that the EM model will necessarily use all five components
-while the DP model will effectively only use as many as are needed for
-a good fit. This is a property of the Dirichlet Process prior. Here we
-can see that the EM model splits some components arbitrarily, because it
-is trying to fit too many components, while the Dirichlet Process model
-adapts it number of state automatically.
+Plot the confidence ellipsoids of a mixture of two Gaussians
+obtained with Expectation Maximisation (``GaussianMixture`` class) and
+Variational Inference (``BayesianGaussianMixture`` class models with
+a Dirichlet process prior).
+
+Both models have access to five components with which to fit the data. Note
+that the Expectation Maximisation model will necessarily use all five
+components while the Variational Inference model will effectively only use as
+many as are needed for a good fit. Here we can see that the Expectation
+Maximisation model splits some components arbitrarily, because it is trying to
+fit too many components, while the Dirichlet Process model adapts it number of
+state automatically.
 
 This example doesn't show it, as we're in a low-dimensional space, but
 another advantage of the Dirichlet process model is that it can fit
@@ -56,7 +58,7 @@ def plot_results(X, Y_, means, covariances, index, title):
         ell.set_alpha(0.5)
         splot.add_artist(ell)
 
-    plt.xlim(-10., 10.)
+    plt.xlim(-9., 5.)
     plt.ylim(-3., 6.)
     plt.xticks(())
     plt.yticks(())
@@ -78,8 +80,9 @@ def plot_results(X, Y_, means, covariances, index, title):
              'Gaussian Mixture')
 
 # Fit a Dirichlet process Gaussian mixture using five components
-dpgmm = mixture.DPGMM(n_components=5, covariance_type='full').fit(X)
-plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm._get_covars(), 1,
-             'Dirichlet Process GMM')
+dpgmm = mixture.BayesianGaussianMixture(n_components=5,
+                                        covariance_type='full').fit(X)
+plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,
+             'Bayesian Gaussian Mixture with a Dirichlet process prior')
 
 plt.show()
diff --git a/examples/mixture/plot_gmm_sin.py b/examples/mixture/plot_gmm_sin.py
index cc9217740a..f5fb2ded45 100755
--- a/examples/mixture/plot_gmm_sin.py
+++ b/examples/mixture/plot_gmm_sin.py
@@ -3,15 +3,40 @@
 Gaussian Mixture Model Sine Curve
 =================================
 
-This example highlights the advantages of the Dirichlet Process:
-complexity control and dealing with sparse data. The dataset is formed
-by 100 points loosely spaced following a noisy sine curve. The fit by
-the GMM class, using the expectation-maximization algorithm to fit a
-mixture of 10 Gaussian components, finds too-small components and very
-little structure. The fits by the Dirichlet process, however, show
-that the model can either learn a global structure for the data (small
-alpha) or easily interpolate to finding relevant local structure
-(large alpha), never falling into the problems shown by the GMM class.
+This example demonstrates the behavior of Gaussian mixture models fit on data
+that was not sampled from a mixture of Gaussian random variables. The dataset
+is formed by 100 points loosely spaced following a noisy sine curve. There is
+therefore no ground truth value for the number of Gaussian components.
+
+The first model is a classical Gaussian Mixture Model with 10 components fit
+with the Expectation-Maximization algorithm.
+
+The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process
+prior fit with variational inference. The low value of the concentration prior
+makes the model favor a lower number of active components. This models
+"decides" to focus its modeling power on the big picture of the structure of
+the dataset: groups of points with alternating directions modeled by
+non-diagonal covariance matrices. Those alternating directions roughly capture
+the alternating nature of the original sine signal.
+
+The third model is also a Bayesian Gaussian mixture model with a Dirichlet
+process prior but this time the value of the concentration prior is higher
+giving the model more liberty to model the fine-grained structure of the data.
+The result is a mixture with a larger number of active components that is
+similar to the first model where we arbitrarily decided to fix the number of
+components to 10.
+
+Which model is the best is a matter of subjective judgement: do we want to
+favor models that only capture the big picture to summarize and explain most of
+the structure of the data while ignoring the details or do we prefer models
+that closely follow the high density regions of the signal?
+
+The last two panels show how we can sample from the last two models. The
+resulting samples distributions do not look exactly like the original data
+distribution. The difference primarily stems from the approximation error we
+made by using a model that assumes that the data was generated by a finite
+number of Gaussian components instead of a continuous noisy sine curve.
+
 """
 
 import itertools
@@ -23,12 +48,14 @@
 
 from sklearn import mixture
 
+print(__doc__)
+
 color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',
                               'darkorange'])
 
 
-def plot_results(X, Y_, means, covariances, index, title):
-    splot = plt.subplot(3, 1, 1 + index)
+def plot_results(X, Y, means, covariances, index, title):
+    splot = plt.subplot(5, 1, 1 + index)
     for i, (mean, covar, color) in enumerate(zip(
             means, covariances, color_iter)):
         v, w = linalg.eigh(covar)
@@ -37,9 +64,9 @@ def plot_results(X, Y_, means, covariances, index, title):
         # as the DP will not use every component it has access to
         # unless it needs it, we shouldn't plot the redundant
         # components.
-        if not np.any(Y_ == i):
+        if not np.any(Y == i):
             continue
-        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
+        plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)
 
         # Plot an ellipse to show the Gaussian component
         angle = np.arctan(u[1] / u[0])
@@ -56,7 +83,24 @@ def plot_results(X, Y_, means, covariances, index, title):
     plt.yticks(())
 
 
-# Number of samples per component
+def plot_samples(X, Y, n_components, index, title):
+    plt.subplot(5, 1, 4 + index)
+    for i, color in zip(range(n_components), color_iter):
+        # as the DP will not use every component it has access to
+        # unless it needs it, we shouldn't plot the redundant
+        # components.
+        if not np.any(Y == i):
+            continue
+        plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)
+
+    plt.xlim(-6., 4. * np.pi - 6.)
+    plt.ylim(-5., 5.)
+    plt.title(title)
+    plt.xticks(())
+    plt.yticks(())
+
+
+# Parameters
 n_samples = 100
 
 # Generate random sample following a sine curve
@@ -69,22 +113,42 @@ def plot_results(X, Y_, means, covariances, index, title):
     X[i, 0] = x + np.random.normal(0, 0.1)
     X[i, 1] = 3. * (np.sin(x) + np.random.normal(0, .2))
 
+plt.figure(figsize=(10, 10))
+plt.subplots_adjust(bottom=.04, top=0.95, hspace=.2, wspace=.05,
+                    left=.03, right=.97)
+
 # Fit a Gaussian mixture with EM using ten components
 gmm = mixture.GaussianMixture(n_components=10, covariance_type='full',
                               max_iter=100).fit(X)
 plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,
              'Expectation-maximization')
 
-# Fit a Dirichlet process Gaussian mixture using ten components
-dpgmm = mixture.DPGMM(n_components=10, covariance_type='full', alpha=0.01,
-                      n_iter=100).fit(X)
-plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm._get_covars(), 1,
-             'Dirichlet Process,alpha=0.01')
-
+dpgmm = mixture.BayesianGaussianMixture(
+    n_components=10, covariance_type='full', weight_concentration_prior=1e-2,
+    weight_concentration_prior_type='dirichlet_process',
+    mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),
+    init_params="random", max_iter=100, random_state=2).fit(X)
+plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,
+             "Bayesian Gaussian mixture models with a Dirichlet process prior "
+             r"for $\gamma_0=0.01$.")
+
+X_s, y_s = dpgmm.sample(n_samples=2000)
+plot_samples(X_s, y_s, dpgmm.n_components, 0,
+             "Gaussian mixture with a Dirichlet process prior "
+             r"for $\gamma_0=0.01$ sampled with $2000$ samples.")
+
+dpgmm = mixture.BayesianGaussianMixture(
+    n_components=10, covariance_type='full', weight_concentration_prior=1e+2,
+    weight_concentration_prior_type='dirichlet_process',
+    mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),
+    init_params="kmeans", max_iter=100, random_state=2).fit(X)
+plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 2,
+             "Bayesian Gaussian mixture models with a Dirichlet process prior "
+             r"for $\gamma_0=100$")
+
+X_s, y_s = dpgmm.sample(n_samples=2000)
+plot_samples(X_s, y_s, dpgmm.n_components, 1,
+             "Gaussian mixture with a Dirichlet process prior "
+             r"for $\gamma_0=100$ sampled with $2000$ samples.")
 
-# Fit a Dirichlet process Gaussian mixture using ten components
-dpgmm = mixture.DPGMM(n_components=10, covariance_type='diag', alpha=100.,
-                      n_iter=100).fit(X)
-plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm._get_covars(), 2,
-             'Dirichlet Process,alpha=100.')
 plt.show()
diff --git a/examples/model_selection/plot_underfitting_overfitting.py b/examples/model_selection/plot_underfitting_overfitting.py
index ff454664c7..538a8eb581 100755
--- a/examples/model_selection/plot_underfitting_overfitting.py
+++ b/examples/model_selection/plot_underfitting_overfitting.py
@@ -52,7 +52,7 @@
 
     # Evaluate the models using crossvalidation
     scores = cross_val_score(pipeline, X[:, np.newaxis], y,
-                             scoring="mean_squared_error", cv=10)
+                             scoring="neg_mean_squared_error", cv=10)
 
     X_test = np.linspace(0, 1, 100)
     plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")
diff --git a/examples/plot_kernel_ridge_regression.py b/examples/plot_kernel_ridge_regression.py
index 0f1943d555..fe372aa0b9 100755
--- a/examples/plot_kernel_ridge_regression.py
+++ b/examples/plot_kernel_ridge_regression.py
@@ -154,14 +154,14 @@
 kr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)
 train_sizes, train_scores_svr, test_scores_svr = \
     learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),
-                   scoring="mean_squared_error", cv=10)
+                   scoring="neg_mean_squared_error", cv=10)
 train_sizes_abs, train_scores_kr, test_scores_kr = \
     learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),
-                   scoring="mean_squared_error", cv=10)
+                   scoring="neg_mean_squared_error", cv=10)
 
-plt.plot(train_sizes, test_scores_svr.mean(1), 'o-', color="r",
+plt.plot(train_sizes, -test_scores_svr.mean(1), 'o-', color="r",
          label="SVR")
-plt.plot(train_sizes, test_scores_kr.mean(1), 'o-', color="g",
+plt.plot(train_sizes, -test_scores_kr.mean(1), 'o-', color="g",
          label="KRR")
 plt.xlabel("Train size")
 plt.ylabel("Mean Squared Error")
diff --git a/sklearn/base.py b/sklearn/base.py
index c2b36887bb..d1628f39b3 100755
--- a/sklearn/base.py
+++ b/sklearn/base.py
@@ -12,6 +12,7 @@
 from .utils.fixes import signature
 from .utils.deprecation import deprecated
 from .exceptions import ChangedBehaviorWarning as _ChangedBehaviorWarning
+from . import __version__
 
 
 @deprecated("ChangedBehaviorWarning has been moved into the sklearn.exceptions"
@@ -296,6 +297,24 @@ def __repr__(self):
         return '%s(%s)' % (class_name, _pprint(self.get_params(deep=False),
                                                offset=len(class_name),),)
 
+    def __getstate__(self):
+        if type(self).__module__.startswith('sklearn.'):
+            return dict(self.__dict__.items(), _sklearn_version=__version__)
+        else:
+            return dict(self.__dict__.items())
+
+    def __setstate__(self, state):
+        if type(self).__module__.startswith('sklearn.'):
+            pickle_version = state.pop("_sklearn_version", "pre-0.18")
+            if pickle_version != __version__:
+                warnings.warn(
+                    "Trying to unpickle estimator {0} from version {1} when "
+                    "using version {2}. This might lead to breaking code or "
+                    "invalid results. Use at your own risk.".format(
+                        self.__class__.__name__, pickle_version, __version__),
+                    UserWarning)
+        self.__dict__.update(state)
+
 
 ###############################################################################
 class ClassifierMixin(object):
diff --git a/sklearn/cross_validation.py b/sklearn/cross_validation.py
index 508b0460ec..010f7106a4 100755
--- a/sklearn/cross_validation.py
+++ b/sklearn/cross_validation.py
@@ -27,6 +27,7 @@
 from .utils.validation import (_is_arraylike, _num_samples,
                                column_or_1d)
 from .utils.multiclass import type_of_target
+from .utils.random import choice
 from .externals.joblib import Parallel, delayed, logger
 from .externals.six import with_metaclass
 from .externals.six.moves import zip
@@ -414,9 +415,9 @@ def __init__(self, labels, n_folds=3):
 
         if n_folds > n_labels:
             raise ValueError(
-                    ("Cannot have number of folds n_folds={0} greater"
-                     " than the number of labels: {1}.").format(n_folds,
-                                                                n_labels))
+                ("Cannot have number of folds n_folds={0} greater"
+                 " than the number of labels: {1}.").format(n_folds,
+                                                            n_labels))
 
         # Weight labels by their number of occurrences
         n_samples_per_label = np.bincount(labels)
@@ -906,6 +907,59 @@ def _validate_shuffle_split(n, test_size, train_size):
     return int(n_train), int(n_test)
 
 
+def _approximate_mode(class_counts, n_draws, rng):
+    """Computes approximate mode of multivariate hypergeometric.
+
+    This is an approximation to the mode of the multivariate
+    hypergeometric given by class_counts and n_draws.
+    It shouldn't be off by more than one.
+
+    It is the mostly likely outcome of drawing n_draws many
+    samples from the population given by class_counts.
+
+    Parameters
+    ----------
+    class_counts : ndarray of int
+        Population per class.
+    n_draws : int
+        Number of draws (samples to draw) from the overall population.
+    rng : random state
+        Used to break ties.
+
+    Returns
+    -------
+    sampled_classes : ndarray of int
+        Number of samples drawn from each class.
+        np.sum(sampled_classes) == n_draws
+    """
+    # this computes a bad approximation to the mode of the
+    # multivariate hypergeometric given by class_counts and n_draws
+    continuous = n_draws * class_counts / class_counts.sum()
+    # floored means we don't overshoot n_samples, but probably undershoot
+    floored = np.floor(continuous)
+    # we add samples according to how much "left over" probability
+    # they had, until we arrive at n_samples
+    need_to_add = int(n_draws - floored.sum())
+    if need_to_add > 0:
+        remainder = continuous - floored
+        values = np.sort(np.unique(remainder))[::-1]
+        # add according to remainder, but break ties
+        # randomly to avoid biases
+        for value in values:
+            inds, = np.where(remainder == value)
+            # if we need_to_add less than what's in inds
+            # we draw randomly from them.
+            # if we need to add more, we add them all and
+            # go to the next value
+            add_now = min(len(inds), need_to_add)
+            inds = choice(inds, size=add_now, replace=False, random_state=rng)
+            floored[inds] += 1
+            need_to_add -= add_now
+            if need_to_add == 0:
+                    break
+    return floored.astype(np.int)
+
+
 class StratifiedShuffleSplit(BaseShuffleSplit):
     """Stratified ShuffleSplit cross validation iterator
 
@@ -991,39 +1045,24 @@ def __init__(self, y, n_iter=10, test_size=0.1, train_size=None,
     def _iter_indices(self):
         rng = check_random_state(self.random_state)
         cls_count = bincount(self.y_indices)
-        p_i = cls_count / float(self.n)
-        n_i = np.round(self.n_train * p_i).astype(int)
-        t_i = np.minimum(cls_count - n_i,
-                         np.round(self.n_test * p_i).astype(int))
 
         for n in range(self.n_iter):
+            # if there are ties in the class-counts, we want
+            # to make sure to break them anew in each iteration
+            n_i = _approximate_mode(cls_count, self.n_train, rng)
+            class_counts_remaining = cls_count - n_i
+            t_i = _approximate_mode(class_counts_remaining, self.n_test, rng)
+
             train = []
             test = []
 
-            for i, cls in enumerate(self.classes):
+            for i, _ in enumerate(self.classes):
                 permutation = rng.permutation(cls_count[i])
-                cls_i = np.where((self.y == cls))[0][permutation]
-
-                train.extend(cls_i[:n_i[i]])
-                test.extend(cls_i[n_i[i]:n_i[i] + t_i[i]])
-
-            # Because of rounding issues (as n_train and n_test are not
-            # dividers of the number of elements per class), we may end
-            # up here with less samples in train and test than asked for.
-            if len(train) + len(test) < self.n_train + self.n_test:
-                # We complete by affecting randomly the missing indexes
-                missing_idx = np.where(bincount(train + test,
-                                                minlength=len(self.y)) == 0,
-                                       )[0]
-                missing_idx = rng.permutation(missing_idx)
-                n_missing_train = self.n_train - len(train)
-                n_missing_test = self.n_test - len(test)
-
-                if n_missing_train > 0:
-                    train.extend(missing_idx[:n_missing_train])
-                if n_missing_test > 0:
-                    test.extend(missing_idx[-n_missing_test:])
+                perm_indices_class_i = np.where(
+                    (i == self.y_indices))[0][permutation]
 
+                train.extend(perm_indices_class_i[:n_i[i]])
+                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])
             train = rng.permutation(train)
             test = rng.permutation(test)
 
diff --git a/sklearn/decomposition/pca.py b/sklearn/decomposition/pca.py
index aecab027b7..2a6f0dd013 100755
--- a/sklearn/decomposition/pca.py
+++ b/sklearn/decomposition/pca.py
@@ -108,9 +108,12 @@ class PCA(_BasePCA):
     Linear dimensionality reduction using Singular Value Decomposition of the
     data to project it to a lower dimensional space.
 
-    It uses the scipy.linalg ARPACK implementation of the SVD or a randomized
-    SVD by the method of Halko et al. 2009, depending on which is the most
-    efficient.
+    It uses the LAPACK implementation of the full SVD or a randomized truncated
+    SVD by the method of Halko et al. 2009, depending on the shape of the input
+    data and the number of components to extract.
+
+    It can also use the scipy.sparse.linalg ARPACK implementation of the
+    truncated SVD.
 
     Read more in the :ref:`User Guide <PCA>`.
 
@@ -147,10 +150,13 @@ class PCA(_BasePCA):
     svd_solver : string {'auto', 'full', 'arpack', 'randomized'}
         auto :
             the solver is selected by a default policy based on `X.shape` and
-            `n_components` which favors 'randomized' when the problem is
-            computationally demanding for 'full' PCA
+            `n_components`: if the input data is larger than 500x500 and the
+            number of components to extract is lower than 80% of the smallest
+            dimension of the data, then then more efficient 'randomized'
+            method is enabled. Otherwise the exact full SVD is computed and
+            optionally truncated afterwards.
         full :
-            run exact SVD calling ARPACK solver via
+            run exact full SVD calling the standard LAPACK solver via
             `scipy.linalg.svd` and select the components by postprocessing
         arpack :
             run SVD truncated to n_components calling ARPACK solver via
diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py
index 827a2ef2da..0585438e87 100755
--- a/sklearn/isotonic.py
+++ b/sklearn/isotonic.py
@@ -412,8 +412,7 @@ def predict(self, T):
 
     def __getstate__(self):
         """Pickle-protocol - return state of the estimator. """
-        # copy __dict__
-        state = dict(self.__dict__)
+        state = super(IsotonicRegression, self).__getstate__()
         # remove interpolation method
         state.pop('f_', None)
         return state
@@ -423,6 +422,6 @@ def __setstate__(self, state):
 
         We need to rebuild the interpolation function.
         """
-        self.__dict__.update(state)
+        super(IsotonicRegression, self).__setstate__(state)
         if hasattr(self, '_necessary_X_') and hasattr(self, '_necessary_y_'):
             self._build_f(self._necessary_X_, self._necessary_y_)
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index ff54bba971..fefb914d76 100755
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -352,7 +352,7 @@ def _test_ridge_loo(filter_):
     assert_equal(ridge_gcv3.alpha_, alpha_)
 
     # check that we get same best alpha with a scorer
-    scorer = get_scorer('mean_squared_error')
+    scorer = get_scorer('neg_mean_squared_error')
     ridge_gcv4 = RidgeCV(fit_intercept=False, scoring=scorer)
     ridge_gcv4.fit(filter_(X_diabetes), y_diabetes)
     assert_equal(ridge_gcv4.alpha_, alpha_)
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
index 50d3d3b752..2a7be716ee 100755
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -623,9 +623,10 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
            parameter *labels* improved for multiclass problem.
 
     pos_label : str or int, 1 by default
-        The class to report if ``average='binary'``. Until version 0.18 it is
-        necessary to set ``pos_label=None`` if seeking to use another averaging
-        method over binary targets.
+        The class to report if ``average='binary'`` and the data is binary.
+        If the data are multiclass or multilabel, this will be ignored;
+        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report
+        scores for that label only.
 
     average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \
                        'weighted']
@@ -652,10 +653,6 @@ def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
             meaningful for multilabel classification where this differs from
             :func:`accuracy_score`).
 
-        Note that if ``pos_label`` is given in binary classification with
-        `average != 'binary'`, only that positive class is reported. This
-        behavior is deprecated and will change in version 0.18.
-
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
@@ -729,9 +726,10 @@ def fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,
            parameter *labels* improved for multiclass problem.
 
     pos_label : str or int, 1 by default
-        The class to report if ``average='binary'``. Until version 0.18 it is
-        necessary to set ``pos_label=None`` if seeking to use another averaging
-        method over binary targets.
+        The class to report if ``average='binary'`` and the data is binary.
+        If the data are multiclass or multilabel, this will be ignored;
+        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report
+        scores for that label only.
 
     average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \
                        'weighted']
@@ -758,10 +756,6 @@ def fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,
             meaningful for multilabel classification where this differs from
             :func:`accuracy_score`).
 
-        Note that if ``pos_label`` is given in binary classification with
-        `average != 'binary'`, only that positive class is reported. This
-        behavior is deprecated and will change in version 0.18.
-
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
@@ -905,9 +899,10 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
         ``y_pred`` are used in sorted order.
 
     pos_label : str or int, 1 by default
-        The class to report if ``average='binary'``. Until version 0.18 it is
-        necessary to set ``pos_label=None`` if seeking to use another averaging
-        method over binary targets.
+        The class to report if ``average='binary'`` and the data is binary.
+        If the data are multiclass or multilabel, this will be ignored;
+        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report
+        scores for that label only.
 
     average : string, [None (default), 'binary', 'micro', 'macro', 'samples', \
                        'weighted']
@@ -933,10 +928,6 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
             meaningful for multilabel classification where this differs from
             :func:`accuracy_score`).
 
-        Note that if ``pos_label`` is given in binary classification with
-        `average != 'binary'`, only that positive class is reported. This
-        behavior is deprecated and will change in version 0.18.
-
     warn_for : tuple or set, for internal use
         This determines which warnings will be made in the case that this
         function is being used to return only one of its metrics.
@@ -1008,25 +999,8 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
     y_type, y_true, y_pred = _check_targets(y_true, y_pred)
     present_labels = unique_labels(y_true, y_pred)
 
-    if average == 'binary' and (y_type != 'binary' or pos_label is None):
-        warnings.warn('The default `weighted` averaging is deprecated, '
-                      'and from version 0.18, use of precision, recall or '
-                      'F-score with multiclass or multilabel data or '
-                      'pos_label=None will result in an exception. '
-                      'Please set an explicit value for `average`, one of '
-                      '%s. In cross validation use, for instance, '
-                      'scoring="f1_weighted" instead of scoring="f1".'
-                      % str(average_options), DeprecationWarning, stacklevel=2)
-        average = 'weighted'
-
-    if y_type == 'binary' and pos_label is not None and average is not None:
-        if average != 'binary':
-            warnings.warn('From version 0.18, binary input will not be '
-                          'handled specially when using averaged '
-                          'precision/recall/F-score. '
-                          'Please use average=\'binary\' to report only the '
-                          'positive class performance.', DeprecationWarning)
-        if labels is None or len(labels) <= 2:
+    if average == 'binary':
+        if y_type == 'binary':
             if pos_label not in present_labels:
                 if len(present_labels) < 2:
                     # Only negative labels
@@ -1035,6 +1009,15 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,
                     raise ValueError("pos_label=%r is not a valid label: %r" %
                                      (pos_label, present_labels))
             labels = [pos_label]
+        else:
+            raise ValueError("Target is %s but average='binary'. Please "
+                             "choose another average setting." % y_type)
+    elif pos_label not in (None, 1):
+        warnings.warn("Note that pos_label (set to %r) is ignored when "
+                      "average != 'binary' (got %r). You may use "
+                      "labels=[pos_label] to specify a single positive class."
+                      % (pos_label, average), UserWarning)
+
     if labels is None:
         labels = present_labels
         n_labels = None
@@ -1187,9 +1170,10 @@ def precision_score(y_true, y_pred, labels=None, pos_label=1,
            parameter *labels* improved for multiclass problem.
 
     pos_label : str or int, 1 by default
-        The class to report if ``average='binary'``. Until version 0.18 it is
-        necessary to set ``pos_label=None`` if seeking to use another averaging
-        method over binary targets.
+        The class to report if ``average='binary'`` and the data is binary.
+        If the data are multiclass or multilabel, this will be ignored;
+        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report
+        scores for that label only.
 
     average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \
                        'weighted']
@@ -1216,10 +1200,6 @@ def precision_score(y_true, y_pred, labels=None, pos_label=1,
             meaningful for multilabel classification where this differs from
             :func:`accuracy_score`).
 
-        Note that if ``pos_label`` is given in binary classification with
-        `average != 'binary'`, only that positive class is reported. This
-        behavior is deprecated and will change in version 0.18.
-
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
@@ -1289,9 +1269,10 @@ def recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
            parameter *labels* improved for multiclass problem.
 
     pos_label : str or int, 1 by default
-        The class to report if ``average='binary'``. Until version 0.18 it is
-        necessary to set ``pos_label=None`` if seeking to use another averaging
-        method over binary targets.
+        The class to report if ``average='binary'`` and the data is binary.
+        If the data are multiclass or multilabel, this will be ignored;
+        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report
+        scores for that label only.
 
     average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \
                        'weighted']
@@ -1318,10 +1299,6 @@ def recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary',
             meaningful for multilabel classification where this differs from
             :func:`accuracy_score`).
 
-        Note that if ``pos_label`` is given in binary classification with
-        `average != 'binary'`, only that positive class is reported. This
-        behavior is deprecated and will change in version 0.18.
-
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index 03bee6e506..12f35d2adf 100755
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -27,7 +27,7 @@
 from ..utils import check_consistent_length
 from ..utils import column_or_1d, check_array
 from ..utils.multiclass import type_of_target
-from ..utils.fixes import isclose
+from ..utils.extmath import stable_cumsum
 from ..utils.fixes import bincount
 from ..utils.fixes import array_equal
 from ..utils.stats import rankdata
@@ -330,16 +330,13 @@ def _binary_clf_curve(y_true, y_score, pos_label=None, sample_weight=None):
     # y_score typically has many tied values. Here we extract
     # the indices associated with the distinct values. We also
     # concatenate a value for the end of the curve.
-    # We need to use isclose to avoid spurious repeated thresholds
-    # stemming from floating point roundoff errors.
-    distinct_value_indices = np.where(np.logical_not(isclose(
-        np.diff(y_score), 0)))[0]
+    distinct_value_indices = np.where(np.diff(y_score))[0]
     threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]
 
     # accumulate the true positives with decreasing threshold
-    tps = (y_true * weight).cumsum()[threshold_idxs]
+    tps = stable_cumsum(y_true * weight)[threshold_idxs]
     if sample_weight is not None:
-        fps = weight.cumsum()[threshold_idxs] - tps
+        fps = stable_cumsum(weight)[threshold_idxs] - tps
     else:
         fps = 1 + threshold_idxs - tps
     return fps, tps, y_score[threshold_idxs]
diff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py
index 83975f925e..324c3ce658 100755
--- a/sklearn/metrics/scorer.py
+++ b/sklearn/metrics/scorer.py
@@ -19,6 +19,7 @@
 # License: Simplified BSD
 
 from abc import ABCMeta, abstractmethod
+import warnings
 
 import numpy as np
 
@@ -37,10 +38,16 @@ def __init__(self, score_func, sign, kwargs):
         self._kwargs = kwargs
         self._score_func = score_func
         self._sign = sign
+        # XXX After removing the deprecated scorers (v0.20) remove the
+        # XXX deprecation_msg property again and remove __call__'s body again
+        self._deprecation_msg = None
 
     @abstractmethod
     def __call__(self, estimator, X, y, sample_weight=None):
-        pass
+        if self._deprecation_msg is not None:
+            warnings.warn(self._deprecation_msg,
+                          category=DeprecationWarning,
+                          stacklevel=2)
 
     def __repr__(self):
         kwargs_string = "".join([", %s=%s" % (str(k), str(v))
@@ -79,6 +86,8 @@ def __call__(self, estimator, X, y_true, sample_weight=None):
         score : float
             Score function applied to prediction of estimator on X.
         """
+        super(_PredictScorer, self).__call__(estimator, X, y_true,
+                                             sample_weight=sample_weight)
         y_pred = estimator.predict(X)
         if sample_weight is not None:
             return self._sign * self._score_func(y_true, y_pred,
@@ -114,6 +123,8 @@ def __call__(self, clf, X, y, sample_weight=None):
         score : float
             Score function applied to prediction of estimator on X.
         """
+        super(_ProbaScorer, self).__call__(clf, X, y,
+                                           sample_weight=sample_weight)
         y_pred = clf.predict_proba(X)
         if sample_weight is not None:
             return self._sign * self._score_func(y, y_pred,
@@ -153,6 +164,8 @@ def __call__(self, clf, X, y, sample_weight=None):
         score : float
             Score function applied to prediction of estimator on X.
         """
+        super(_ThresholdScorer, self).__call__(clf, X, y,
+                                               sample_weight=sample_weight)
         y_type = type_of_target(y)
         if y_type not in ("binary", "multilabel-indicator"):
             raise ValueError("{0} format is not supported".format(y_type))
@@ -191,9 +204,11 @@ def get_scorer(scoring):
         try:
             scorer = SCORERS[scoring]
         except KeyError:
+            scorers = [scorer for scorer in SCORERS
+                       if SCORERS[scorer]._deprecation_msg is None]
             raise ValueError('%r is not a valid scoring value. '
                              'Valid options are %s'
-                             % (scoring, sorted(SCORERS.keys())))
+                             % (scoring, sorted(scorers)))
     else:
         scorer = scoring
     return scorer
@@ -313,12 +328,31 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,
 
 # Standard regression scores
 r2_scorer = make_scorer(r2_score)
+neg_mean_squared_error_scorer = make_scorer(mean_squared_error,
+                                            greater_is_better=False)
+deprecation_msg = ('Scoring method mean_squared_error was renamed to '
+                   'neg_mean_squared_error in version 0.18 and will '
+                   'be removed in 0.20.')
 mean_squared_error_scorer = make_scorer(mean_squared_error,
                                         greater_is_better=False)
+mean_squared_error_scorer._deprecation_msg = deprecation_msg
+neg_mean_absolute_error_scorer = make_scorer(mean_absolute_error,
+                                             greater_is_better=False)
+deprecation_msg = ('Scoring method mean_absolute_error was renamed to '
+                   'neg_mean_absolute_error in version 0.18 and will '
+                   'be removed in 0.20.')
 mean_absolute_error_scorer = make_scorer(mean_absolute_error,
                                          greater_is_better=False)
+mean_absolute_error_scorer._deprecation_msg = deprecation_msg
+neg_median_absolute_error_scorer = make_scorer(median_absolute_error,
+                                               greater_is_better=False)
+deprecation_msg = ('Scoring method median_absolute_error was renamed to '
+                   'neg_median_absolute_error in version 0.18 and will '
+                   'be removed in 0.20.')
 median_absolute_error_scorer = make_scorer(median_absolute_error,
                                            greater_is_better=False)
+median_absolute_error_scorer._deprecation_msg = deprecation_msg
+
 
 # Standard Classification Scores
 accuracy_scorer = make_scorer(accuracy_score)
@@ -333,19 +367,29 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,
 recall_scorer = make_scorer(recall_score)
 
 # Score function for probabilistic classification
+neg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False,
+                                  needs_proba=True)
+deprecation_msg = ('Scoring method log_loss was renamed to '
+                   'neg_log_loss in version 0.18 and will be removed in 0.20.')
 log_loss_scorer = make_scorer(log_loss, greater_is_better=False,
                               needs_proba=True)
+log_loss_scorer._deprecation_msg = deprecation_msg
+
 
 # Clustering scores
 adjusted_rand_scorer = make_scorer(adjusted_rand_score)
 
 SCORERS = dict(r2=r2_scorer,
+               neg_median_absolute_error=neg_median_absolute_error_scorer,
+               neg_mean_absolute_error=neg_mean_absolute_error_scorer,
+               neg_mean_squared_error=neg_mean_squared_error_scorer,
                median_absolute_error=median_absolute_error_scorer,
                mean_absolute_error=mean_absolute_error_scorer,
                mean_squared_error=mean_squared_error_scorer,
                accuracy=accuracy_scorer, roc_auc=roc_auc_scorer,
                average_precision=average_precision_scorer,
                log_loss=log_loss_scorer,
+               neg_log_loss=neg_log_loss_scorer,
                adjusted_rand_score=adjusted_rand_scorer)
 
 for name, metric in [('precision', precision_score),
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
index 576c3a362b..dc8a7c0686 100755
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -130,10 +130,8 @@ def test_precision_recall_f1_score_binary():
     # individual scoring function that can be used for grid search: in the
     # binary class case the score is the value of the measure for the positive
     # class (e.g. label == 1). This is deprecated for average != 'binary'.
-    assert_dep_warning = partial(assert_warns, DeprecationWarning)
     for kwargs, my_assert in [({}, assert_no_warnings),
-                              ({'average': 'binary'}, assert_no_warnings),
-                              ({'average': 'micro'}, assert_dep_warning)]:
+                              ({'average': 'binary'}, assert_no_warnings)]:
         ps = my_assert(precision_score, y_true, y_pred, **kwargs)
         assert_array_almost_equal(ps, 0.85, 2)
 
@@ -273,13 +271,24 @@ def test_precision_recall_fscore_support_errors():
 
     # Bad pos_label
     assert_raises(ValueError, precision_recall_fscore_support,
-                  y_true, y_pred, pos_label=2, average='macro')
+                  y_true, y_pred, pos_label=2, average='binary')
 
     # Bad average option
     assert_raises(ValueError, precision_recall_fscore_support,
                   [0, 1, 2], [1, 2, 0], average='mega')
 
 
+def test_precision_recall_f_unused_pos_label():
+    # Check warning that pos_label unused when set to non-default value
+    # but average != 'binary'; even if data is binary.
+    assert_warns_message(UserWarning,
+                         "Note that pos_label (set to 2) is "
+                         "ignored when average != 'binary' (got 'macro'). You "
+                         "may use labels=[pos_label] to specify a single "
+                         "positive class.", precision_recall_fscore_support,
+                         [1, 2, 1], [1, 2, 2], pos_label=2, average='macro')
+
+
 def test_confusion_matrix_binary():
     # Test confusion matrix - binary classification case
     y_true, y_pred, _ = make_prediction(binary=True)
@@ -458,17 +467,24 @@ def test_precision_refcall_f1_score_multilabel_unordered_labels():
             assert_array_equal(s, [0, 1, 1, 0])
 
 
-def test_precision_recall_f1_score_multiclass_pos_label_none():
-    # Test Precision Recall and F1 Score for multiclass classification task
-    # GH Issue #1296
-    # initialize data
+def test_precision_recall_f1_score_binary_averaged():
     y_true = np.array([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1])
     y_pred = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1])
 
     # compute scores with default labels introspection
-    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,
-                                                 pos_label=None,
+    ps, rs, fs, _ = precision_recall_fscore_support(y_true, y_pred,
+                                                    average=None)
+    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred,
                                                  average='macro')
+    assert_equal(p, np.mean(ps))
+    assert_equal(r, np.mean(rs))
+    assert_equal(f, np.mean(fs))
+    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred,
+                                                 average='weighted')
+    support = np.bincount(y_true)
+    assert_equal(p, np.average(ps, weights=support))
+    assert_equal(r, np.average(rs, weights=support))
+    assert_equal(f, np.average(fs, weights=support))
 
 
 def test_zero_precision_recall():
@@ -1041,37 +1057,37 @@ def test_prf_warnings():
                'being set to 0.0 in labels with no true samples.')
         my_assert(w, msg, f, [1, 1, 2], [0, 1, 2], average=average)
 
-        # average of per-sample scores
-        msg = ('Precision and F-score are ill-defined and '
-               'being set to 0.0 in samples with no predicted labels.')
-        my_assert(w, msg, f, np.array([[1, 0], [1, 0]]),
-                  np.array([[1, 0], [0, 0]]), average='samples')
+    # average of per-sample scores
+    msg = ('Precision and F-score are ill-defined and '
+           'being set to 0.0 in samples with no predicted labels.')
+    my_assert(w, msg, f, np.array([[1, 0], [1, 0]]),
+              np.array([[1, 0], [0, 0]]), average='samples')
 
-        msg = ('Recall and F-score are ill-defined and '
-               'being set to 0.0 in samples with no true labels.')
-        my_assert(w, msg, f, np.array([[1, 0], [0, 0]]),
-                  np.array([[1, 0], [1, 0]]),
-                  average='samples')
+    msg = ('Recall and F-score are ill-defined and '
+           'being set to 0.0 in samples with no true labels.')
+    my_assert(w, msg, f, np.array([[1, 0], [0, 0]]),
+              np.array([[1, 0], [1, 0]]),
+              average='samples')
 
-        # single score: micro-average
-        msg = ('Precision and F-score are ill-defined and '
-               'being set to 0.0 due to no predicted samples.')
-        my_assert(w, msg, f, np.array([[1, 1], [1, 1]]),
-                  np.array([[0, 0], [0, 0]]), average='micro')
+    # single score: micro-average
+    msg = ('Precision and F-score are ill-defined and '
+           'being set to 0.0 due to no predicted samples.')
+    my_assert(w, msg, f, np.array([[1, 1], [1, 1]]),
+              np.array([[0, 0], [0, 0]]), average='micro')
 
-        msg = ('Recall and F-score are ill-defined and '
-               'being set to 0.0 due to no true samples.')
-        my_assert(w, msg, f, np.array([[0, 0], [0, 0]]),
-                  np.array([[1, 1], [1, 1]]), average='micro')
+    msg = ('Recall and F-score are ill-defined and '
+           'being set to 0.0 due to no true samples.')
+    my_assert(w, msg, f, np.array([[0, 0], [0, 0]]),
+              np.array([[1, 1], [1, 1]]), average='micro')
 
-        # single postive label
-        msg = ('Precision and F-score are ill-defined and '
-               'being set to 0.0 due to no predicted samples.')
-        my_assert(w, msg, f, [1, 1], [-1, -1], average='macro')
+    # single postive label
+    msg = ('Precision and F-score are ill-defined and '
+           'being set to 0.0 due to no predicted samples.')
+    my_assert(w, msg, f, [1, 1], [-1, -1], average='binary')
 
-        msg = ('Recall and F-score are ill-defined and '
-               'being set to 0.0 due to no true samples.')
-        my_assert(w, msg, f, [-1, -1], [1, 1], average='macro')
+    msg = ('Recall and F-score are ill-defined and '
+           'being set to 0.0 due to no true samples.')
+    my_assert(w, msg, f, [-1, -1], [1, 1], average='binary')
 
 
 def test_recall_warnings():
@@ -1128,32 +1144,23 @@ def test_fscore_warnings():
                          'being set to 0.0 due to no true samples.')
 
 
-def test_prf_average_compat():
-    # Ensure warning if f1_score et al.'s average is implicit for multiclass
-    y_true = [1, 2, 3, 3]
-    y_pred = [1, 2, 3, 1]
-    y_true_bin = [0, 1, 1]
-    y_pred_bin = [0, 1, 0]
-
-    for metric in [precision_score, recall_score, f1_score,
-                   partial(fbeta_score, beta=2)]:
-        score = assert_warns(DeprecationWarning, metric, y_true, y_pred)
-        score_weighted = assert_no_warnings(metric, y_true, y_pred,
-                                            average='weighted')
-        assert_equal(score, score_weighted,
-                     'average does not act like "weighted" by default')
-
-        # check binary passes without warning
-        assert_no_warnings(metric, y_true_bin, y_pred_bin)
-
-        # but binary with pos_label=None should behave like multiclass
-        score = assert_warns(DeprecationWarning, metric,
-                             y_true_bin, y_pred_bin, pos_label=None)
-        score_weighted = assert_no_warnings(metric, y_true_bin, y_pred_bin,
-                                            pos_label=None, average='weighted')
-        assert_equal(score, score_weighted,
-                     'average does not act like "weighted" by default with '
-                     'binary data and pos_label=None')
+def test_prf_average_binary_data_non_binary():
+    # Error if user does not explicitly set non-binary average mode
+    y_true_mc = [1, 2, 3, 3]
+    y_pred_mc = [1, 2, 3, 1]
+    y_true_ind = np.array([[0, 1, 1], [1, 0, 0], [0, 0, 1]])
+    y_pred_ind = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])
+
+    for y_true, y_pred, y_type in [
+        (y_true_mc, y_pred_mc, 'multiclass'),
+        (y_true_ind, y_pred_ind, 'multilabel-indicator'),
+    ]:
+        for metric in [precision_score, recall_score, f1_score,
+                       partial(fbeta_score, beta=2)]:
+            assert_raise_message(ValueError,
+                                 "Target is %s but average='binary'. Please "
+                                 "choose another average setting." % y_type,
+                                 metric, y_true, y_pred)
 
 
 def test__check_targets():
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index 4974d29356..fa4c7e8d31 100755
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -217,6 +217,13 @@
 METRIC_UNDEFINED_MULTICLASS = [
     "brier_score_loss",
     "matthews_corrcoef_score",
+
+    # with default average='binary', multiclass is prohibited
+    "precision_score",
+    "recall_score",
+    "f1_score",
+    "f2_score",
+    "f0.5_score",
 ]
 
 # Metric undefined with "binary" or "multiclass" input
@@ -303,17 +310,15 @@
     "jaccard_similarity_score", "unnormalized_jaccard_similarity_score",
     "zero_one_loss", "unnormalized_zero_one_loss",
 
-    "precision_score", "recall_score", "f1_score", "f2_score", "f0.5_score",
-
     "weighted_f0.5_score", "weighted_f1_score", "weighted_f2_score",
     "weighted_precision_score", "weighted_recall_score",
 
-    "micro_f0.5_score", "micro_f1_score", "micro_f2_score",
-    "micro_precision_score", "micro_recall_score",
-
     "macro_f0.5_score", "macro_f1_score", "macro_f2_score",
     "macro_precision_score", "macro_recall_score",
 
+    "micro_f0.5_score", "micro_f1_score", "micro_f2_score",
+    "micro_precision_score", "micro_recall_score",
+
     "samples_f0.5_score", "samples_f1_score", "samples_f2_score",
     "samples_precision_score", "samples_recall_score",
 ]
@@ -332,7 +337,11 @@
     "jaccard_similarity_score", "unnormalized_jaccard_similarity_score",
     "zero_one_loss", "unnormalized_zero_one_loss",
 
-    "f1_score", "weighted_f1_score", "micro_f1_score", "macro_f1_score",
+    "f1_score", "micro_f1_score", "macro_f1_score",
+    "weighted_recall_score",
+    # P = R = F = accuracy in multiclass case
+    "micro_f0.5_score", "micro_f1_score", "micro_f2_score",
+    "micro_precision_score", "micro_recall_score",
 
     "matthews_corrcoef_score", "mean_absolute_error", "mean_squared_error",
     "median_absolute_error",
@@ -349,11 +358,8 @@
 
     "precision_score", "recall_score", "f2_score", "f0.5_score",
 
-    "weighted_f0.5_score", "weighted_f2_score", "weighted_precision_score",
-    "weighted_recall_score",
-
-    "micro_f0.5_score", "micro_f2_score", "micro_precision_score",
-    "micro_recall_score",
+    "weighted_f0.5_score", "weighted_f1_score", "weighted_f2_score",
+    "weighted_precision_score",
 
     "macro_f0.5_score", "macro_f2_score", "macro_precision_score",
     "macro_recall_score", "log_loss", "hinge_loss"
@@ -382,7 +388,8 @@ def test_symmetry():
     # We shouldn't forget any metrics
     assert_equal(set(SYMMETRIC_METRICS).union(
         NOT_SYMMETRIC_METRICS, THRESHOLDED_METRICS,
-        METRIC_UNDEFINED_BINARY_MULTICLASS), set(ALL_METRICS))
+        METRIC_UNDEFINED_BINARY_MULTICLASS),
+        set(ALL_METRICS))
 
     assert_equal(
         set(SYMMETRIC_METRICS).intersection(set(NOT_SYMMETRIC_METRICS)),
@@ -1062,6 +1069,7 @@ def test_sample_weight_invariance(n_samples=50):
                    y_pred)
 
 
+@ignore_warnings
 def test_no_averaging_labels():
     # test labels argument when not using averaging
     # in multi-class and multi-label cases
@@ -1075,7 +1083,7 @@ def test_no_averaging_labels():
     for name in METRICS_WITH_AVERAGING:
         for y_true, y_pred in [[y_true_multiclass, y_pred_multiclass],
                                [y_true_multilabel, y_pred_multilabel]]:
-            if name not in MULTILABELS_METRICS and y_pred.shape[1] > 0:
+            if name not in MULTILABELS_METRICS and y_pred.ndim > 1:
                 continue
 
             metric = ALL_METRICS[name]
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index 19739d720d..0ba1d858ab 100755
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -7,7 +7,6 @@
 
 from sklearn import datasets
 from sklearn import svm
-from sklearn import ensemble
 
 from sklearn.datasets import make_multilabel_classification
 from sklearn.random_projection import sparse_random_matrix
@@ -170,29 +169,6 @@ def test_roc_returns_consistency():
     assert_equal(fpr.shape, thresholds.shape)
 
 
-def test_roc_nonrepeating_thresholds():
-    # Test to ensure that we don't return spurious repeating thresholds.
-    # Duplicated thresholds can arise due to machine precision issues.
-    dataset = datasets.load_digits()
-    X = dataset['data']
-    y = dataset['target']
-
-    # This random forest classifier can only return probabilities
-    # significant to two decimal places
-    clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=0)
-
-    # How well can the classifier predict whether a digit is less than 5?
-    # This task contributes floating point roundoff errors to the probabilities
-    train, test = slice(None, None, 2), slice(1, None, 2)
-    probas_pred = clf.fit(X[train], y[train]).predict_proba(X[test])
-    y_score = probas_pred[:, :5].sum(axis=1)  # roundoff errors begin here
-    y_true = [yy < 5 for yy in y[test]]
-
-    # Check for repeating values in the thresholds
-    fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=False)
-    assert_equal(thresholds.size, np.unique(np.round(thresholds, 2)).size)
-
-
 def test_roc_curve_multi():
     # roc_curve not applicable for multi-class problems
     y_true, _, probas_pred = make_prediction(binary=False)
@@ -621,18 +597,25 @@ def test_precision_recall_curve_toydata():
 def test_score_scale_invariance():
     # Test that average_precision_score and roc_auc_score are invariant by
     # the scaling or shifting of probabilities
+    # This test was expanded (added scaled_down) in response to github
+    # issue #3864 (and others), where overly aggressive rounding was causing
+    # problems for users with very small y_score values
     y_true, _, probas_pred = make_prediction(binary=True)
 
     roc_auc = roc_auc_score(y_true, probas_pred)
-    roc_auc_scaled = roc_auc_score(y_true, 100 * probas_pred)
+    roc_auc_scaled_up = roc_auc_score(y_true, 100 * probas_pred)
+    roc_auc_scaled_down = roc_auc_score(y_true, 1e-6 * probas_pred)
     roc_auc_shifted = roc_auc_score(y_true, probas_pred - 10)
-    assert_equal(roc_auc, roc_auc_scaled)
+    assert_equal(roc_auc, roc_auc_scaled_up)
+    assert_equal(roc_auc, roc_auc_scaled_down)
     assert_equal(roc_auc, roc_auc_shifted)
 
     pr_auc = average_precision_score(y_true, probas_pred)
-    pr_auc_scaled = average_precision_score(y_true, 100 * probas_pred)
+    pr_auc_scaled_up = average_precision_score(y_true, 100 * probas_pred)
+    pr_auc_scaled_down = average_precision_score(y_true, 1e-6 * probas_pred)
     pr_auc_shifted = average_precision_score(y_true, probas_pred - 10)
-    assert_equal(pr_auc, pr_auc_scaled)
+    assert_equal(pr_auc, pr_auc_scaled_up)
+    assert_equal(pr_auc, pr_auc_scaled_down)
     assert_equal(pr_auc, pr_auc_shifted)
 
 
diff --git a/sklearn/metrics/tests/test_regression.py b/sklearn/metrics/tests/test_regression.py
index 046a01cd4c..600bcc135a 100755
--- a/sklearn/metrics/tests/test_regression.py
+++ b/sklearn/metrics/tests/test_regression.py
@@ -47,7 +47,6 @@ def test_multioutput_regression():
     assert_almost_equal(error, -.875)
 
 
-
 def test_regression_metrics_at_limits():
     assert_almost_equal(mean_squared_error([0.], [0.]), 0.00, 2)
     assert_almost_equal(mean_absolute_error([0.], [0.]), 0.00, 2)
diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py
index df5f0de3d2..c3643e8103 100755
--- a/sklearn/metrics/tests/test_score_objects.py
+++ b/sklearn/metrics/tests/test_score_objects.py
@@ -13,6 +13,7 @@
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import assert_not_equal
+from sklearn.utils.testing import assert_warns_message
 
 from sklearn.base import BaseEstimator
 from sklearn.metrics import (f1_score, r2_score, roc_auc_score, fbeta_score,
@@ -37,14 +38,16 @@
 from sklearn.externals import joblib
 
 
-REGRESSION_SCORERS = ['r2', 'mean_absolute_error', 'mean_squared_error',
-                      'median_absolute_error']
+REGRESSION_SCORERS = ['r2', 'neg_mean_absolute_error',
+                      'neg_mean_squared_error', 'neg_median_absolute_error',
+                      'mean_absolute_error',
+                      'mean_squared_error', 'median_absolute_error']
 
 CLF_SCORERS = ['accuracy', 'f1', 'f1_weighted', 'f1_macro', 'f1_micro',
                'roc_auc', 'average_precision', 'precision',
                'precision_weighted', 'precision_macro', 'precision_micro',
                'recall', 'recall_weighted', 'recall_macro', 'recall_micro',
-               'log_loss',
+               'neg_log_loss', 'log_loss',
                'adjusted_rand_score'  # not really, but works
                ]
 
@@ -259,7 +262,7 @@ def test_thresholded_scorers():
     assert_almost_equal(score1, score2)
     assert_almost_equal(score1, score3)
 
-    logscore = get_scorer('log_loss')(clf, X_test, y_test)
+    logscore = get_scorer('neg_log_loss')(clf, X_test, y_test)
     logloss = log_loss(y_test, clf.predict_proba(X_test))
     assert_almost_equal(-logscore, logloss)
 
@@ -413,3 +416,22 @@ def test_scorer_memmap_input():
     # float values.
     for name in SCORERS.keys():
         yield check_scorer_memmap, name
+
+
+def test_deprecated_names():
+    X, y = make_blobs(random_state=0, centers=2)
+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
+    clf = LogisticRegression(random_state=0)
+    clf.fit(X_train, y_train)
+
+    for name in ('mean_absolute_error', 'mean_squared_error',
+                 'median_absolute_error', 'log_loss'):
+        warning_msg = "Scoring method %s was renamed to" % name
+        for scorer in (get_scorer(name), SCORERS[name]):
+            assert_warns_message(DeprecationWarning,
+                                 warning_msg,
+                                 scorer, clf, X, y)
+
+        assert_warns_message(DeprecationWarning,
+                             warning_msg,
+                             cross_val_score, clf, X, y, scoring=name)
diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index 062d403228..38ee12e925 100755
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -130,15 +130,17 @@ def _check_parameters(self, X):
         """
         pass
 
-    def _initialize_parameters(self, X):
+    def _initialize_parameters(self, X, random_state):
         """Initialize the model parameters.
 
         Parameters
         ----------
         X : array-like, shape  (n_samples, n_features)
+
+        random_state: RandomState
+            A random number generator instance.
         """
         n_samples, _ = X.shape
-        random_state = check_random_state(self.random_state)
 
         if self.init_params == 'kmeans':
             resp = np.zeros((n_samples, self.n_components))
@@ -195,12 +197,14 @@ def fit(self, X, y=None):
         max_lower_bound = -np.infty
         self.converged_ = False
 
+        random_state = check_random_state(self.random_state)
+
         n_samples, _ = X.shape
         for init in range(n_init):
             self._print_verbose_msg_init_beg(init)
 
             if do_init:
-                self._initialize_parameters(X)
+                self._initialize_parameters(X, random_state)
                 self.lower_bound_ = -np.infty
 
             for n_iter in range(self.max_iter):
@@ -228,7 +232,7 @@ def fit(self, X, y=None):
         if not self.converged_:
             warnings.warn('Initialization %d did not converged. '
                           'Try different init parameters, '
-                          'or increase n_init, tol '
+                          'or increase max_iter, tol '
                           'or check for degenerate data.'
                           % (init + 1), ConvergenceWarning)
 
@@ -355,6 +359,51 @@ def predict_proba(self, X):
         _, log_resp = self._estimate_log_prob_resp(X)
         return np.exp(log_resp)
 
+    def sample(self, n_samples=1):
+        """Generate random samples from the fitted Gaussian distribution.
+
+        Parameters
+        ----------
+        n_samples : int, optional
+            Number of samples to generate. Defaults to 1.
+
+        Returns
+        -------
+        X : array, shape (n_samples, n_features)
+            Randomly generated sample
+        """
+        self._check_is_fitted()
+
+        if n_samples < 1:
+            raise ValueError(
+                "Invalid value for 'n_samples': %d . The sampling requires at "
+                "least one sample." % (self.n_components))
+
+        _, n_features = self.means_.shape
+        rng = check_random_state(self.random_state)
+        n_samples_comp = np.round(self.weights_ * n_samples).astype(int)
+
+        if self.covariance_type == 'full':
+            X = np.vstack([
+                rng.multivariate_normal(mean, covariance, int(sample))
+                for (mean, covariance, sample) in zip(
+                    self.means_, self.covariances_, n_samples_comp)])
+        elif self.covariance_type == "tied":
+            X = np.vstack([
+                rng.multivariate_normal(mean, self.covariances_, int(sample))
+                for (mean, sample) in zip(
+                    self.means_, n_samples_comp)])
+        else:
+            X = np.vstack([
+                mean + rng.randn(sample, n_features) * np.sqrt(covariance)
+                for (mean, covariance, sample) in zip(
+                    self.means_, self.covariances_, n_samples_comp)])
+
+        y = np.concatenate([j * np.ones(sample, dtype=int)
+                           for j, sample in enumerate(n_samples_comp)])
+
+        return (X, y)
+
     def _estimate_weighted_log_prob(self, X):
         """Estimate the weighted log-probabilities, log P(X | Z) + log weights.
 
diff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py
index 2b5d4458c8..97aaacb456 100755
--- a/sklearn/mixture/bayesian_mixture.py
+++ b/sklearn/mixture/bayesian_mixture.py
@@ -5,7 +5,7 @@
 
 import math
 import numpy as np
-from scipy.special import digamma, gammaln
+from scipy.special import betaln, digamma, gammaln
 
 from .base import BaseMixture, _check_shape
 from .gaussian_mixture import _check_precision_matrix
@@ -63,19 +63,29 @@ def _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):
 
 
 class BayesianGaussianMixture(BaseMixture):
-    """Variational estimation of a Gaussian mixture.
+    """Variational Bayesian estimation of a Gaussian mixture.
 
     This class allows to infer an approximate posterior distribution over the
     parameters of a Gaussian mixture distribution. The effective number of
     components can be inferred from the data.
 
+    This class implements two types of prior for the weights distribution: a
+    finite mixture model with Dirichlet distribution and an infinite mixture
+    model with the Dirichlet Process. In practice Dirichlet Process inference
+    algorithm is approximated and uses a truncated distribution with a fixed
+    maximum number of components (called the Stick-breaking representation).
+    The number of components actually used almost always depends on the data.
+
+    .. versionadded:: 0.18
+    *BayesianGaussianMixture*.
+
     Read more in the :ref:`User Guide <bgmm>`.
 
     Parameters
     ----------
     n_components : int, defaults to 1.
         The number of mixture components. Depending on the data and the value
-        of the `dirichlet_concentration_prior` the model can decide to not use
+        of the `weight_concentration_prior` the model can decide to not use
         all the components by setting some component `weights_` to values very
         close to zero. The number of effective components is therefore smaller
         than n_components.
@@ -83,10 +93,10 @@ class BayesianGaussianMixture(BaseMixture):
     covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'.
         String describing the type of covariance parameters to use.
         Must be one of::
-        'full' (each component has its own general covariance matrix).
+        'full' (each component has its own general covariance matrix),
         'tied' (all components share the same general covariance matrix),
         'diag' (each component has its own diagonal covariance matrix),
-        'spherical' (each component has its own single variance),
+        'spherical' (each component has its own single variance).
 
     tol : float, defaults to 1e-3.
         The convergence threshold. EM iterations will stop when the
@@ -111,7 +121,13 @@ class BayesianGaussianMixture(BaseMixture):
         'kmeans' : responsibilities are initialized using kmeans.
         'random' : responsibilities are initialized randomly.
 
-    dirichlet_concentration_prior : float | None, optional.
+    weight_concentration_prior_type : str, defaults to 'dirichlet_process'.
+        String describing the type of the weight concentration prior.
+        Must be one of::
+        'dirichlet_process' (using the Stick-breaking representation),
+        'dirichlet_distribution' (can favor more uniform weights).
+
+    weight_concentration_prior : float | None, optional.
         The dirichlet concentration of each component on the weight
         distribution (Dirichlet). The higher concentration puts more mass in
         the center and will lead to more components being active, while a lower
@@ -122,7 +138,7 @@ class BayesianGaussianMixture(BaseMixture):
     mean_precision_prior : float | None, optional.
         The precision prior on the mean distribution (Gaussian).
         Controls the extend to where means can be placed. Smaller
-        values concentrates the means of each clusters around `mean_prior`.
+        values concentrate the means of each clusters around `mean_prior`.
         The value of the parameter must be greater than 0.
         If it is None, it's set to 1.
 
@@ -157,6 +173,9 @@ class BayesianGaussianMixture(BaseMixture):
         it prints also the log probability and the time needed
         for each step.
 
+    verbose_interval : int, default to 10.
+        Number of iteration done before the next print.
+
     Attributes
     ----------
     weights_ : array-like, shape (`n_components`,)
@@ -210,21 +229,25 @@ class BayesianGaussianMixture(BaseMixture):
         Lower bound value on the likelihood (of the training data with
         respect to the model) of the best fit of inference.
 
-    dirichlet_concentration_prior_ : float
+    weight_concentration_prior_ : tuple or float
         The dirichlet concentration of each component on the weight
-        distribution (Dirichlet). The higher concentration puts more mass in
+        distribution (Dirichlet). The type depends on
+        `weight_concentration_prior_type`::
+            (float, float) if 'dirichlet_process' (Beta parameters),
+            float          if 'dirichlet_distribution' (Dirichlet parameters).
+        The higher concentration puts more mass in
         the center and will lead to more components being active, while a lower
         concentration parameter will lead to more mass at the edge of the
         simplex.
 
-    dirichlet_concentration_ : array-like, shape (`n_components`, )
+    weight_concentration_ : array-like, shape (`n_components`, )
         The dirichlet concentration of each component on the weight
         distribution (Dirichlet).
 
     mean_precision_prior : float
         The precision prior on the mean distribution (Gaussian).
         Controls the extend to where means can be placed.
-        Smaller values concentrates the means of each clusters around
+        Smaller values concentrate the means of each clusters around
         `mean_prior`.
 
     mean_precision_ : array-like, shape (`n_components`, )
@@ -251,15 +274,32 @@ class BayesianGaussianMixture(BaseMixture):
     See Also
     --------
     GaussianMixture : Finite Gaussian mixture fit with EM.
+
+    References
+    ----------
+
+    .. [1] `Bishop, Christopher M. (2006). "Pattern recognition and machine
+       learning". Vol. 4 No. 4. New York: Springer.
+       <http://www.springer.com/kr/book/9780387310732>`_
+
+    .. [2] `Hagai Attias. (2000). "A Variational Bayesian Framework for
+       Graphical Models". In Advances in Neural Information Processing
+       Systems 12.
+       <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.2841&rep=rep1&type=pdf>`_
+
+    .. [3] `Blei, David M. and Michael I. Jordan. (2006). "Variational
+       inference for Dirichlet process mixtures". Bayesian analysis 1.1
+       <http://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf>
     """
 
     def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
                  reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',
-                 dirichlet_concentration_prior=None,
+                 weight_concentration_prior_type='dirichlet_process',
+                 weight_concentration_prior=None,
                  mean_precision_prior=None, mean_prior=None,
                  degrees_of_freedom_prior=None, covariance_prior=None,
                  random_state=None, warm_start=False, verbose=0,
-                 verbose_interval=20):
+                 verbose_interval=10):
         super(BayesianGaussianMixture, self).__init__(
             n_components=n_components, tol=tol, reg_covar=reg_covar,
             max_iter=max_iter, n_init=n_init, init_params=init_params,
@@ -267,7 +307,8 @@ def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
             verbose=verbose, verbose_interval=verbose_interval)
 
         self.covariance_type = covariance_type
-        self.dirichlet_concentration_prior = dirichlet_concentration_prior
+        self.weight_concentration_prior_type = weight_concentration_prior_type
+        self.weight_concentration_prior = weight_concentration_prior
         self.mean_precision_prior = mean_precision_prior
         self.mean_prior = mean_prior
         self.degrees_of_freedom_prior = degrees_of_freedom_prior
@@ -285,6 +326,15 @@ def _check_parameters(self, X):
                              "'covariance_type' should be in "
                              "['spherical', 'tied', 'diag', 'full']"
                              % self.covariance_type)
+
+        if (self.weight_concentration_prior_type not in
+                ['dirichlet_process', 'dirichlet_distribution']):
+            raise ValueError(
+                "Invalid value for 'weight_concentration_prior_type': %s "
+                "'weight_concentration_prior_type' should be in "
+                "['dirichlet_process', 'dirichlet_distribution']"
+                % self.weight_concentration_prior_type)
+
         self._check_weights_parameters()
         self._check_means_parameters(X)
         self._check_precision_parameters(X)
@@ -292,15 +342,15 @@ def _check_parameters(self, X):
 
     def _check_weights_parameters(self):
         """Check the parameter of the Dirichlet distribution."""
-        if self.dirichlet_concentration_prior is None:
-            self.dirichlet_concentration_prior_ = 1. / self.n_components
-        elif self.dirichlet_concentration_prior > 0.:
-            self.dirichlet_concentration_prior_ = (
-                self.dirichlet_concentration_prior)
+        if self.weight_concentration_prior is None:
+            self.weight_concentration_prior_ = 1. / self.n_components
+        elif self.weight_concentration_prior > 0.:
+            self.weight_concentration_prior_ = (
+                self.weight_concentration_prior)
         else:
-            raise ValueError("The parameter 'dirichlet_concentration_prior' "
+            raise ValueError("The parameter 'weight_concentration_prior' "
                              "should be greater than 0., but got %.3f."
-                             % self.dirichlet_concentration_prior)
+                             % self.weight_concentration_prior)
 
     def _check_means_parameters(self, X):
         """Check the parameters of the Gaussian distribution.
@@ -410,8 +460,16 @@ def _estimate_weights(self, nk):
         ----------
         nk : array-like, shape (n_components,)
         """
-        self.dirichlet_concentration_ = (
-            self.dirichlet_concentration_prior_ + nk)
+        if self.weight_concentration_prior_type == 'dirichlet_process':
+            # For dirichlet process weight_concentration will be a tuple
+            # containing the two parameters of the beta distribution
+            self.weight_concentration_ = (
+                1. + nk,
+                (self.weight_concentration_prior_ +
+                 np.hstack((np.cumsum(nk[::-1])[-2::-1], 0))))
+        else:
+            # case Variationnal Gaussian mixture with dirichlet distribution
+            self.weight_concentration_ = self.weight_concentration_prior_ + nk
 
     def _estimate_means(self, nk, xk):
         """Estimate the parameters of the Gaussian distribution.
@@ -575,7 +633,7 @@ def _estimate_wishart_spherical(self, nk, xk, sk):
         self.covariances_ /= self.degrees_of_freedom_
 
     def _check_is_fitted(self):
-        check_is_fitted(self, ['dirichlet_concentration_', 'mean_precision_',
+        check_is_fitted(self, ['weight_concentration_', 'mean_precision_',
                                'means_', 'degrees_of_freedom_',
                                'covariances_', 'precisions_',
                                'precisions_cholesky_'])
@@ -600,8 +658,17 @@ def _m_step(self, X, log_resp):
         self._estimate_precisions(nk, xk, sk)
 
     def _estimate_log_weights(self):
-        return (digamma(self.dirichlet_concentration_) -
-                digamma(np.sum(self.dirichlet_concentration_)))
+        if self.weight_concentration_prior_type == 'dirichlet_process':
+            digamma_sum = digamma(self.weight_concentration_[0] +
+                                  self.weight_concentration_[1])
+            digamma_a = digamma(self.weight_concentration_[0])
+            digamma_b = digamma(self.weight_concentration_[1])
+            return (digamma_a - digamma_sum +
+                    np.hstack((0, np.cumsum(digamma_b - digamma_sum)[:-1])))
+        else:
+            # case Variationnal Gaussian mixture with dirichlet distribution
+            return (digamma(self.weight_concentration_) -
+                    digamma(np.sum(self.weight_concentration_)))
 
     def _estimate_log_prob(self, X):
         _, n_features = X.shape
@@ -657,25 +724,41 @@ def _compute_lower_bound(self, log_resp, log_prob_norm):
             log_wishart = np.sum(_log_wishart_norm(
                 self.degrees_of_freedom_, log_det_precisions_chol, n_features))
 
-        return (-np.sum(np.exp(log_resp) * log_resp) - log_wishart -
-                _log_dirichlet_norm(self.dirichlet_concentration_) -
+        if self.weight_concentration_prior_type == 'dirichlet_process':
+            log_norm_weight = -np.sum(betaln(self.weight_concentration_[0],
+                                             self.weight_concentration_[1]))
+        else:
+            log_norm_weight = _log_dirichlet_norm(self.weight_concentration_)
+
+        return (-np.sum(np.exp(log_resp) * log_resp) -
+                log_wishart - log_norm_weight -
                 0.5 * n_features * np.sum(np.log(self.mean_precision_)))
 
     def _get_parameters(self):
-        return (self.dirichlet_concentration_,
+        return (self.weight_concentration_,
                 self.mean_precision_, self.means_,
                 self.degrees_of_freedom_, self.covariances_,
                 self.precisions_cholesky_)
 
     def _set_parameters(self, params):
-        (self.dirichlet_concentration_, self.mean_precision_, self.means_,
+        (self.weight_concentration_, self.mean_precision_, self.means_,
          self.degrees_of_freedom_, self.covariances_,
          self.precisions_cholesky_) = params
 
-        # Attributes computation
-        self. weights_ = (self.dirichlet_concentration_ /
-                          np.sum(self.dirichlet_concentration_))
+        # Weights computation
+        if self.weight_concentration_prior_type == "dirichlet_process":
+            weight_dirichlet_sum = (self.weight_concentration_[0] +
+                                    self.weight_concentration_[1])
+            tmp = self.weight_concentration_[1] / weight_dirichlet_sum
+            self.weights_ = (
+                self.weight_concentration_[0] / weight_dirichlet_sum *
+                np.hstack((1, np.cumprod(tmp[:-1]))))
+            self.weights_ /= np.sum(self.weights_)
+        else:
+            self. weights_ = (self.weight_concentration_ /
+                              np.sum(self.weight_concentration_))
 
+        # Precisions matrices computation
         if self.covariance_type == 'full':
             self.precisions_ = np.array([
                 np.dot(prec_chol, prec_chol.T)
diff --git a/sklearn/mixture/dpgmm.py b/sklearn/mixture/dpgmm.py
index ee2bd64911..a8a1e2d928 100755
--- a/sklearn/mixture/dpgmm.py
+++ b/sklearn/mixture/dpgmm.py
@@ -10,6 +10,13 @@
 #         Fabian Pedregosa <fabian.pedregosa@inria.fr>
 #
 
+# Important note for the deprecation cleaning of 0.20 :
+# All the function and classes of this file have been deprecated in 0.18.
+# When you remove this file please also remove the related files
+# - 'sklearn/mixture/gmm.py'
+# - 'sklearn/mixture/test_dpgmm.py'
+# - 'sklearn/mixture/test_gmm.py'
+
 import numpy as np
 from scipy.special import digamma as _digamma, gammaln as _gammaln
 from scipy import linalg
@@ -616,9 +623,11 @@ def _fit(self, X, y=None):
         return z
 
 
-@deprecated("The DPGMM class is not working correctly and it's better "
-            "to not use it. DPGMM is deprecated in 0.18 and "
-            "will be removed in 0.20.")
+@deprecated("The `DPGMM` class is not working correctly and it's better "
+            "to use `sklearn.mixture.BayesianGaussianMixture` class with "
+            "parameter `weight_concentration_prior_type='dirichlet_process'` "
+            "instead. DPGMM is deprecated in 0.18 and will be "
+            "removed in 0.20.")
 class DPGMM(_DPGMMBase):
     def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
                  random_state=None, tol=1e-3, verbose=0, min_covar=None,
@@ -630,8 +639,10 @@ def __init__(self, n_components=1, covariance_type='diag', alpha=1.0,
             init_params=init_params)
 
 
-@deprecated("The VBGMM class is not working correctly and it's better "
-            "to use sklearn.mixture.BayesianGaussianMixture class instead. "
+@deprecated("The `VBGMM` class is not working correctly and it's better "
+            "to use `sklearn.mixture.BayesianGaussianMixture` class with "
+            "parameter `weight_concentration_prior_type="
+            "'dirichlet_distribution'` instead. "
             "VBGMM is deprecated in 0.18 and will be removed in 0.20.")
 class VBGMM(_DPGMMBase):
     """Variational Inference for the Gaussian Mixture Model
diff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py
index 749af6c82f..94c3a07d93 100755
--- a/sklearn/mixture/gaussian_mixture.py
+++ b/sklearn/mixture/gaussian_mixture.py
@@ -439,6 +439,11 @@ class GaussianMixture(BaseMixture):
     This class allows to estimate the parameters of a Gaussian mixture
     distribution.
 
+    .. versionadded:: 0.18
+    *GaussianMixture*.
+
+    Read more in the :ref:`User Guide <gmm>`.
+
     Parameters
     ----------
     n_components : int, defaults to 1.
@@ -448,10 +453,10 @@ class GaussianMixture(BaseMixture):
         defaults to 'full'.
         String describing the type of covariance parameters to use.
         Must be one of::
-        'full' (each component has its own general covariance matrix).
+        'full' (each component has its own general covariance matrix),
         'tied' (all components share the same general covariance matrix),
         'diag' (each component has its own diagonal covariance matrix),
-        'spherical' (each component has its own single variance),
+        'spherical' (each component has its own single variance).
 
     tol : float, defaults to 1e-3.
         The convergence threshold. EM iterations will stop when the
@@ -506,6 +511,9 @@ class GaussianMixture(BaseMixture):
         it prints also the log probability and the time needed
         for each step.
 
+    verbose_interval : int, default to 10.
+        Number of iteration done before the next print.
+
     Attributes
     ----------
     weights_ : array-like, shape (n_components,)
@@ -559,8 +567,8 @@ class GaussianMixture(BaseMixture):
 
     See Also
     --------
-    BayesianGaussianMixture : Finite gaussian mixture model fit with a
-        variational algorithm.
+    BayesianGaussianMixture : Gaussian mixture model fit with a variational
+        inference.
     """
 
     def __init__(self, n_components=1, covariance_type='full', tol=1e-3,
diff --git a/sklearn/mixture/gmm.py b/sklearn/mixture/gmm.py
index e5c8734c77..d1a4eb818a 100755
--- a/sklearn/mixture/gmm.py
+++ b/sklearn/mixture/gmm.py
@@ -9,6 +9,13 @@
 #         Fabian Pedregosa <fabian.pedregosa@inria.fr>
 #         Bertrand Thirion <bertrand.thirion@inria.fr>
 
+# Important note for the deprecation cleaning of 0.20 :
+# All the functions and classes of this file have been deprecated in 0.18.
+# When you remove this file please also remove the related files
+# - 'sklearn/mixture/dpgmm.py'
+# - 'sklearn/mixture/test_dpgmm.py'
+# - 'sklearn/mixture/test_gmm.py'
+
 import numpy as np
 from scipy import linalg
 from time import time
@@ -65,6 +72,9 @@ def log_multivariate_normal_density(X, means, covars, covariance_type='diag'):
         X, means, covars)
 
 
+@deprecated("The function sample_gaussian is deprecated in 0.18"
+            " and will be removed in 0.20."
+            " Use numpy.random.multivariate_normal instead.")
 def sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,
                     random_state=None):
     """Generate random samples from a Gaussian distribution.
diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py
index 3003a94446..e678c07d92 100755
--- a/sklearn/mixture/tests/test_bayesian_mixture.py
+++ b/sklearn/mixture/tests/test_bayesian_mixture.py
@@ -19,15 +19,16 @@
 
 
 COVARIANCE_TYPE = ['full', 'tied', 'diag', 'spherical']
+PRIOR_TYPE = ['dirichlet_process', 'dirichlet_distribution']
 
 
 def test_log_dirichlet_norm():
     rng = np.random.RandomState(0)
 
-    dirichlet_concentration = rng.rand(2)
-    expected_norm = (gammaln(np.sum(dirichlet_concentration)) -
-                     np.sum(gammaln(dirichlet_concentration)))
-    predected_norm = _log_dirichlet_norm(dirichlet_concentration)
+    weight_concentration = rng.rand(2)
+    expected_norm = (gammaln(np.sum(weight_concentration)) -
+                     np.sum(gammaln(weight_concentration)))
+    predected_norm = _log_dirichlet_norm(weight_concentration)
 
     assert_almost_equal(expected_norm, predected_norm)
 
@@ -64,8 +65,22 @@ def test_bayesian_mixture_covariance_type():
                          "Invalid value for 'covariance_type': %s "
                          "'covariance_type' should be in "
                          "['spherical', 'tied', 'diag', 'full']"
-                         % covariance_type,
-                         bgmm.fit, X)
+                         % covariance_type, bgmm.fit, X)
+
+
+def test_bayesian_mixture_weight_concentration_prior_type():
+    rng = np.random.RandomState(0)
+    n_samples, n_features = 10, 2
+    X = rng.rand(n_samples, n_features)
+
+    bad_prior_type = 'bad_prior_type'
+    bgmm = BayesianGaussianMixture(
+        weight_concentration_prior_type=bad_prior_type, random_state=rng)
+    assert_raise_message(ValueError,
+                         "Invalid value for 'weight_concentration_prior_type':"
+                         " %s 'weight_concentration_prior_type' should be in "
+                         "['dirichlet_process', 'dirichlet_distribution']"
+                         % bad_prior_type, bgmm.fit, X)
 
 
 def test_bayesian_mixture_weights_prior_initialisation():
@@ -73,29 +88,29 @@ def test_bayesian_mixture_weights_prior_initialisation():
     n_samples, n_components, n_features = 10, 5, 2
     X = rng.rand(n_samples, n_features)
 
-    # Check raise message for a bad value of dirichlet_concentration_prior
-    bad_dirichlet_concentration_prior_ = 0.
+    # Check raise message for a bad value of weight_concentration_prior
+    bad_weight_concentration_prior_ = 0.
     bgmm = BayesianGaussianMixture(
-        dirichlet_concentration_prior=bad_dirichlet_concentration_prior_,
+        weight_concentration_prior=bad_weight_concentration_prior_,
         random_state=0)
     assert_raise_message(ValueError,
-                         "The parameter 'dirichlet_concentration_prior' "
+                         "The parameter 'weight_concentration_prior' "
                          "should be greater than 0., but got %.3f."
-                         % bad_dirichlet_concentration_prior_,
+                         % bad_weight_concentration_prior_,
                          bgmm.fit, X)
 
-    # Check correct init for a given value of dirichlet_concentration_prior
-    dirichlet_concentration_prior = rng.rand()
+    # Check correct init for a given value of weight_concentration_prior
+    weight_concentration_prior = rng.rand()
     bgmm = BayesianGaussianMixture(
-        dirichlet_concentration_prior=dirichlet_concentration_prior,
+        weight_concentration_prior=weight_concentration_prior,
         random_state=rng).fit(X)
-    assert_almost_equal(dirichlet_concentration_prior,
-                        bgmm.dirichlet_concentration_prior_)
+    assert_almost_equal(weight_concentration_prior,
+                        bgmm.weight_concentration_prior_)
 
-    # Check correct init for the default value of dirichlet_concentration_prior
+    # Check correct init for the default value of weight_concentration_prior
     bgmm = BayesianGaussianMixture(n_components=n_components,
                                    random_state=rng).fit(X)
-    assert_almost_equal(1. / n_components, bgmm.dirichlet_concentration_prior_)
+    assert_almost_equal(1. / n_components, bgmm.weight_concentration_prior_)
 
 
 def test_bayesian_mixture_means_prior_initialisation():
@@ -237,44 +252,57 @@ def test_bayesian_mixture_weights():
     n_samples, n_features = 10, 2
 
     X = rng.rand(n_samples, n_features)
-    bgmm = BayesianGaussianMixture(random_state=rng).fit(X)
-
-    # Check the weights values
-    expected_weights = (bgmm.dirichlet_concentration_ /
-                        np.sum(bgmm.dirichlet_concentration_))
-    predected_weights = bgmm.weights_
 
-    assert_almost_equal(expected_weights, predected_weights)
+    # Case Dirichlet distribution for the weight concentration prior type
+    bgmm = BayesianGaussianMixture(
+        weight_concentration_prior_type="dirichlet_distribution",
+        n_components=3, random_state=rng).fit(X)
 
-    # Check the weights sum = 1
+    expected_weights = (bgmm.weight_concentration_ /
+                        np.sum(bgmm.weight_concentration_))
+    assert_almost_equal(expected_weights, bgmm.weights_)
     assert_almost_equal(np.sum(bgmm.weights_), 1.0)
 
+    # Case Dirichlet process for the weight concentration prior type
+    dpgmm = BayesianGaussianMixture(
+        weight_concentration_prior_type="dirichlet_process",
+        n_components=3, random_state=rng).fit(X)
+    weight_dirichlet_sum = (dpgmm.weight_concentration_[0] +
+                            dpgmm.weight_concentration_[1])
+    tmp = dpgmm.weight_concentration_[1] / weight_dirichlet_sum
+    expected_weights = (dpgmm.weight_concentration_[0] / weight_dirichlet_sum *
+                        np.hstack((1, np.cumprod(tmp[:-1]))))
+    expected_weights /= np.sum(expected_weights)
+    assert_almost_equal(expected_weights, dpgmm.weights_)
+    assert_almost_equal(np.sum(dpgmm.weights_), 1.0)
+
 
 @ignore_warnings(category=ConvergenceWarning)
 def test_monotonic_likelihood():
     # We check that each step of the each step of variational inference without
     # regularization improve monotonically the training set of the bound
     rng = np.random.RandomState(0)
-    rand_data = RandomData(rng, scale=7)
+    rand_data = RandomData(rng, scale=20)
     n_components = rand_data.n_components
 
-    for covar_type in COVARIANCE_TYPE:
-        X = rand_data.X[covar_type]
-        bgmm = BayesianGaussianMixture(n_components=2 * n_components,
-                                       covariance_type=covar_type,
-                                       warm_start=True, max_iter=1,
-                                       random_state=rng, tol=1e-4)
-        current_lower_bound = -np.infty
-        # Do one training iteration at a time so we can make sure that the
-        # training log likelihood increases after each iteration.
-        for _ in range(500):
-            prev_lower_bound = current_lower_bound
-            current_lower_bound = bgmm.fit(X).lower_bound_
-            assert_greater_equal(current_lower_bound, prev_lower_bound)
-
-            if bgmm.converged_:
-                break
-        assert(bgmm.converged_)
+    for prior_type in PRIOR_TYPE:
+        for covar_type in COVARIANCE_TYPE:
+            X = rand_data.X[covar_type]
+            bgmm = BayesianGaussianMixture(
+                weight_concentration_prior_type=prior_type,
+                n_components=2 * n_components, covariance_type=covar_type,
+                warm_start=True, max_iter=1, random_state=rng, tol=1e-4)
+            current_lower_bound = -np.infty
+            # Do one training iteration at a time so we can make sure that the
+            # training log likelihood increases after each iteration.
+            for _ in range(600):
+                prev_lower_bound = current_lower_bound
+                current_lower_bound = bgmm.fit(X).lower_bound_
+                assert_greater_equal(current_lower_bound, prev_lower_bound)
+
+                if bgmm.converged_:
+                    break
+            assert(bgmm.converged_)
 
 
 def test_compare_covar_type():
@@ -284,46 +312,55 @@ def test_compare_covar_type():
     rand_data = RandomData(rng, scale=7)
     X = rand_data.X['full']
     n_components = rand_data.n_components
-    # Computation of the full_covariance
-    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
-                                   covariance_type='full',
-                                   max_iter=1, random_state=0, tol=1e-7)
-    bgmm._check_initial_parameters(X)
-    bgmm._initialize_parameters(X)
-    full_covariances = (bgmm.covariances_ *
-                        bgmm.degrees_of_freedom_[:, np.newaxis, np.newaxis])
-
-    # Check tied_covariance = mean(full_covariances, 0)
-    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
-                                   covariance_type='tied',
-                                   max_iter=1, random_state=0, tol=1e-7)
-    bgmm._check_initial_parameters(X)
-    bgmm._initialize_parameters(X)
-
-    tied_covariance = bgmm.covariances_ * bgmm.degrees_of_freedom_
-    assert_almost_equal(tied_covariance, np.mean(full_covariances, 0))
-
-    # Check diag_covariance = diag(full_covariances)
-    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
-                                   covariance_type='diag',
-                                   max_iter=1, random_state=0, tol=1e-7)
-    bgmm._check_initial_parameters(X)
-    bgmm._initialize_parameters(X)
-
-    diag_covariances = (bgmm.covariances_ *
-                        bgmm.degrees_of_freedom_[:, np.newaxis])
-    assert_almost_equal(diag_covariances,
-                        np.array([np.diag(cov) for cov in full_covariances]))
-
-    # Check spherical_covariance = np.mean(diag_covariances, 0)
-    bgmm = BayesianGaussianMixture(n_components=2 * n_components,
-                                   covariance_type='spherical',
-                                   max_iter=1, random_state=0, tol=1e-7)
-    bgmm._check_initial_parameters(X)
-    bgmm._initialize_parameters(X)
-
-    spherical_covariances = bgmm.covariances_ * bgmm.degrees_of_freedom_
-    assert_almost_equal(spherical_covariances, np.mean(diag_covariances, 1))
+
+    for prior_type in PRIOR_TYPE:
+        # Computation of the full_covariance
+        bgmm = BayesianGaussianMixture(
+            weight_concentration_prior_type=prior_type,
+            n_components=2 * n_components, covariance_type='full',
+            max_iter=1, random_state=0, tol=1e-7)
+        bgmm._check_initial_parameters(X)
+        bgmm._initialize_parameters(X, np.random.RandomState(0))
+        full_covariances = (
+            bgmm.covariances_ *
+            bgmm.degrees_of_freedom_[:, np.newaxis, np.newaxis])
+
+        # Check tied_covariance = mean(full_covariances, 0)
+        bgmm = BayesianGaussianMixture(
+            weight_concentration_prior_type=prior_type,
+            n_components=2 * n_components, covariance_type='tied',
+            max_iter=1, random_state=0, tol=1e-7)
+        bgmm._check_initial_parameters(X)
+        bgmm._initialize_parameters(X, np.random.RandomState(0))
+
+        tied_covariance = bgmm.covariances_ * bgmm.degrees_of_freedom_
+        assert_almost_equal(tied_covariance, np.mean(full_covariances, 0))
+
+        # Check diag_covariance = diag(full_covariances)
+        bgmm = BayesianGaussianMixture(
+            weight_concentration_prior_type=prior_type,
+            n_components=2 * n_components, covariance_type='diag',
+            max_iter=1, random_state=0, tol=1e-7)
+        bgmm._check_initial_parameters(X)
+        bgmm._initialize_parameters(X, np.random.RandomState(0))
+
+        diag_covariances = (bgmm.covariances_ *
+                            bgmm.degrees_of_freedom_[:, np.newaxis])
+        assert_almost_equal(diag_covariances,
+                            np.array([np.diag(cov)
+                                     for cov in full_covariances]))
+
+        # Check spherical_covariance = np.mean(diag_covariances, 0)
+        bgmm = BayesianGaussianMixture(
+            weight_concentration_prior_type=prior_type,
+            n_components=2 * n_components, covariance_type='spherical',
+            max_iter=1, random_state=0, tol=1e-7)
+        bgmm._check_initial_parameters(X)
+        bgmm._initialize_parameters(X, np.random.RandomState(0))
+
+        spherical_covariances = bgmm.covariances_ * bgmm.degrees_of_freedom_
+        assert_almost_equal(
+            spherical_covariances, np.mean(diag_covariances, 1))
 
 
 @ignore_warnings(category=ConvergenceWarning)
@@ -357,3 +394,28 @@ def test_check_covariance_precision():
         else:
             assert_almost_equal(bgmm.covariances_ * bgmm.precisions_,
                                 np.ones(n_components))
+
+
+@ignore_warnings(category=ConvergenceWarning)
+def test_invariant_translation():
+    # We check here that adding a constant in the data change correctly the
+    # parameters of the mixture
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=100)
+    n_components = 2 * rand_data.n_components
+
+    for prior_type in PRIOR_TYPE:
+        for covar_type in COVARIANCE_TYPE:
+            X = rand_data.X[covar_type]
+            bgmm1 = BayesianGaussianMixture(
+                weight_concentration_prior_type=prior_type,
+                n_components=n_components, max_iter=100, random_state=0,
+                tol=1e-3, reg_covar=0).fit(X)
+            bgmm2 = BayesianGaussianMixture(
+                weight_concentration_prior_type=prior_type,
+                n_components=n_components, max_iter=100, random_state=0,
+                tol=1e-3, reg_covar=0).fit(X + 100)
+
+            assert_almost_equal(bgmm1.means_, bgmm2.means_ - 100)
+            assert_almost_equal(bgmm1.weights_, bgmm2.weights_)
+            assert_almost_equal(bgmm1.covariances_, bgmm2.covariances_)
diff --git a/sklearn/mixture/tests/test_dpgmm.py b/sklearn/mixture/tests/test_dpgmm.py
index 363d31bc29..8ca38626b4 100755
--- a/sklearn/mixture/tests/test_dpgmm.py
+++ b/sklearn/mixture/tests/test_dpgmm.py
@@ -1,3 +1,9 @@
+# Important note for the deprecation cleaning of 0.20 :
+# All the function and classes of this file have been deprecated in 0.18.
+# When you remove this file please also remove the related files
+# - 'sklearn/mixture/dpgmm.py'
+# - 'sklearn/mixture/gmm.py'
+# - 'sklearn/mixture/test_gmm.py'
 import unittest
 import sys
 
@@ -143,10 +149,12 @@ def test_wishart_logz():
 
 @ignore_warnings(category=DeprecationWarning)
 def test_DPGMM_deprecation():
-    assert_warns_message(DeprecationWarning, "The DPGMM class is"
-                         " not working correctly and it's better "
-                         "to not use it. DPGMM is deprecated in 0.18 "
-                         "and will be removed in 0.20.", DPGMM)
+    assert_warns_message(
+      DeprecationWarning, "The `DPGMM` class is not working correctly and "
+      "it's better to use `sklearn.mixture.BayesianGaussianMixture` class "
+      "with parameter `weight_concentration_prior_type='dirichlet_process'` "
+      "instead. DPGMM is deprecated in 0.18 and will be removed in 0.20.",
+      DPGMM)
 
 
 def do_model(self, **kwds):
@@ -183,11 +191,12 @@ class TestDPGMMWithFullCovars(unittest.TestCase, DPGMMTester):
 
 
 def test_VBGMM_deprecation():
-    assert_warns_message(DeprecationWarning, "The VBGMM class is not working "
-                         "correctly and it's better to use "
-                         "sklearn.mixture.BayesianGaussianMixture class "
-                         "instead. VBGMM is deprecated in 0.18 and will be "
-                         "removed in 0.20.", VBGMM)
+    assert_warns_message(
+        DeprecationWarning, "The `VBGMM` class is not working correctly and "
+        "it's better to use `sklearn.mixture.BayesianGaussianMixture` class "
+        "with parameter `weight_concentration_prior_type="
+        "'dirichlet_distribution'` instead. VBGMM is deprecated "
+        "in 0.18 and will be removed in 0.20.", VBGMM)
 
 
 class VBGMMTester(GMMTester):
diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py
index 194248ac7a..43d61e010a 100755
--- a/sklearn/mixture/tests/test_gaussian_mixture.py
+++ b/sklearn/mixture/tests/test_gaussian_mixture.py
@@ -33,6 +33,7 @@
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_warns_message
+from sklearn.utils.testing import ignore_warnings
 
 
 COVARIANCE_TYPE = ['full', 'tied', 'diag', 'spherical']
@@ -650,7 +651,7 @@ def test_gaussian_mixture_fit_convergence_warning():
         assert_warns_message(ConvergenceWarning,
                              'Initialization %d did not converged. '
                              'Try different init parameters, '
-                             'or increase n_init, tol '
+                             'or increase max_iter, tol '
                              'or check for degenerate data.'
                              % max_iter, g.fit, X)
 
@@ -913,3 +914,60 @@ def test_property():
                                       gmm.covariances_)
         else:
             assert_array_almost_equal(gmm.precisions_, 1. / gmm.covariances_)
+
+
+def test_sample():
+    rng = np.random.RandomState(0)
+    rand_data = RandomData(rng, scale=7)
+    n_features, n_components = rand_data.n_features, rand_data.n_components
+
+    for covar_type in COVARIANCE_TYPE:
+        X = rand_data.X[covar_type]
+
+        gmm = GaussianMixture(n_components=n_components,
+                              covariance_type=covar_type, random_state=rng)
+        # To sample we need that GaussianMixture is fitted
+        assert_raise_message(NotFittedError, "This GaussianMixture instance "
+                             "is not fitted", gmm.sample, 0)
+        gmm.fit(X)
+
+        assert_raise_message(ValueError, "Invalid value for 'n_samples",
+                             gmm.sample, 0)
+
+        # Just to make sure the class samples correctly
+        X_s, y_s = gmm.sample(20000)
+        for k in range(n_features):
+            if covar_type == 'full':
+                assert_array_almost_equal(gmm.covariances_[k],
+                                          np.cov(X_s[y_s == k].T), decimal=1)
+            elif covar_type == 'tied':
+                assert_array_almost_equal(gmm.covariances_,
+                                          np.cov(X_s[y_s == k].T), decimal=1)
+            elif covar_type == 'diag':
+                assert_array_almost_equal(gmm.covariances_[k],
+                                          np.diag(np.cov(X_s[y_s == k].T)),
+                                          decimal=1)
+            else:
+                assert_array_almost_equal(
+                    gmm.covariances_[k], np.var(X_s[y_s == k] - gmm.means_[k]),
+                    decimal=1)
+
+        means_s = np.array([np.mean(X_s[y_s == k], 0)
+                           for k in range(n_features)])
+        assert_array_almost_equal(gmm.means_, means_s, decimal=1)
+
+
+@ignore_warnings(category=ConvergenceWarning)
+def test_init():
+    # We check that by increasing the n_init number we have a better solution
+    random_state = 0
+    rand_data = RandomData(np.random.RandomState(random_state), scale=1)
+    n_components = rand_data.n_components
+    X = rand_data.X['full']
+
+    gmm1 = GaussianMixture(n_components=n_components, n_init=1,
+                           max_iter=1, random_state=random_state).fit(X)
+    gmm2 = GaussianMixture(n_components=n_components, n_init=100,
+                           max_iter=1, random_state=random_state).fit(X)
+
+    assert_greater(gmm2.lower_bound_, gmm1.lower_bound_)
diff --git a/sklearn/mixture/tests/test_gmm.py b/sklearn/mixture/tests/test_gmm.py
index 66e581a582..55f0dfb83f 100755
--- a/sklearn/mixture/tests/test_gmm.py
+++ b/sklearn/mixture/tests/test_gmm.py
@@ -1,5 +1,9 @@
-# These tests are those of the deprecated GMM class
-
+# Important note for the deprecation cleaning of 0.20 :
+# All the functions and classes of this file have been deprecated in 0.18.
+# When you remove this file please remove the related files
+# - 'sklearn/mixture/dpgmm.py'
+# - 'sklearn/mixture/gmm.py'
+# - 'sklearn/mixture/test_dpgmm.py'
 import unittest
 import copy
 import sys
diff --git a/sklearn/model_selection/__init__.py b/sklearn/model_selection/__init__.py
index c2acf500af..f5ab0d7526 100755
--- a/sklearn/model_selection/__init__.py
+++ b/sklearn/model_selection/__init__.py
@@ -1,14 +1,14 @@
 from ._split import BaseCrossValidator
 from ._split import KFold
-from ._split import LabelKFold
+from ._split import GroupKFold
 from ._split import StratifiedKFold
 from ._split import TimeSeriesSplit
-from ._split import LeaveOneLabelOut
+from ._split import LeaveOneGroupOut
 from ._split import LeaveOneOut
-from ._split import LeavePLabelOut
+from ._split import LeavePGroupsOut
 from ._split import LeavePOut
 from ._split import ShuffleSplit
-from ._split import LabelShuffleSplit
+from ._split import GroupShuffleSplit
 from ._split import StratifiedShuffleSplit
 from ._split import PredefinedSplit
 from ._split import train_test_split
@@ -30,11 +30,11 @@
            'GridSearchCV',
            'TimeSeriesSplit',
            'KFold',
-           'LabelKFold',
-           'LabelShuffleSplit',
-           'LeaveOneLabelOut',
+           'GroupKFold',
+           'GroupShuffleSplit',
+           'LeaveOneGroupOut',
            'LeaveOneOut',
-           'LeavePLabelOut',
+           'LeavePGroupsOut',
            'LeavePOut',
            'ParameterGrid',
            'ParameterSampler',
diff --git a/sklearn/model_selection/_search.py b/sklearn/model_selection/_search.py
index c96ee7f19d..f1880555df 100755
--- a/sklearn/model_selection/_search.py
+++ b/sklearn/model_selection/_search.py
@@ -528,15 +528,15 @@ def inverse_transform(self, Xt):
         self._check_is_fitted('inverse_transform')
         return self.best_estimator_.transform(Xt)
 
-    def _fit(self, X, y, labels, parameter_iterable):
+    def _fit(self, X, y, groups, parameter_iterable):
         """Actual fitting,  performing the search over parameters."""
 
         estimator = self.estimator
         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
         self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)
 
-        X, y, labels = indexable(X, y, labels)
-        n_splits = cv.get_n_splits(X, y, labels)
+        X, y, groups = indexable(X, y, groups)
+        n_splits = cv.get_n_splits(X, y, groups)
         if self.verbose > 0 and isinstance(parameter_iterable, Sized):
             n_candidates = len(parameter_iterable)
             print("Fitting {0} folds for each of {1} candidates, totalling"
@@ -554,7 +554,7 @@ def _fit(self, X, y, labels, parameter_iterable):
                                   self.fit_params, return_parameters=True,
                                   error_score=self.error_score)
           for parameters in parameter_iterable
-          for train, test in cv.split(X, y, labels))
+          for train, test in cv.split(X, y, groups))
 
         test_scores, test_sample_counts, _, parameters = zip(*out)
 
@@ -876,7 +876,7 @@ def __init__(self, estimator, param_grid, scoring=None, fit_params=None,
         self.param_grid = param_grid
         _check_param_grid(param_grid)
 
-    def fit(self, X, y=None, labels=None):
+    def fit(self, X, y=None, groups=None):
         """Run fit with all sets of parameters.
 
         Parameters
@@ -890,11 +890,11 @@ def fit(self, X, y=None, labels=None):
             Target relative to X for classification or regression;
             None for unsupervised learning.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
         """
-        return self._fit(X, y, labels, ParameterGrid(self.param_grid))
+        return self._fit(X, y, groups, ParameterGrid(self.param_grid))
 
 
 class RandomizedSearchCV(BaseSearchCV):
@@ -1104,7 +1104,7 @@ def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
             pre_dispatch=pre_dispatch, error_score=error_score)
 
-    def fit(self, X, y=None, labels=None):
+    def fit(self, X, y=None, groups=None):
         """Run fit on the estimator with randomly drawn parameters.
 
         Parameters
@@ -1117,11 +1117,11 @@ def fit(self, X, y=None, labels=None):
             Target relative to X for classification or regression;
             None for unsupervised learning.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
         """
         sampled_params = ParameterSampler(self.param_distributions,
                                           self.n_iter,
                                           random_state=self.random_state)
-        return self._fit(X, y, labels, sampled_params)
+        return self._fit(X, y, groups, sampled_params)
diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py
index 5989edd30b..3570d688ff 100755
--- a/sklearn/model_selection/_split.py
+++ b/sklearn/model_selection/_split.py
@@ -30,18 +30,19 @@
 from ..externals.six.moves import zip
 from ..utils.fixes import bincount
 from ..utils.fixes import signature
+from ..utils.random import choice
 from ..base import _pprint
 from ..gaussian_process.kernels import Kernel as GPKernel
 
 __all__ = ['BaseCrossValidator',
            'KFold',
-           'LabelKFold',
-           'LeaveOneLabelOut',
+           'GroupKFold',
+           'LeaveOneGroupOut',
            'LeaveOneOut',
-           'LeavePLabelOut',
+           'LeavePGroupsOut',
            'LeavePOut',
            'ShuffleSplit',
-           'LabelShuffleSplit',
+           'GroupShuffleSplit',
            'StratifiedKFold',
            'StratifiedShuffleSplit',
            'PredefinedSplit',
@@ -60,7 +61,7 @@ def __init__(self):
         # see #6304
         pass
 
-    def split(self, X, y=None, labels=None):
+    def split(self, X, y=None, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -72,7 +73,7 @@ def split(self, X, y=None, labels=None):
         y : array-like, of length n_samples
             The target variable for supervised learning problems.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -84,31 +85,31 @@ def split(self, X, y=None, labels=None):
         test : ndarray
             The testing set indices for that split.
         """
-        X, y, labels = indexable(X, y, labels)
+        X, y, groups = indexable(X, y, groups)
         indices = np.arange(_num_samples(X))
-        for test_index in self._iter_test_masks(X, y, labels):
+        for test_index in self._iter_test_masks(X, y, groups):
             train_index = indices[np.logical_not(test_index)]
             test_index = indices[test_index]
             yield train_index, test_index
 
     # Since subclasses must implement either _iter_test_masks or
     # _iter_test_indices, neither can be abstract.
-    def _iter_test_masks(self, X=None, y=None, labels=None):
+    def _iter_test_masks(self, X=None, y=None, groups=None):
         """Generates boolean masks corresponding to test sets.
 
-        By default, delegates to _iter_test_indices(X, y, labels)
+        By default, delegates to _iter_test_indices(X, y, groups)
         """
-        for test_index in self._iter_test_indices(X, y, labels):
+        for test_index in self._iter_test_indices(X, y, groups):
             test_mask = np.zeros(_num_samples(X), dtype=np.bool)
             test_mask[test_index] = True
             yield test_mask
 
-    def _iter_test_indices(self, X=None, y=None, labels=None):
+    def _iter_test_indices(self, X=None, y=None, groups=None):
         """Generates integer indices corresponding to test sets."""
         raise NotImplementedError
 
     @abstractmethod
-    def get_n_splits(self, X=None, y=None, labels=None):
+    def get_n_splits(self, X=None, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator"""
 
     def __repr__(self):
@@ -154,17 +155,17 @@ class LeaveOneOut(BaseCrossValidator):
 
     See also
     --------
-    LeaveOneLabelOut
+    LeaveOneGroupOut
         For splitting the data according to explicit, domain-specific
         stratification of the dataset.
 
-    LabelKFold: K-fold iterator variant with non-overlapping labels.
+    GroupKFold: K-fold iterator variant with non-overlapping groups.
     """
 
-    def _iter_test_indices(self, X, y=None, labels=None):
+    def _iter_test_indices(self, X, y=None, groups=None):
         return range(_num_samples(X))
 
-    def get_n_splits(self, X, y=None, labels=None):
+    def get_n_splits(self, X, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -176,7 +177,7 @@ def get_n_splits(self, X, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
 
         Returns
@@ -236,11 +237,11 @@ class LeavePOut(BaseCrossValidator):
     def __init__(self, p):
         self.p = p
 
-    def _iter_test_indices(self, X, y=None, labels=None):
+    def _iter_test_indices(self, X, y=None, groups=None):
         for combination in combinations(range(_num_samples(X)), self.p):
             yield np.array(combination)
 
-    def get_n_splits(self, X, y=None, labels=None):
+    def get_n_splits(self, X, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -252,7 +253,7 @@ def get_n_splits(self, X, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
         """
         if X is None:
@@ -261,7 +262,7 @@ def get_n_splits(self, X, y=None, labels=None):
 
 
 class _BaseKFold(with_metaclass(ABCMeta, BaseCrossValidator)):
-    """Base class for KFold, LabelKFold, and StratifiedKFold"""
+    """Base class for KFold, GroupKFold, and StratifiedKFold"""
 
     @abstractmethod
     def __init__(self, n_splits, shuffle, random_state):
@@ -285,7 +286,7 @@ def __init__(self, n_splits, shuffle, random_state):
         self.shuffle = shuffle
         self.random_state = random_state
 
-    def split(self, X, y=None, labels=None):
+    def split(self, X, y=None, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -297,7 +298,7 @@ def split(self, X, y=None, labels=None):
         y : array-like, shape (n_samples,)
             The target variable for supervised learning problems.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -309,7 +310,7 @@ def split(self, X, y=None, labels=None):
         test : ndarray
             The testing set indices for that split.
         """
-        X, y, labels = indexable(X, y, labels)
+        X, y, groups = indexable(X, y, groups)
         n_samples = _num_samples(X)
         if self.n_splits > n_samples:
             raise ValueError(
@@ -317,10 +318,10 @@ def split(self, X, y=None, labels=None):
                  " than the number of samples: {1}.").format(self.n_splits,
                                                              n_samples))
 
-        for train, test in super(_BaseKFold, self).split(X, y, labels):
+        for train, test in super(_BaseKFold, self).split(X, y, groups):
             yield train, test
 
-    def get_n_splits(self, X=None, y=None, labels=None):
+    def get_n_splits(self, X=None, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -331,7 +332,7 @@ def get_n_splits(self, X=None, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
 
         Returns
@@ -391,18 +392,18 @@ class KFold(_BaseKFold):
     See also
     --------
     StratifiedKFold
-        Takes label information into account to avoid building folds with
+        Takes group information into account to avoid building folds with
         imbalanced class distributions (for binary or multiclass
         classification tasks).
 
-    LabelKFold: K-fold iterator variant with non-overlapping labels.
+    GroupKFold: K-fold iterator variant with non-overlapping groups.
     """
 
     def __init__(self, n_splits=3, shuffle=False,
                  random_state=None):
         super(KFold, self).__init__(n_splits, shuffle, random_state)
 
-    def _iter_test_indices(self, X, y=None, labels=None):
+    def _iter_test_indices(self, X, y=None, groups=None):
         n_samples = _num_samples(X)
         indices = np.arange(n_samples)
         if self.shuffle:
@@ -418,14 +419,14 @@ def _iter_test_indices(self, X, y=None, labels=None):
             current = stop
 
 
-class LabelKFold(_BaseKFold):
-    """K-fold iterator variant with non-overlapping labels.
+class GroupKFold(_BaseKFold):
+    """K-fold iterator variant with non-overlapping groups.
 
-    The same label will not appear in two different folds (the number of
-    distinct labels has to be at least equal to the number of folds).
+    The same group will not appear in two different folds (the number of
+    distinct groups has to be at least equal to the number of folds).
 
     The folds are approximately balanced in the sense that the number of
-    distinct labels is approximately the same in each fold.
+    distinct groups is approximately the same in each fold.
 
     Parameters
     ----------
@@ -434,16 +435,16 @@ class LabelKFold(_BaseKFold):
 
     Examples
     --------
-    >>> from sklearn.model_selection import LabelKFold
+    >>> from sklearn.model_selection import GroupKFold
     >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
     >>> y = np.array([1, 2, 3, 4])
-    >>> labels = np.array([0, 0, 2, 2])
-    >>> label_kfold = LabelKFold(n_splits=2)
-    >>> label_kfold.get_n_splits(X, y, labels)
+    >>> groups = np.array([0, 0, 2, 2])
+    >>> group_kfold = GroupKFold(n_splits=2)
+    >>> group_kfold.get_n_splits(X, y, groups)
     2
-    >>> print(label_kfold)
-    LabelKFold(n_splits=2)
-    >>> for train_index, test_index in label_kfold.split(X, y, labels):
+    >>> print(group_kfold)
+    GroupKFold(n_splits=2)
+    >>> for train_index, test_index in group_kfold.split(X, y, groups):
     ...     print("TRAIN:", train_index, "TEST:", test_index)
     ...     X_train, X_test = X[train_index], X[test_index]
     ...     y_train, y_test = y[train_index], y[test_index]
@@ -460,46 +461,46 @@ class LabelKFold(_BaseKFold):
 
     See also
     --------
-    LeaveOneLabelOut
+    LeaveOneGroupOut
         For splitting the data according to explicit domain-specific
         stratification of the dataset.
     """
     def __init__(self, n_splits=3):
-        super(LabelKFold, self).__init__(n_splits, shuffle=False,
+        super(GroupKFold, self).__init__(n_splits, shuffle=False,
                                          random_state=None)
 
-    def _iter_test_indices(self, X, y, labels):
-        if labels is None:
-            raise ValueError("The labels parameter should not be None")
+    def _iter_test_indices(self, X, y, groups):
+        if groups is None:
+            raise ValueError("The groups parameter should not be None")
 
-        unique_labels, labels = np.unique(labels, return_inverse=True)
-        n_labels = len(unique_labels)
+        unique_groups, groups = np.unique(groups, return_inverse=True)
+        n_groups = len(unique_groups)
 
-        if self.n_splits > n_labels:
+        if self.n_splits > n_groups:
             raise ValueError("Cannot have number of splits n_splits=%d greater"
-                             " than the number of labels: %d."
-                             % (self.n_splits, n_labels))
+                             " than the number of groups: %d."
+                             % (self.n_splits, n_groups))
 
-        # Weight labels by their number of occurrences
-        n_samples_per_label = np.bincount(labels)
+        # Weight groups by their number of occurrences
+        n_samples_per_group = np.bincount(groups)
 
-        # Distribute the most frequent labels first
-        indices = np.argsort(n_samples_per_label)[::-1]
-        n_samples_per_label = n_samples_per_label[indices]
+        # Distribute the most frequent groups first
+        indices = np.argsort(n_samples_per_group)[::-1]
+        n_samples_per_group = n_samples_per_group[indices]
 
         # Total weight of each fold
         n_samples_per_fold = np.zeros(self.n_splits)
 
-        # Mapping from label index to fold index
-        label_to_fold = np.zeros(len(unique_labels))
+        # Mapping from group index to fold index
+        group_to_fold = np.zeros(len(unique_groups))
 
         # Distribute samples by adding the largest weight to the lightest fold
-        for label_index, weight in enumerate(n_samples_per_label):
+        for group_index, weight in enumerate(n_samples_per_group):
             lightest_fold = np.argmin(n_samples_per_fold)
             n_samples_per_fold[lightest_fold] += weight
-            label_to_fold[indices[label_index]] = lightest_fold
+            group_to_fold[indices[group_index]] = lightest_fold
 
-        indices = label_to_fold[labels]
+        indices = group_to_fold[groups]
 
         for f in range(self.n_splits):
             yield np.where(indices == f)[0]
@@ -556,7 +557,7 @@ class StratifiedKFold(_BaseKFold):
     def __init__(self, n_splits=3, shuffle=False, random_state=None):
         super(StratifiedKFold, self).__init__(n_splits, shuffle, random_state)
 
-    def _make_test_folds(self, X, y=None, labels=None):
+    def _make_test_folds(self, X, y=None, groups=None):
         if self.shuffle:
             rng = check_random_state(self.random_state)
         else:
@@ -565,17 +566,17 @@ def _make_test_folds(self, X, y=None, labels=None):
         n_samples = y.shape[0]
         unique_y, y_inversed = np.unique(y, return_inverse=True)
         y_counts = bincount(y_inversed)
-        min_labels = np.min(y_counts)
+        min_groups = np.min(y_counts)
         if np.all(self.n_splits > y_counts):
-            raise ValueError("All the n_labels for individual classes"
+            raise ValueError("All the n_groups for individual classes"
                              " are less than n_splits=%d."
                              % (self.n_splits))
-        if self.n_splits > min_labels:
+        if self.n_splits > min_groups:
             warnings.warn(("The least populated class in y has only %d"
                            " members, which is too few. The minimum"
-                           " number of labels for any class cannot"
+                           " number of groups for any class cannot"
                            " be less than n_splits=%d."
-                           % (min_labels, self.n_splits)), Warning)
+                           % (min_groups, self.n_splits)), Warning)
 
         # pre-assign each sample to a test fold index using individual KFold
         # splitting strategies for each class so as to respect the balance of
@@ -603,12 +604,12 @@ def _make_test_folds(self, X, y=None, labels=None):
 
         return test_folds
 
-    def _iter_test_masks(self, X, y=None, labels=None):
+    def _iter_test_masks(self, X, y=None, groups=None):
         test_folds = self._make_test_folds(X, y)
         for i in range(self.n_splits):
             yield test_folds == i
 
-    def split(self, X, y, labels=None):
+    def split(self, X, y, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -620,7 +621,7 @@ def split(self, X, y, labels=None):
         y : array-like, shape (n_samples,)
             The target variable for supervised learning problems.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -632,7 +633,7 @@ def split(self, X, y, labels=None):
         test : ndarray
             The testing set indices for that split.
         """
-        return super(StratifiedKFold, self).split(X, y, labels)
+        return super(StratifiedKFold, self).split(X, y, groups)
 
 
 class TimeSeriesSplit(_BaseKFold):
@@ -685,7 +686,7 @@ def __init__(self, n_splits=3):
                                               shuffle=False,
                                               random_state=None)
 
-    def split(self, X, y=None, labels=None):
+    def split(self, X, y=None, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -697,7 +698,7 @@ def split(self, X, y=None, labels=None):
         y : array-like, shape (n_samples,)
             The target variable for supervised learning problems.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -709,7 +710,7 @@ def split(self, X, y=None, labels=None):
         test : ndarray
             The testing set indices for that split.
         """
-        X, y, labels = indexable(X, y, labels)
+        X, y, groups = indexable(X, y, groups)
         n_samples = _num_samples(X)
         n_splits = self.n_splits
         n_folds = n_splits + 1
@@ -727,30 +728,30 @@ def split(self, X, y=None, labels=None):
                    indices[test_start:test_start + test_size])
 
 
-class LeaveOneLabelOut(BaseCrossValidator):
-    """Leave One Label Out cross-validator
+class LeaveOneGroupOut(BaseCrossValidator):
+    """Leave One Group Out cross-validator
 
     Provides train/test indices to split data according to a third-party
-    provided label. This label information can be used to encode arbitrary
+    provided group. This group information can be used to encode arbitrary
     domain specific stratifications of the samples as integers.
 
-    For instance the labels could be the year of collection of the samples
+    For instance the groups could be the year of collection of the samples
     and thus allow for cross-validation against time-based splits.
 
     Read more in the :ref:`User Guide <cross_validation>`.
 
     Examples
     --------
-    >>> from sklearn.model_selection import LeaveOneLabelOut
+    >>> from sklearn.model_selection import LeaveOneGroupOut
     >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
     >>> y = np.array([1, 2, 1, 2])
-    >>> labels = np.array([1, 1, 2, 2])
-    >>> lol = LeaveOneLabelOut()
-    >>> lol.get_n_splits(X, y, labels)
+    >>> groups = np.array([1, 1, 2, 2])
+    >>> lol = LeaveOneGroupOut()
+    >>> lol.get_n_splits(X, y, groups)
     2
     >>> print(lol)
-    LeaveOneLabelOut()
-    >>> for train_index, test_index in lol.split(X, y, labels):
+    LeaveOneGroupOut()
+    >>> for train_index, test_index in lol.split(X, y, groups):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
     ...    y_train, y_test = y[train_index], y[test_index]
@@ -766,16 +767,16 @@ class LeaveOneLabelOut(BaseCrossValidator):
 
     """
 
-    def _iter_test_masks(self, X, y, labels):
-        if labels is None:
-            raise ValueError("The labels parameter should not be None")
-        # We make a copy of labels to avoid side-effects during iteration
-        labels = np.array(labels, copy=True)
-        unique_labels = np.unique(labels)
-        for i in unique_labels:
-            yield labels == i
+    def _iter_test_masks(self, X, y, groups):
+        if groups is None:
+            raise ValueError("The groups parameter should not be None")
+        # We make a copy of groups to avoid side-effects during iteration
+        groups = np.array(groups, copy=True)
+        unique_groups = np.unique(groups)
+        for i in unique_groups:
+            yield groups == i
 
-    def get_n_splits(self, X, y, labels):
+    def get_n_splits(self, X, y, groups):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -786,7 +787,7 @@ def get_n_splits(self, X, y, labels):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -795,45 +796,45 @@ def get_n_splits(self, X, y, labels):
         n_splits : int
             Returns the number of splitting iterations in the cross-validator.
         """
-        if labels is None:
-            raise ValueError("The labels parameter should not be None")
-        return len(np.unique(labels))
+        if groups is None:
+            raise ValueError("The groups parameter should not be None")
+        return len(np.unique(groups))
 
 
-class LeavePLabelOut(BaseCrossValidator):
-    """Leave P Labels Out cross-validator
+class LeavePGroupsOut(BaseCrossValidator):
+    """Leave P Group(s) Out cross-validator
 
     Provides train/test indices to split data according to a third-party
-    provided label. This label information can be used to encode arbitrary
+    provided group. This group information can be used to encode arbitrary
     domain specific stratifications of the samples as integers.
 
-    For instance the labels could be the year of collection of the samples
+    For instance the groups could be the year of collection of the samples
     and thus allow for cross-validation against time-based splits.
 
-    The difference between LeavePLabelOut and LeaveOneLabelOut is that
+    The difference between LeavePGroupsOut and LeaveOneGroupOut is that
     the former builds the test sets with all the samples assigned to
-    ``p`` different values of the labels while the latter uses samples
-    all assigned the same labels.
+    ``p`` different values of the groups while the latter uses samples
+    all assigned the same groups.
 
     Read more in the :ref:`User Guide <cross_validation>`.
 
     Parameters
     ----------
-    n_labels : int
-        Number of labels (``p``) to leave out in the test split.
+    n_groups : int
+        Number of groups (``p``) to leave out in the test split.
 
     Examples
     --------
-    >>> from sklearn.model_selection import LeavePLabelOut
+    >>> from sklearn.model_selection import LeavePGroupsOut
     >>> X = np.array([[1, 2], [3, 4], [5, 6]])
     >>> y = np.array([1, 2, 1])
-    >>> labels = np.array([1, 2, 3])
-    >>> lpl = LeavePLabelOut(n_labels=2)
-    >>> lpl.get_n_splits(X, y, labels)
+    >>> groups = np.array([1, 2, 3])
+    >>> lpl = LeavePGroupsOut(n_groups=2)
+    >>> lpl.get_n_splits(X, y, groups)
     3
     >>> print(lpl)
-    LeavePLabelOut(n_labels=2)
-    >>> for train_index, test_index in lpl.split(X, y, labels):
+    LeavePGroupsOut(n_groups=2)
+    >>> for train_index, test_index in lpl.split(X, y, groups):
     ...    print("TRAIN:", train_index, "TEST:", test_index)
     ...    X_train, X_test = X[train_index], X[test_index]
     ...    y_train, y_test = y[train_index], y[test_index]
@@ -850,25 +851,25 @@ class LeavePLabelOut(BaseCrossValidator):
 
     See also
     --------
-    LabelKFold: K-fold iterator variant with non-overlapping labels.
+    GroupKFold: K-fold iterator variant with non-overlapping groups.
     """
 
-    def __init__(self, n_labels):
-        self.n_labels = n_labels
+    def __init__(self, n_groups):
+        self.n_groups = n_groups
 
-    def _iter_test_masks(self, X, y, labels):
-        if labels is None:
-            raise ValueError("The labels parameter should not be None")
-        labels = np.array(labels, copy=True)
-        unique_labels = np.unique(labels)
-        combi = combinations(range(len(unique_labels)), self.n_labels)
+    def _iter_test_masks(self, X, y, groups):
+        if groups is None:
+            raise ValueError("The groups parameter should not be None")
+        groups = np.array(groups, copy=True)
+        unique_groups = np.unique(groups)
+        combi = combinations(range(len(unique_groups)), self.n_groups)
         for indices in combi:
             test_index = np.zeros(_num_samples(X), dtype=np.bool)
-            for l in unique_labels[np.array(indices)]:
-                test_index[labels == l] = True
+            for l in unique_groups[np.array(indices)]:
+                test_index[groups == l] = True
             yield test_index
 
-    def get_n_splits(self, X, y, labels):
+    def get_n_splits(self, X, y, groups):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -879,7 +880,7 @@ def get_n_splits(self, X, y, labels):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -888,9 +889,9 @@ def get_n_splits(self, X, y, labels):
         n_splits : int
             Returns the number of splitting iterations in the cross-validator.
         """
-        if labels is None:
-            raise ValueError("The labels parameter should not be None")
-        return int(comb(len(np.unique(labels)), self.n_labels, exact=True))
+        if groups is None:
+            raise ValueError("The groups parameter should not be None")
+        return int(comb(len(np.unique(groups)), self.n_groups, exact=True))
 
 
 class BaseShuffleSplit(with_metaclass(ABCMeta)):
@@ -904,7 +905,7 @@ def __init__(self, n_splits=10, test_size=0.1, train_size=None,
         self.train_size = train_size
         self.random_state = random_state
 
-    def split(self, X, y=None, labels=None):
+    def split(self, X, y=None, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -916,7 +917,7 @@ def split(self, X, y=None, labels=None):
         y : array-like, shape (n_samples,)
             The target variable for supervised learning problems.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -928,15 +929,15 @@ def split(self, X, y=None, labels=None):
         test : ndarray
             The testing set indices for that split.
         """
-        X, y, labels = indexable(X, y, labels)
-        for train, test in self._iter_indices(X, y, labels):
+        X, y, groups = indexable(X, y, groups)
+        for train, test in self._iter_indices(X, y, groups):
             yield train, test
 
     @abstractmethod
-    def _iter_indices(self, X, y=None, labels=None):
+    def _iter_indices(self, X, y=None, groups=None):
         """Generate (train, test) indices"""
 
-    def get_n_splits(self, X=None, y=None, labels=None):
+    def get_n_splits(self, X=None, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -947,7 +948,7 @@ def get_n_splits(self, X=None, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
 
         Returns
@@ -1018,7 +1019,7 @@ class ShuffleSplit(BaseShuffleSplit):
     TRAIN: [0 2] TEST: [3]
     """
 
-    def _iter_indices(self, X, y=None, labels=None):
+    def _iter_indices(self, X, y=None, groups=None):
         n_samples = _num_samples(X)
         n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,
                                                   self.train_size)
@@ -1031,26 +1032,26 @@ def _iter_indices(self, X, y=None, labels=None):
             yield ind_train, ind_test
 
 
-class LabelShuffleSplit(ShuffleSplit):
-    '''Shuffle-Labels-Out cross-validation iterator
+class GroupShuffleSplit(ShuffleSplit):
+    '''Shuffle-Group(s)-Out cross-validation iterator
 
     Provides randomized train/test indices to split data according to a
-    third-party provided label. This label information can be used to encode
+    third-party provided group. This group information can be used to encode
     arbitrary domain specific stratifications of the samples as integers.
 
-    For instance the labels could be the year of collection of the samples
+    For instance the groups could be the year of collection of the samples
     and thus allow for cross-validation against time-based splits.
 
-    The difference between LeavePLabelOut and LabelShuffleSplit is that
-    the former generates splits using all subsets of size ``p`` unique labels,
-    whereas LabelShuffleSplit generates a user-determined number of random
-    test splits, each with a user-determined fraction of unique labels.
+    The difference between LeavePGroupsOut and GroupShuffleSplit is that
+    the former generates splits using all subsets of size ``p`` unique groups,
+    whereas GroupShuffleSplit generates a user-determined number of random
+    test splits, each with a user-determined fraction of unique groups.
 
     For example, a less computationally intensive alternative to
-    ``LeavePLabelOut(p=10)`` would be
-    ``LabelShuffleSplit(test_size=10, n_splits=100)``.
+    ``LeavePGroupsOut(p=10)`` would be
+    ``GroupShuffleSplit(test_size=10, n_splits=100)``.
 
-    Note: The parameters ``test_size`` and ``train_size`` refer to labels, and
+    Note: The parameters ``test_size`` and ``train_size`` refer to groups, and
     not to samples, as in ShuffleSplit.
 
 
@@ -1061,14 +1062,14 @@ class LabelShuffleSplit(ShuffleSplit):
 
     test_size : float (default 0.2), int, or None
         If float, should be between 0.0 and 1.0 and represent the
-        proportion of the labels to include in the test split. If
-        int, represents the absolute number of test labels. If None,
+        proportion of the groups to include in the test split. If
+        int, represents the absolute number of test groups. If None,
         the value is automatically set to the complement of the train size.
 
     train_size : float, int, or None (default is None)
         If float, should be between 0.0 and 1.0 and represent the
-        proportion of the labels to include in the train split. If
-        int, represents the absolute number of train labels. If None,
+        proportion of the groups to include in the train split. If
+        int, represents the absolute number of train groups. If None,
         the value is automatically set to the complement of the test size.
 
     random_state : int or RandomState
@@ -1077,27 +1078,94 @@ class LabelShuffleSplit(ShuffleSplit):
 
     def __init__(self, n_splits=5, test_size=0.2, train_size=None,
                  random_state=None):
-        super(LabelShuffleSplit, self).__init__(
+        super(GroupShuffleSplit, self).__init__(
             n_splits=n_splits,
             test_size=test_size,
             train_size=train_size,
             random_state=random_state)
 
-    def _iter_indices(self, X, y, labels):
-        if labels is None:
-            raise ValueError("The labels parameter should not be None")
-        classes, label_indices = np.unique(labels, return_inverse=True)
-        for label_train, label_test in super(
-                LabelShuffleSplit, self)._iter_indices(X=classes):
+    def _iter_indices(self, X, y, groups):
+        if groups is None:
+            raise ValueError("The groups parameter should not be None")
+        classes, group_indices = np.unique(groups, return_inverse=True)
+        for group_train, group_test in super(
+                GroupShuffleSplit, self)._iter_indices(X=classes):
             # these are the indices of classes in the partition
             # invert them into data indices
 
-            train = np.flatnonzero(np.in1d(label_indices, label_train))
-            test = np.flatnonzero(np.in1d(label_indices, label_test))
+            train = np.flatnonzero(np.in1d(group_indices, group_train))
+            test = np.flatnonzero(np.in1d(group_indices, group_test))
 
             yield train, test
 
 
+def _approximate_mode(class_counts, n_draws, rng):
+    """Computes approximate mode of multivariate hypergeometric.
+
+    This is an approximation to the mode of the multivariate
+    hypergeometric given by class_counts and n_draws.
+    It shouldn't be off by more than one.
+
+    It is the mostly likely outcome of drawing n_draws many
+    samples from the population given by class_counts.
+
+    Parameters
+    ----------
+    class_counts : ndarray of int
+        Population per class.
+    n_draws : int
+        Number of draws (samples to draw) from the overall population.
+    rng : random state
+        Used to break ties.
+
+    Returns
+    -------
+    sampled_classes : ndarray of int
+        Number of samples drawn from each class.
+        np.sum(sampled_classes) == n_draws
+
+    Examples
+    --------
+    >>> from sklearn.model_selection._split import _approximate_mode
+    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)
+    array([2, 1])
+    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)
+    array([3, 1])
+    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),
+    ...                   n_draws=2, rng=0)
+    array([0, 1, 1, 0])
+    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),
+    ...                   n_draws=2, rng=42)
+    array([1, 1, 0, 0])
+    """
+    # this computes a bad approximation to the mode of the
+    # multivariate hypergeometric given by class_counts and n_draws
+    continuous = n_draws * class_counts / class_counts.sum()
+    # floored means we don't overshoot n_samples, but probably undershoot
+    floored = np.floor(continuous)
+    # we add samples according to how much "left over" probability
+    # they had, until we arrive at n_samples
+    need_to_add = int(n_draws - floored.sum())
+    if need_to_add > 0:
+        remainder = continuous - floored
+        values = np.sort(np.unique(remainder))[::-1]
+        # add according to remainder, but break ties
+        # randomly to avoid biases
+        for value in values:
+            inds, = np.where(remainder == value)
+            # if we need_to_add less than what's in inds
+            # we draw randomly from them.
+            # if we need to add more, we add them all and
+            # go to the next value
+            add_now = min(len(inds), need_to_add)
+            inds = choice(inds, size=add_now, replace=False, random_state=rng)
+            floored[inds] += 1
+            need_to_add -= add_now
+            if need_to_add == 0:
+                break
+    return floored.astype(np.int)
+
+
 class StratifiedShuffleSplit(BaseShuffleSplit):
     """Stratified ShuffleSplit cross-validator
 
@@ -1157,7 +1225,7 @@ def __init__(self, n_splits=10, test_size=0.1, train_size=None,
         super(StratifiedShuffleSplit, self).__init__(
             n_splits, test_size, train_size, random_state)
 
-    def _iter_indices(self, X, y, labels=None):
+    def _iter_indices(self, X, y, groups=None):
         n_samples = _num_samples(X)
         n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,
                                                   self.train_size)
@@ -1168,7 +1236,7 @@ def _iter_indices(self, X, y, labels=None):
         if np.min(class_counts) < 2:
             raise ValueError("The least populated class in y has only 1"
                              " member, which is too few. The minimum"
-                             " number of labels for any class cannot"
+                             " number of groups for any class cannot"
                              " be less than 2.")
 
         if n_train < n_classes:
@@ -1181,12 +1249,14 @@ def _iter_indices(self, X, y, labels=None):
                              (n_test, n_classes))
 
         rng = check_random_state(self.random_state)
-        p_i = class_counts / float(n_samples)
-        n_i = np.round(n_train * p_i).astype(int)
-        t_i = np.minimum(class_counts - n_i,
-                         np.round(n_test * p_i).astype(int))
 
         for _ in range(self.n_splits):
+            # if there are ties in the class-counts, we want
+            # to make sure to break them anew in each iteration
+            n_i = _approximate_mode(class_counts, n_train, rng)
+            class_counts_remaining = class_counts - n_i
+            t_i = _approximate_mode(class_counts_remaining, n_test, rng)
+
             train = []
             test = []
 
@@ -1196,29 +1266,12 @@ def _iter_indices(self, X, y, labels=None):
 
                 train.extend(perm_indices_class_i[:n_i[i]])
                 test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])
-
-            # Because of rounding issues (as n_train and n_test are not
-            # dividers of the number of elements per class), we may end
-            # up here with less samples in train and test than asked for.
-            if len(train) + len(test) < n_train + n_test:
-                # We complete by affecting randomly the missing indexes
-                missing_indices = np.where(bincount(train + test,
-                                                    minlength=len(y)) == 0)[0]
-                missing_indices = rng.permutation(missing_indices)
-                n_missing_train = n_train - len(train)
-                n_missing_test = n_test - len(test)
-
-                if n_missing_train > 0:
-                    train.extend(missing_indices[:n_missing_train])
-                if n_missing_test > 0:
-                    test.extend(missing_indices[-n_missing_test:])
-
             train = rng.permutation(train)
             test = rng.permutation(test)
 
             yield train, test
 
-    def split(self, X, y, labels=None):
+    def split(self, X, y, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -1230,7 +1283,7 @@ def split(self, X, y, labels=None):
         y : array-like, shape (n_samples,)
             The target variable for supervised learning problems.
 
-        labels : array-like, with shape (n_samples,), optional
+        groups : array-like, with shape (n_samples,), optional
             Group labels for the samples used while splitting the dataset into
             train/test set.
 
@@ -1242,7 +1295,7 @@ def split(self, X, y, labels=None):
         test : ndarray
             The testing set indices for that split.
         """
-        return super(StratifiedShuffleSplit, self).split(X, y, labels)
+        return super(StratifiedShuffleSplit, self).split(X, y, groups)
 
 
 def _validate_shuffle_split_init(test_size, train_size):
@@ -1285,13 +1338,13 @@ def _validate_shuffle_split(n_samples, test_size, train_size):
     Validation helper to check if the test/test sizes are meaningful wrt to the
     size of the data (n_samples)
     """
-    if (test_size is not None and np.asarray(test_size).dtype.kind == 'i'
-            and test_size >= n_samples):
+    if (test_size is not None and np.asarray(test_size).dtype.kind == 'i' and
+            test_size >= n_samples):
         raise ValueError('test_size=%d should be smaller than the number of '
                          'samples %d' % (test_size, n_samples))
 
-    if (train_size is not None and np.asarray(train_size).dtype.kind == 'i'
-            and train_size >= n_samples):
+    if (train_size is not None and np.asarray(train_size).dtype.kind == 'i' and
+            train_size >= n_samples):
         raise ValueError("train_size=%d should be smaller than the number of"
                          " samples %d" % (train_size, n_samples))
 
@@ -1353,7 +1406,7 @@ def __init__(self, test_fold):
         self.unique_folds = np.unique(self.test_fold)
         self.unique_folds = self.unique_folds[self.unique_folds != -1]
 
-    def split(self, X=None, y=None, labels=None):
+    def split(self, X=None, y=None, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -1364,7 +1417,7 @@ def split(self, X=None, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
 
         Returns
@@ -1389,7 +1442,7 @@ def _iter_test_masks(self):
             test_mask[test_index] = True
             yield test_mask
 
-    def get_n_splits(self, X=None, y=None, labels=None):
+    def get_n_splits(self, X=None, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -1400,7 +1453,7 @@ def get_n_splits(self, X=None, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
 
         Returns
@@ -1416,7 +1469,7 @@ class _CVIterableWrapper(BaseCrossValidator):
     def __init__(self, cv):
         self.cv = cv
 
-    def get_n_splits(self, X=None, y=None, labels=None):
+    def get_n_splits(self, X=None, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator
 
         Parameters
@@ -1427,7 +1480,7 @@ def get_n_splits(self, X=None, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
 
         Returns
@@ -1437,7 +1490,7 @@ def get_n_splits(self, X=None, y=None, labels=None):
         """
         return len(self.cv)  # Both iterables and old-cv objects support len
 
-    def split(self, X=None, y=None, labels=None):
+    def split(self, X=None, y=None, groups=None):
         """Generate indices to split data into training and test set.
 
         Parameters
@@ -1448,7 +1501,7 @@ def split(self, X=None, y=None, labels=None):
         y : object
             Always ignored, exists for compatibility.
 
-        labels : object
+        groups : object
             Always ignored, exists for compatibility.
 
         Returns
@@ -1550,7 +1603,7 @@ def train_test_split(*arrays, **options):
 
     stratify : array-like or None (default is None)
         If not None, data is split in a stratified fashion, using this as
-        the labels array.
+        the groups array.
 
     Returns
     -------
diff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py
index 183f16a10f..1ed36d9b7b 100755
--- a/sklearn/model_selection/_validation.py
+++ b/sklearn/model_selection/_validation.py
@@ -27,41 +27,13 @@
 from ..metrics.scorer import check_scoring
 from ..exceptions import FitFailedWarning
 
-from ._split import KFold
-from ._split import LabelKFold
-from ._split import LeaveOneLabelOut
-from ._split import LeaveOneOut
-from ._split import LeavePLabelOut
-from ._split import LeavePOut
-from ._split import ShuffleSplit
-from ._split import LabelShuffleSplit
-from ._split import StratifiedKFold
-from ._split import StratifiedShuffleSplit
-from ._split import PredefinedSplit
 from ._split import check_cv, _safe_split
 
 __all__ = ['cross_val_score', 'cross_val_predict', 'permutation_test_score',
            'learning_curve', 'validation_curve']
 
-ALL_CVS = {'KFold': KFold,
-           'LabelKFold': LabelKFold,
-           'LeaveOneLabelOut': LeaveOneLabelOut,
-           'LeaveOneOut': LeaveOneOut,
-           'LeavePLabelOut': LeavePLabelOut,
-           'LeavePOut': LeavePOut,
-           'ShuffleSplit': ShuffleSplit,
-           'LabelShuffleSplit': LabelShuffleSplit,
-           'StratifiedKFold': StratifiedKFold,
-           'StratifiedShuffleSplit': StratifiedShuffleSplit,
-           'PredefinedSplit': PredefinedSplit}
-
-LABEL_CVS = {'LabelKFold': LabelKFold,
-             'LeaveOneLabelOut': LeaveOneLabelOut,
-             'LeavePLabelOut': LeavePLabelOut,
-             'LabelShuffleSplit': LabelShuffleSplit}
-
-
-def cross_val_score(estimator, X, y=None, labels=None, scoring=None, cv=None,
+
+def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,
                     n_jobs=1, verbose=0, fit_params=None,
                     pre_dispatch='2*n_jobs'):
     """Evaluate a score by cross-validation
@@ -80,7 +52,7 @@ def cross_val_score(estimator, X, y=None, labels=None, scoring=None, cv=None,
         The target variable to try to predict in the case of
         supervised learning.
 
-    labels : array-like, with shape (n_samples,), optional
+    groups : array-like, with shape (n_samples,), optional
         Group labels for the samples used while splitting the dataset into
         train/test set.
 
@@ -153,7 +125,7 @@ def cross_val_score(estimator, X, y=None, labels=None, scoring=None, cv=None,
         Make a scorer from a performance metric or loss function.
 
     """
-    X, y, labels = indexable(X, y, labels)
+    X, y, groups = indexable(X, y, groups)
 
     cv = check_cv(cv, y, classifier=is_classifier(estimator))
     scorer = check_scoring(estimator, scoring=scoring)
@@ -164,7 +136,7 @@ def cross_val_score(estimator, X, y=None, labels=None, scoring=None, cv=None,
     scores = parallel(delayed(_fit_and_score)(clone(estimator), X, y, scorer,
                                               train, test, verbose, None,
                                               fit_params)
-                      for train, test in cv.split(X, y, labels))
+                      for train, test in cv.split(X, y, groups))
     return np.array(scores)[:, 0]
 
 
@@ -314,7 +286,7 @@ def _score(estimator, X_test, y_test, scorer):
     return score
 
 
-def cross_val_predict(estimator, X, y=None, labels=None, cv=None, n_jobs=1,
+def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,
                       verbose=0, fit_params=None, pre_dispatch='2*n_jobs',
                       method='predict'):
     """Generate cross-validated estimates for each input data point
@@ -333,7 +305,7 @@ def cross_val_predict(estimator, X, y=None, labels=None, cv=None, n_jobs=1,
         The target variable to try to predict in the case of
         supervised learning.
 
-    labels : array-like, with shape (n_samples,), optional
+    groups : array-like, with shape (n_samples,), optional
         Group labels for the samples used while splitting the dataset into
         train/test set.
 
@@ -397,7 +369,7 @@ def cross_val_predict(estimator, X, y=None, labels=None, cv=None, n_jobs=1,
     >>> lasso = linear_model.Lasso()
     >>> y_pred = cross_val_predict(lasso, X, y)
     """
-    X, y, labels = indexable(X, y, labels)
+    X, y, groups = indexable(X, y, groups)
 
     cv = check_cv(cv, y, classifier=is_classifier(estimator))
 
@@ -412,7 +384,7 @@ def cross_val_predict(estimator, X, y=None, labels=None, cv=None, n_jobs=1,
                         pre_dispatch=pre_dispatch)
     prediction_blocks = parallel(delayed(_fit_and_predict)(
         clone(estimator), X, y, train, test, verbose, fit_params, method)
-        for train, test in cv.split(X, y, labels))
+        for train, test in cv.split(X, y, groups))
 
     # Concatenate the predictions
     predictions = [pred_block_i for pred_block_i, _ in prediction_blocks]
@@ -525,7 +497,7 @@ def _index_param_value(X, v, indices):
     return safe_indexing(v, indices)
 
 
-def permutation_test_score(estimator, X, y, labels=None, cv=None,
+def permutation_test_score(estimator, X, y, groups=None, cv=None,
                            n_permutations=100, n_jobs=1, random_state=0,
                            verbose=0, scoring=None):
     """Evaluate the significance of a cross-validated score with permutations
@@ -544,9 +516,15 @@ def permutation_test_score(estimator, X, y, labels=None, cv=None,
         The target variable to try to predict in the case of
         supervised learning.
 
-    labels : array-like, with shape (n_samples,), optional
-        Group labels for the samples used while splitting the dataset into
-        train/test set.
+    groups : array-like, with shape (n_samples,), optional
+        Labels to constrain permutation within groups, i.e. ``y`` values
+        are permuted among samples with the same group identifier.
+        When not specified, ``y`` values are permuted among all samples.
+
+        When a grouped cross-validator is used, the group labels are
+        also passed on to the ``split`` method of the cross-validator. The
+        cross-validator uses them for grouping the samples  while splitting
+        the dataset into train/test set.
 
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
@@ -606,7 +584,7 @@ def permutation_test_score(estimator, X, y, labels=None, cv=None,
         vol. 11
 
     """
-    X, y, labels = indexable(X, y, labels)
+    X, y, groups = indexable(X, y, groups)
 
     cv = check_cv(cv, y, classifier=is_classifier(estimator))
     scorer = check_scoring(estimator, scoring=scoring)
@@ -614,11 +592,11 @@ def permutation_test_score(estimator, X, y, labels=None, cv=None,
 
     # We clone the estimator to make sure that all the folds are
     # independent, and that it is pickle-able.
-    score = _permutation_test_score(clone(estimator), X, y, labels, cv, scorer)
+    score = _permutation_test_score(clone(estimator), X, y, groups, cv, scorer)
     permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(
         delayed(_permutation_test_score)(
-            clone(estimator), X, _shuffle(y, labels, random_state),
-            labels, cv, scorer)
+            clone(estimator), X, _shuffle(y, groups, random_state),
+            groups, cv, scorer)
         for _ in range(n_permutations))
     permutation_scores = np.array(permutation_scores)
     pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)
@@ -628,28 +606,28 @@ def permutation_test_score(estimator, X, y, labels=None, cv=None,
 permutation_test_score.__test__ = False  # to avoid a pb with nosetests
 
 
-def _permutation_test_score(estimator, X, y, labels, cv, scorer):
+def _permutation_test_score(estimator, X, y, groups, cv, scorer):
     """Auxiliary function for permutation_test_score"""
     avg_score = []
-    for train, test in cv.split(X, y, labels):
+    for train, test in cv.split(X, y, groups):
         estimator.fit(X[train], y[train])
         avg_score.append(scorer(estimator, X[test], y[test]))
     return np.mean(avg_score)
 
 
-def _shuffle(y, labels, random_state):
-    """Return a shuffled copy of y eventually shuffle among same labels."""
-    if labels is None:
+def _shuffle(y, groups, random_state):
+    """Return a shuffled copy of y eventually shuffle among same groups."""
+    if groups is None:
         indices = random_state.permutation(len(y))
     else:
-        indices = np.arange(len(labels))
-        for label in np.unique(labels):
-            this_mask = (labels == label)
+        indices = np.arange(len(groups))
+        for group in np.unique(groups):
+            this_mask = (groups == group)
             indices[this_mask] = random_state.permutation(indices[this_mask])
     return y[indices]
 
 
-def learning_curve(estimator, X, y, labels=None,
+def learning_curve(estimator, X, y, groups=None,
                    train_sizes=np.linspace(0.1, 1.0, 5), cv=None, scoring=None,
                    exploit_incremental_learning=False, n_jobs=1,
                    pre_dispatch="all", verbose=0):
@@ -679,7 +657,7 @@ def learning_curve(estimator, X, y, labels=None,
         Target relative to X for classification or regression;
         None for unsupervised learning.
 
-    labels : array-like, with shape (n_samples,), optional
+    groups : array-like, with shape (n_samples,), optional
         Group labels for the samples used while splitting the dataset into
         train/test set.
 
@@ -749,10 +727,10 @@ def learning_curve(estimator, X, y, labels=None,
     if exploit_incremental_learning and not hasattr(estimator, "partial_fit"):
         raise ValueError("An estimator must support the partial_fit interface "
                          "to exploit incremental learning")
-    X, y, labels = indexable(X, y, labels)
+    X, y, groups = indexable(X, y, groups)
 
     cv = check_cv(cv, y, classifier=is_classifier(estimator))
-    cv_iter = cv.split(X, y, labels)
+    cv_iter = cv.split(X, y, groups)
     # Make a list since we will be iterating multiple times over the folds
     cv_iter = list(cv_iter)
     scorer = check_scoring(estimator, scoring=scoring)
@@ -773,7 +751,7 @@ def learning_curve(estimator, X, y, labels=None,
         classes = np.unique(y) if is_classifier(estimator) else None
         out = parallel(delayed(_incremental_fit_estimator)(
             clone(estimator), X, y, classes, train, test, train_sizes_abs,
-            scorer, verbose) for train, test in cv.split(X, y, labels))
+            scorer, verbose) for train, test in cv.split(X, y, groups))
     else:
         out = parallel(delayed(_fit_and_score)(
             clone(estimator), X, y, scorer, train[:n_train_samples], test,
@@ -869,7 +847,7 @@ def _incremental_fit_estimator(estimator, X, y, classes, train, test,
     return np.array((train_scores, test_scores)).T
 
 
-def validation_curve(estimator, X, y, param_name, param_range, labels=None,
+def validation_curve(estimator, X, y, param_name, param_range, groups=None,
                      cv=None, scoring=None, n_jobs=1, pre_dispatch="all",
                      verbose=0):
     """Validation curve.
@@ -902,7 +880,7 @@ def validation_curve(estimator, X, y, param_name, param_range, labels=None,
     param_range : array-like, shape (n_values,)
         The values of the parameter that will be evaluated.
 
-    labels : array-like, with shape (n_samples,), optional
+    groups : array-like, with shape (n_samples,), optional
         Group labels for the samples used while splitting the dataset into
         train/test set.
 
@@ -950,7 +928,7 @@ def validation_curve(estimator, X, y, param_name, param_range, labels=None,
     See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`
 
     """
-    X, y, labels = indexable(X, y, labels)
+    X, y, groups = indexable(X, y, groups)
 
     cv = check_cv(cv, y, classifier=is_classifier(estimator))
 
@@ -961,7 +939,7 @@ def validation_curve(estimator, X, y, param_name, param_range, labels=None,
     out = parallel(delayed(_fit_and_score)(
         estimator, X, y, scorer, train, test, verbose,
         parameters={param_name: v}, fit_params=None, return_train_score=True)
-        for train, test in cv.split(X, y, labels) for v in param_range)
+        for train, test in cv.split(X, y, groups) for v in param_range)
 
     out = np.asarray(out)[:, :2]
     n_params = len(param_range)
diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py
index 03eafdafb1..141c1a21b4 100755
--- a/sklearn/model_selection/tests/test_search.py
+++ b/sklearn/model_selection/tests/test_search.py
@@ -37,10 +37,10 @@
 from sklearn.model_selection import KFold
 from sklearn.model_selection import StratifiedKFold
 from sklearn.model_selection import StratifiedShuffleSplit
-from sklearn.model_selection import LeaveOneLabelOut
-from sklearn.model_selection import LeavePLabelOut
-from sklearn.model_selection import LabelKFold
-from sklearn.model_selection import LabelShuffleSplit
+from sklearn.model_selection import LeaveOneGroupOut
+from sklearn.model_selection import LeavePGroupsOut
+from sklearn.model_selection import GroupKFold
+from sklearn.model_selection import GroupShuffleSplit
 from sklearn.model_selection import GridSearchCV
 from sklearn.model_selection import RandomizedSearchCV
 from sklearn.model_selection import ParameterGrid
@@ -224,28 +224,28 @@ def test_grid_search_score_method():
     assert_almost_equal(score_auc, score_no_score_auc)
 
 
-def test_grid_search_labels():
-    # Check if ValueError (when labels is None) propagates to GridSearchCV
-    # And also check if labels is correctly passed to the cv object
+def test_grid_search_groups():
+    # Check if ValueError (when groups is None) propagates to GridSearchCV
+    # And also check if groups is correctly passed to the cv object
     rng = np.random.RandomState(0)
 
     X, y = make_classification(n_samples=15, n_classes=2, random_state=0)
-    labels = rng.randint(0, 3, 15)
+    groups = rng.randint(0, 3, 15)
 
     clf = LinearSVC(random_state=0)
     grid = {'C': [1]}
 
-    label_cvs = [LeaveOneLabelOut(), LeavePLabelOut(2), LabelKFold(),
-                 LabelShuffleSplit()]
-    for cv in label_cvs:
+    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2), GroupKFold(),
+                 GroupShuffleSplit()]
+    for cv in group_cvs:
         gs = GridSearchCV(clf, grid, cv=cv)
         assert_raise_message(ValueError,
-                             "The labels parameter should not be None",
+                             "The groups parameter should not be None",
                              gs.fit, X, y)
-        gs.fit(X, y, labels)
+        gs.fit(X, y, groups=groups)
 
-    non_label_cvs = [StratifiedKFold(), StratifiedShuffleSplit()]
-    for cv in non_label_cvs:
+    non_group_cvs = [StratifiedKFold(), StratifiedShuffleSplit()]
+    for cv in non_group_cvs:
         gs = GridSearchCV(clf, grid, cv=cv)
         # Should not raise an error
         gs.fit(X, y)
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
index d28148efe6..ec4f07aef8 100755
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -29,14 +29,14 @@
 from sklearn.model_selection import cross_val_score
 from sklearn.model_selection import KFold
 from sklearn.model_selection import StratifiedKFold
-from sklearn.model_selection import LabelKFold
+from sklearn.model_selection import GroupKFold
 from sklearn.model_selection import TimeSeriesSplit
 from sklearn.model_selection import LeaveOneOut
-from sklearn.model_selection import LeaveOneLabelOut
+from sklearn.model_selection import LeaveOneGroupOut
 from sklearn.model_selection import LeavePOut
-from sklearn.model_selection import LeavePLabelOut
+from sklearn.model_selection import LeavePGroupsOut
 from sklearn.model_selection import ShuffleSplit
-from sklearn.model_selection import LabelShuffleSplit
+from sklearn.model_selection import GroupShuffleSplit
 from sklearn.model_selection import StratifiedShuffleSplit
 from sklearn.model_selection import PredefinedSplit
 from sklearn.model_selection import check_cv
@@ -132,7 +132,7 @@ def get_params(self, deep=False):
 @ignore_warnings
 def test_cross_validator_with_default_params():
     n_samples = 4
-    n_unique_labels = 4
+    n_unique_groups = 4
     n_splits = 2
     p = 2
     n_shuffle_splits = 10  # (the default value)
@@ -140,13 +140,13 @@ def test_cross_validator_with_default_params():
     X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
     X_1d = np.array([1, 2, 3, 4])
     y = np.array([1, 1, 2, 2])
-    labels = np.array([1, 2, 3, 4])
+    groups = np.array([1, 2, 3, 4])
     loo = LeaveOneOut()
     lpo = LeavePOut(p)
     kf = KFold(n_splits)
     skf = StratifiedKFold(n_splits)
-    lolo = LeaveOneLabelOut()
-    lopo = LeavePLabelOut(p)
+    lolo = LeaveOneGroupOut()
+    lopo = LeavePGroupsOut(p)
     ss = ShuffleSplit(random_state=0)
     ps = PredefinedSplit([1, 1, 2, 2])  # n_splits = np of unique folds = 2
 
@@ -154,14 +154,14 @@ def test_cross_validator_with_default_params():
     lpo_repr = "LeavePOut(p=2)"
     kf_repr = "KFold(n_splits=2, random_state=None, shuffle=False)"
     skf_repr = "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)"
-    lolo_repr = "LeaveOneLabelOut()"
-    lopo_repr = "LeavePLabelOut(n_labels=2)"
+    lolo_repr = "LeaveOneGroupOut()"
+    lopo_repr = "LeavePGroupsOut(n_groups=2)"
     ss_repr = ("ShuffleSplit(n_splits=10, random_state=0, test_size=0.1, "
                "train_size=None)")
     ps_repr = "PredefinedSplit(test_fold=array([1, 1, 2, 2]))"
 
     n_splits_expected = [n_samples, comb(n_samples, p), n_splits, n_splits,
-                         n_unique_labels, comb(n_unique_labels, p),
+                         n_unique_groups, comb(n_unique_groups, p),
                          n_shuffle_splits, 2]
 
     for i, (cv, cv_repr) in enumerate(zip(
@@ -169,14 +169,14 @@ def test_cross_validator_with_default_params():
             [loo_repr, lpo_repr, kf_repr, skf_repr, lolo_repr, lopo_repr,
              ss_repr, ps_repr])):
         # Test if get_n_splits works correctly
-        assert_equal(n_splits_expected[i], cv.get_n_splits(X, y, labels))
+        assert_equal(n_splits_expected[i], cv.get_n_splits(X, y, groups))
 
         # Test if the cross-validator works as expected even if
         # the data is 1d
-        np.testing.assert_equal(list(cv.split(X, y, labels)),
-                                list(cv.split(X_1d, y, labels)))
+        np.testing.assert_equal(list(cv.split(X, y, groups)),
+                                list(cv.split(X_1d, y, groups)))
         # Test that train, test indices returned are integers
-        for train, test in cv.split(X, y, labels):
+        for train, test in cv.split(X, y, groups):
             assert_equal(np.asarray(train).dtype.kind, 'i')
             assert_equal(np.asarray(train).dtype.kind, 'i')
 
@@ -196,17 +196,17 @@ def check_valid_split(train, test, n_samples=None):
         assert_equal(train.union(test), set(range(n_samples)))
 
 
-def check_cv_coverage(cv, X, y, labels, expected_n_splits=None):
+def check_cv_coverage(cv, X, y, groups, expected_n_splits=None):
     n_samples = _num_samples(X)
     # Check that a all the samples appear at least once in a test fold
     if expected_n_splits is not None:
-        assert_equal(cv.get_n_splits(X, y, labels), expected_n_splits)
+        assert_equal(cv.get_n_splits(X, y, groups), expected_n_splits)
     else:
-        expected_n_splits = cv.get_n_splits(X, y, labels)
+        expected_n_splits = cv.get_n_splits(X, y, groups)
 
     collected_test_samples = set()
     iterations = 0
-    for train, test in cv.split(X, y, labels):
+    for train, test in cv.split(X, y, groups):
         check_valid_split(train, test, n_samples=n_samples)
         iterations += 1
         collected_test_samples.update(test)
@@ -236,9 +236,9 @@ def test_kfold_valueerrors():
     # side of the split at each split
     with warnings.catch_warnings():
         warnings.simplefilter("ignore")
-        check_cv_coverage(skf_3, X2, y, labels=None, expected_n_splits=3)
+        check_cv_coverage(skf_3, X2, y, groups=None, expected_n_splits=3)
 
-    # Check that errors are raised if all n_labels for individual
+    # Check that errors are raised if all n_groups for individual
     # classes are less than n_splits.
     y = np.array([3, 3, -1, -1, 2])
 
@@ -268,13 +268,13 @@ def test_kfold_indices():
     # Check all indices are returned in the test folds
     X1 = np.ones(18)
     kf = KFold(3)
-    check_cv_coverage(kf, X1, y=None, labels=None, expected_n_splits=3)
+    check_cv_coverage(kf, X1, y=None, groups=None, expected_n_splits=3)
 
     # Check all indices are returned in the test folds even when equal-sized
     # folds are not possible
     X2 = np.ones(17)
     kf = KFold(3)
-    check_cv_coverage(kf, X2, y=None, labels=None, expected_n_splits=3)
+    check_cv_coverage(kf, X2, y=None, groups=None, expected_n_splits=3)
 
     # Check if get_n_splits returns the number of folds
     assert_equal(5, KFold(5).get_n_splits(X2))
@@ -443,7 +443,7 @@ def test_shuffle_stratifiedkfold():
     for (_, test0), (_, test1) in zip(kf0.split(X_40, y),
                                       kf1.split(X_40, y)):
         assert_not_equal(set(test0), set(test1))
-    check_cv_coverage(kf0, X_40, y, labels=None, expected_n_splits=5)
+    check_cv_coverage(kf0, X_40, y, groups=None, expected_n_splits=5)
 
 
 def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
@@ -535,17 +535,33 @@ def test_stratified_shuffle_split_init():
                   StratifiedShuffleSplit(test_size=2).split(X, y))
 
 
+def test_stratified_shuffle_split_respects_test_size():
+    y = np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2])
+    test_size = 5
+    train_size = 10
+    sss = StratifiedShuffleSplit(6, test_size=test_size, train_size=train_size,
+                                 random_state=0).split(np.ones(len(y)), y)
+    for train, test in sss:
+        assert_equal(len(train), train_size)
+        assert_equal(len(test), test_size)
+
+
 def test_stratified_shuffle_split_iter():
     ys = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),
           np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),
-          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),
+          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),
           np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),
-          np.array([-1] * 800 + [1] * 50)
+          np.array([-1] * 800 + [1] * 50),
+          np.concatenate([[i] * (100 + i) for i in range(11)])
           ]
 
     for y in ys:
         sss = StratifiedShuffleSplit(6, test_size=0.33,
                                      random_state=0).split(np.ones(len(y)), y)
+        # this is how test-size is computed internally
+        # in _validate_shuffle_split
+        test_size = np.ceil(0.33 * len(y))
+        train_size = len(y) - test_size
         for train, test in sss:
             assert_array_equal(np.unique(y[train]), np.unique(y[test]))
             # Checks if folds keep classes proportions
@@ -556,7 +572,9 @@ def test_stratified_shuffle_split_iter():
                                   return_inverse=True)[1]) /
                       float(len(y[test])))
             assert_array_almost_equal(p_train, p_test, 1)
-            assert_equal(y[train].size + y[test].size, y.size)
+            assert_equal(len(train) + len(test), y.size)
+            assert_equal(len(train), train_size)
+            assert_equal(len(test), test_size)
             assert_array_equal(np.lib.arraysetops.intersect1d(train, test), [])
 
 
@@ -572,13 +590,13 @@ def assert_counts_are_ok(idx_counts, p):
         threshold = 0.05 / n_splits
         bf = stats.binom(n_splits, p)
         for count in idx_counts:
-            p = bf.pmf(count)
-            assert_true(p > threshold,
+            prob = bf.pmf(count)
+            assert_true(prob > threshold,
                         "An index is not drawn with chance corresponding "
                         "to even draws")
 
     for n_samples in (6, 22):
-        labels = np.array((n_samples // 2) * [0, 1])
+        groups = np.array((n_samples // 2) * [0, 1])
         splits = StratifiedShuffleSplit(n_splits=n_splits,
                                         test_size=1. / n_folds,
                                         random_state=0)
@@ -586,25 +604,24 @@ def assert_counts_are_ok(idx_counts, p):
         train_counts = [0] * n_samples
         test_counts = [0] * n_samples
         n_splits_actual = 0
-        for train, test in splits.split(X=np.ones(n_samples), y=labels):
+        for train, test in splits.split(X=np.ones(n_samples), y=groups):
             n_splits_actual += 1
             for counter, ids in [(train_counts, train), (test_counts, test)]:
                 for id in ids:
                     counter[id] += 1
         assert_equal(n_splits_actual, n_splits)
 
-        n_train, n_test = _validate_shuffle_split(n_samples,
-                                                  test_size=1./n_folds,
-                                                  train_size=1.-(1./n_folds))
+        n_train, n_test = _validate_shuffle_split(
+            n_samples, test_size=1. / n_folds, train_size=1. - (1. / n_folds))
 
         assert_equal(len(train), n_train)
         assert_equal(len(test), n_test)
         assert_equal(len(set(train).intersection(test)), 0)
 
-        label_counts = np.unique(labels)
+        group_counts = np.unique(groups)
         assert_equal(splits.test_size, 1.0 / n_folds)
-        assert_equal(n_train + n_test, len(labels))
-        assert_equal(len(label_counts), 2)
+        assert_equal(n_train + n_test, len(groups))
+        assert_equal(len(group_counts), 2)
         ex_test_p = float(n_test) / n_samples
         ex_train_p = float(n_train) / n_samples
 
@@ -647,28 +664,28 @@ def test_predefinedsplit_with_kfold_split():
     assert_array_equal(ps_test, kf_test)
 
 
-def test_label_shuffle_split():
-    labels = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),
+def test_group_shuffle_split():
+    groups = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),
               np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),
               np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),
               np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]
 
-    for l in labels:
+    for l in groups:
         X = y = np.ones(len(l))
         n_splits = 6
         test_size = 1./3
-        slo = LabelShuffleSplit(n_splits, test_size=test_size, random_state=0)
+        slo = GroupShuffleSplit(n_splits, test_size=test_size, random_state=0)
 
         # Make sure the repr works
         repr(slo)
 
         # Test that the length is correct
-        assert_equal(slo.get_n_splits(X, y, labels=l), n_splits)
+        assert_equal(slo.get_n_splits(X, y, groups=l), n_splits)
 
         l_unique = np.unique(l)
 
-        for train, test in slo.split(X, y, labels=l):
-            # First test: no train label is in the test set and vice versa
+        for train, test in slo.split(X, y, groups=l):
+            # First test: no train group is in the test set and vice versa
             l_train_unique = np.unique(l[train])
             l_test_unique = np.unique(l[test])
             assert_false(np.any(np.in1d(l[train], l_test_unique)))
@@ -681,33 +698,33 @@ def test_label_shuffle_split():
             assert_array_equal(np.intersect1d(train, test), [])
 
             # Fourth test:
-            # unique train and test labels are correct, +- 1 for rounding error
+            # unique train and test groups are correct, +- 1 for rounding error
             assert_true(abs(len(l_test_unique) -
                             round(test_size * len(l_unique))) <= 1)
             assert_true(abs(len(l_train_unique) -
                             round((1.0 - test_size) * len(l_unique))) <= 1)
 
 
-def test_leave_label_out_changing_labels():
-    # Check that LeaveOneLabelOut and LeavePLabelOut work normally if
-    # the labels variable is changed before calling split
-    labels = np.array([0, 1, 2, 1, 1, 2, 0, 0])
-    X = np.ones(len(labels))
-    labels_changing = np.array(labels, copy=True)
-    lolo = LeaveOneLabelOut().split(X, labels=labels)
-    lolo_changing = LeaveOneLabelOut().split(X, labels=labels)
-    lplo = LeavePLabelOut(n_labels=2).split(X, labels=labels)
-    lplo_changing = LeavePLabelOut(n_labels=2).split(X, labels=labels)
-    labels_changing[:] = 0
+def test_leave_group_out_changing_groups():
+    # Check that LeaveOneGroupOut and LeavePGroupsOut work normally if
+    # the groups variable is changed before calling split
+    groups = np.array([0, 1, 2, 1, 1, 2, 0, 0])
+    X = np.ones(len(groups))
+    groups_changing = np.array(groups, copy=True)
+    lolo = LeaveOneGroupOut().split(X, groups=groups)
+    lolo_changing = LeaveOneGroupOut().split(X, groups=groups)
+    lplo = LeavePGroupsOut(n_groups=2).split(X, groups=groups)
+    lplo_changing = LeavePGroupsOut(n_groups=2).split(X, groups=groups)
+    groups_changing[:] = 0
     for llo, llo_changing in [(lolo, lolo_changing), (lplo, lplo_changing)]:
         for (train, test), (train_chan, test_chan) in zip(llo, llo_changing):
             assert_array_equal(train, train_chan)
             assert_array_equal(test, test_chan)
 
-    # n_splits = no of 2 (p) label combinations of the unique labels = 3C2 = 3
-    assert_equal(3, LeavePLabelOut(n_labels=2).get_n_splits(X, y, labels))
-    # n_splits = no of unique labels (C(uniq_lbls, 1) = n_unique_labels)
-    assert_equal(3, LeaveOneLabelOut().get_n_splits(X, y, labels))
+    # n_splits = no of 2 (p) group combinations of the unique groups = 3C2 = 3
+    assert_equal(3, LeavePGroupsOut(n_groups=2).get_n_splits(X, y, groups))
+    # n_splits = no of unique groups (C(uniq_lbls, 1) = n_unique_groups)
+    assert_equal(3, LeaveOneGroupOut().get_n_splits(X, y, groups))
 
 
 def test_train_test_split_errors():
@@ -914,11 +931,11 @@ def test_cv_iterable_wrapper():
     assert_equal(len(cv), wrapped_old_skf.get_n_splits())
 
 
-def test_label_kfold():
+def test_group_kfold():
     rng = np.random.RandomState(0)
 
     # Parameters of the test
-    n_labels = 15
+    n_groups = 15
     n_samples = 1000
     n_splits = 5
 
@@ -926,34 +943,34 @@ def test_label_kfold():
 
     # Construct the test data
     tolerance = 0.05 * n_samples  # 5 percent error allowed
-    labels = rng.randint(0, n_labels, n_samples)
+    groups = rng.randint(0, n_groups, n_samples)
 
-    ideal_n_labels_per_fold = n_samples // n_splits
+    ideal_n_groups_per_fold = n_samples // n_splits
 
-    len(np.unique(labels))
+    len(np.unique(groups))
     # Get the test fold indices from the test set indices of each fold
     folds = np.zeros(n_samples)
-    lkf = LabelKFold(n_splits=n_splits)
-    for i, (_, test) in enumerate(lkf.split(X, y, labels)):
+    lkf = GroupKFold(n_splits=n_splits)
+    for i, (_, test) in enumerate(lkf.split(X, y, groups)):
         folds[test] = i
 
     # Check that folds have approximately the same size
-    assert_equal(len(folds), len(labels))
+    assert_equal(len(folds), len(groups))
     for i in np.unique(folds):
         assert_greater_equal(tolerance,
-                             abs(sum(folds == i) - ideal_n_labels_per_fold))
+                             abs(sum(folds == i) - ideal_n_groups_per_fold))
 
-    # Check that each label appears only in 1 fold
-    for label in np.unique(labels):
-        assert_equal(len(np.unique(folds[labels == label])), 1)
+    # Check that each group appears only in 1 fold
+    for group in np.unique(groups):
+        assert_equal(len(np.unique(folds[groups == group])), 1)
 
-    # Check that no label is on both sides of the split
-    labels = np.asarray(labels, dtype=object)
-    for train, test in lkf.split(X, y, labels):
-        assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)
+    # Check that no group is on both sides of the split
+    groups = np.asarray(groups, dtype=object)
+    for train, test in lkf.split(X, y, groups):
+        assert_equal(len(np.intersect1d(groups[train], groups[test])), 0)
 
     # Construct the test data
-    labels = np.array(['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean',
+    groups = np.array(['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean',
                        'Francis', 'Robert', 'Michel', 'Rachel', 'Lois',
                        'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean',
                        'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix',
@@ -961,41 +978,41 @@ def test_label_kfold():
                        'Madmood', 'Cary', 'Mary', 'Alexandre', 'David',
                        'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia'])
 
-    n_labels = len(np.unique(labels))
-    n_samples = len(labels)
+    n_groups = len(np.unique(groups))
+    n_samples = len(groups)
     n_splits = 5
     tolerance = 0.05 * n_samples  # 5 percent error allowed
-    ideal_n_labels_per_fold = n_samples // n_splits
+    ideal_n_groups_per_fold = n_samples // n_splits
 
     X = y = np.ones(n_samples)
 
     # Get the test fold indices from the test set indices of each fold
     folds = np.zeros(n_samples)
-    for i, (_, test) in enumerate(lkf.split(X, y, labels)):
+    for i, (_, test) in enumerate(lkf.split(X, y, groups)):
         folds[test] = i
 
     # Check that folds have approximately the same size
-    assert_equal(len(folds), len(labels))
+    assert_equal(len(folds), len(groups))
     for i in np.unique(folds):
         assert_greater_equal(tolerance,
-                             abs(sum(folds == i) - ideal_n_labels_per_fold))
+                             abs(sum(folds == i) - ideal_n_groups_per_fold))
 
-    # Check that each label appears only in 1 fold
+    # Check that each group appears only in 1 fold
     with warnings.catch_warnings():
         warnings.simplefilter("ignore", DeprecationWarning)
-        for label in np.unique(labels):
-            assert_equal(len(np.unique(folds[labels == label])), 1)
+        for group in np.unique(groups):
+            assert_equal(len(np.unique(folds[groups == group])), 1)
 
-    # Check that no label is on both sides of the split
-    labels = np.asarray(labels, dtype=object)
-    for train, test in lkf.split(X, y, labels):
-        assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)
+    # Check that no group is on both sides of the split
+    groups = np.asarray(groups, dtype=object)
+    for train, test in lkf.split(X, y, groups):
+        assert_equal(len(np.intersect1d(groups[train], groups[test])), 0)
 
-    # Should fail if there are more folds than labels
-    labels = np.array([1, 1, 1, 2, 2])
-    X = y = np.ones(len(labels))
+    # Should fail if there are more folds than groups
+    groups = np.array([1, 1, 1, 2, 2])
+    X = y = np.ones(len(groups))
     assert_raises_regexp(ValueError, "Cannot have number of splits.*greater",
-                         next, LabelKFold(n_splits=3).split(X, y, labels))
+                         next, GroupKFold(n_splits=3).split(X, y, groups))
 
 
 def test_time_series_cv():
@@ -1041,16 +1058,16 @@ def test_nested_cv():
     rng = np.random.RandomState(0)
 
     X, y = make_classification(n_samples=15, n_classes=2, random_state=0)
-    labels = rng.randint(0, 5, 15)
+    groups = rng.randint(0, 5, 15)
 
-    cvs = [LeaveOneLabelOut(), LeaveOneOut(), LabelKFold(), StratifiedKFold(),
+    cvs = [LeaveOneGroupOut(), LeaveOneOut(), GroupKFold(), StratifiedKFold(),
            StratifiedShuffleSplit(n_splits=3, random_state=0)]
 
     for inner_cv, outer_cv in combinations_with_replacement(cvs, 2):
         gs = GridSearchCV(Ridge(), param_grid={'alpha': [1, .1]},
                           cv=inner_cv)
-        cross_val_score(gs, X=X, y=y, labels=labels, cv=outer_cv,
-                        fit_params={'labels': labels})
+        cross_val_score(gs, X=X, y=y, groups=groups, cv=outer_cv,
+                        fit_params={'groups': groups})
 
 
 def test_build_repr():
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
index 62a8600056..67937711ec 100755
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -29,10 +29,10 @@
 from sklearn.model_selection import KFold
 from sklearn.model_selection import StratifiedKFold
 from sklearn.model_selection import LeaveOneOut
-from sklearn.model_selection import LeaveOneLabelOut
-from sklearn.model_selection import LeavePLabelOut
-from sklearn.model_selection import LabelKFold
-from sklearn.model_selection import LabelShuffleSplit
+from sklearn.model_selection import LeaveOneGroupOut
+from sklearn.model_selection import LeavePGroupsOut
+from sklearn.model_selection import GroupKFold
+from sklearn.model_selection import GroupShuffleSplit
 from sklearn.model_selection import learning_curve
 from sklearn.model_selection import validation_curve
 from sklearn.model_selection._validation import _check_is_permutation
@@ -181,22 +181,22 @@ def test_cross_val_score():
     assert_raises(ValueError, cross_val_score, clf, X_3d, y2)
 
 
-def test_cross_val_score_predict_labels():
-    # Check if ValueError (when labels is None) propagates to cross_val_score
+def test_cross_val_score_predict_groups():
+    # Check if ValueError (when groups is None) propagates to cross_val_score
     # and cross_val_predict
-    # And also check if labels is correctly passed to the cv object
+    # And also check if groups is correctly passed to the cv object
     X, y = make_classification(n_samples=20, n_classes=2, random_state=0)
 
     clf = SVC(kernel="linear")
 
-    label_cvs = [LeaveOneLabelOut(), LeavePLabelOut(2), LabelKFold(),
-                 LabelShuffleSplit()]
-    for cv in label_cvs:
+    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2), GroupKFold(),
+                 GroupShuffleSplit()]
+    for cv in group_cvs:
         assert_raise_message(ValueError,
-                             "The labels parameter should not be None",
+                             "The groups parameter should not be None",
                              cross_val_score, estimator=clf, X=X, y=y, cv=cv)
         assert_raise_message(ValueError,
-                             "The labels parameter should not be None",
+                             "The groups parameter should not be None",
                              cross_val_predict, estimator=clf, X=X, y=y, cv=cv)
 
 
@@ -348,9 +348,10 @@ def test_cross_val_score_with_score_func_regression():
     assert_array_almost_equal(r2_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
 
     # Mean squared error; this is a loss function, so "scores" are negative
-    mse_scores = cross_val_score(reg, X, y, cv=5, scoring="mean_squared_error")
-    expected_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
-    assert_array_almost_equal(mse_scores, expected_mse, 2)
+    neg_mse_scores = cross_val_score(reg, X, y, cv=5,
+                                     scoring="neg_mean_squared_error")
+    expected_neg_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
+    assert_array_almost_equal(neg_mse_scores, expected_neg_mse, 2)
 
     # Explained variance
     scoring = make_scorer(explained_variance_score)
@@ -371,21 +372,21 @@ def test_permutation_score():
     assert_greater(score, 0.9)
     assert_almost_equal(pvalue, 0.0, 1)
 
-    score_label, _, pvalue_label = permutation_test_score(
+    score_group, _, pvalue_group = permutation_test_score(
         svm, X, y, n_permutations=30, cv=cv, scoring="accuracy",
-        labels=np.ones(y.size), random_state=0)
-    assert_true(score_label == score)
-    assert_true(pvalue_label == pvalue)
+        groups=np.ones(y.size), random_state=0)
+    assert_true(score_group == score)
+    assert_true(pvalue_group == pvalue)
 
     # check that we obtain the same results with a sparse representation
     svm_sparse = SVC(kernel='linear')
     cv_sparse = StratifiedKFold(2)
-    score_label, _, pvalue_label = permutation_test_score(
+    score_group, _, pvalue_group = permutation_test_score(
         svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse,
-        scoring="accuracy", labels=np.ones(y.size), random_state=0)
+        scoring="accuracy", groups=np.ones(y.size), random_state=0)
 
-    assert_true(score_label == score)
-    assert_true(pvalue_label == pvalue)
+    assert_true(score_group == score)
+    assert_true(pvalue_group == pvalue)
 
     # test with custom scoring object
     def custom_score(y_true, y_pred):
@@ -482,7 +483,7 @@ def test_cross_val_predict():
     assert_equal(len(preds), len(y))
 
     class BadCV():
-        def split(self, X, y=None, labels=None):
+        def split(self, X, y=None, groups=None):
             for i in range(4):
                 yield np.array([0, 1, 2, 3]), np.array([4, 5, 6, 7, 8])
 
diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py
index dba71b00c5..e1838dea6c 100755
--- a/sklearn/neural_network/multilayer_perceptron.py
+++ b/sklearn/neural_network/multilayer_perceptron.py
@@ -13,7 +13,6 @@
 import warnings
 
 from ..base import BaseEstimator, ClassifierMixin, RegressorMixin
-from ._base import logistic, softmax
 from ._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS
 from ._stochastic_optimizers import SGDOptimizer, AdamOptimizer
 from ..model_selection import train_test_split
@@ -25,7 +24,7 @@
 from ..exceptions import ConvergenceWarning
 from ..utils.extmath import safe_sparse_dot
 from ..utils.validation import check_is_fitted
-from ..utils.multiclass import _check_partial_fit_first_call
+from ..utils.multiclass import _check_partial_fit_first_call, unique_labels
 from ..utils.multiclass import type_of_target
 
 
@@ -82,7 +81,7 @@ def _unpack(self, packed_parameters):
             start, end = self._intercept_indptr[i]
             self.intercepts_[i] = packed_parameters[start:end]
 
-    def _forward_pass(self, activations, with_output_activation=True):
+    def _forward_pass(self, activations):
         """Perform a forward pass on the network by computing the values
         of the neurons in the hidden layers and the output layer.
 
@@ -108,9 +107,8 @@ def _forward_pass(self, activations, with_output_activation=True):
                 activations[i + 1] = hidden_activation(activations[i + 1])
 
         # For the last layer
-        if with_output_activation:
-            output_activation = ACTIVATIONS[self.out_activation_]
-            activations[i + 1] = output_activation(activations[i + 1])
+        output_activation = ACTIVATIONS[self.out_activation_]
+        activations[i + 1] = output_activation(activations[i + 1])
 
         return activations
 
@@ -223,7 +221,10 @@ def _backprop(self, X, y, activations, deltas, coef_grads,
         activations = self._forward_pass(activations)
 
         # Get loss
-        loss = LOSS_FUNCTIONS[self.loss](y, activations[-1])
+        loss_func_name = self.loss
+        if loss_func_name == 'log_loss' and self.out_activation_ == 'logistic':
+            loss_func_name = 'binary_log_loss'
+        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])
         # Add L2 regularization term to loss
         values = np.sum(
             np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))
@@ -268,23 +269,19 @@ def _initialize(self, y, layer_units):
         if not isinstance(self, ClassifierMixin):
             self.out_activation_ = 'identity'
         # Output for multi class
-        elif self.label_binarizer_.y_type_ == 'multiclass':
+        elif self._label_binarizer.y_type_ == 'multiclass':
             self.out_activation_ = 'softmax'
         # Output for binary class and multi-label
         else:
             self.out_activation_ = 'logistic'
-            if self.loss == 'log_loss':
-                self.loss = 'binary_log_loss'
 
         # Initialize coefficient and intercept layers
         self.coefs_ = []
         self.intercepts_ = []
 
         for i in range(self.n_layers_ - 1):
-            rng = check_random_state(self.random_state)
             coef_init, intercept_init = self._init_coef(layer_units[i],
-                                                        layer_units[i + 1],
-                                                        rng)
+                                                        layer_units[i + 1])
             self.coefs_.append(coef_init)
             self.intercepts_.append(intercept_init)
 
@@ -297,7 +294,7 @@ def _initialize(self, y, layer_units):
             else:
                 self.best_loss_ = np.inf
 
-    def _init_coef(self, fan_in, fan_out, rng):
+    def _init_coef(self, fan_in, fan_out):
         if self.activation == 'logistic':
             # Use the initialization method recommended by
             # Glorot et al.
@@ -309,8 +306,10 @@ def _init_coef(self, fan_in, fan_out, rng):
             raise ValueError("Unknown activation function %s" %
                              self.activation)
 
-        coef_init = rng.uniform(-init_bound, init_bound, (fan_in, fan_out))
-        intercept_init = rng.uniform(-init_bound, init_bound, fan_out)
+        coef_init = self._random_state.uniform(-init_bound, init_bound,
+                                               (fan_in, fan_out))
+        intercept_init = self._random_state.uniform(-init_bound, init_bound,
+                                                    fan_out)
         return coef_init, intercept_init
 
     def _fit(self, X, y, incremental=False):
@@ -338,6 +337,9 @@ def _fit(self, X, y, incremental=False):
         layer_units = ([n_features] + hidden_layer_sizes +
                        [self.n_outputs_])
 
+        # check random state
+        self._random_state = check_random_state(self.random_state)
+
         if not hasattr(self, 'coefs_') or (not self.warm_start and not
                                            incremental):
             # First time training the model
@@ -420,9 +422,11 @@ def _validate_hyperparameters(self):
         if self.learning_rate not in ["constant", "invscaling", "adaptive"]:
             raise ValueError("learning rate %s is not supported. " %
                              self.learning_rate)
-        if self.algorithm not in _STOCHASTIC_ALGOS + ["l-bfgs"]:
-            raise ValueError("The algorithm %s is not supported. " %
-                             self.algorithm)
+        supported_algorithms = _STOCHASTIC_ALGOS + ["l-bfgs"]
+        if self.algorithm not in supported_algorithms:
+            raise ValueError("The algorithm %s is not supported. "
+                             " Expected one of: %s" %
+                             (self.algorithm, ", ".join(supported_algorithms)))
 
     def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
                    intercept_grads, layer_units):
@@ -466,7 +470,6 @@ def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
 
     def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
                         intercept_grads, layer_units, incremental):
-        rng = check_random_state(self.random_state)
 
         if not incremental or not hasattr(self, '_optimizer'):
             params = self.coefs_ + self.intercepts_
@@ -484,10 +487,10 @@ def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
         early_stopping = self.early_stopping and not incremental
         if early_stopping:
             X, X_val, y, y_val = train_test_split(
-                X, y, random_state=self.random_state,
+                X, y, random_state=self._random_state,
                 test_size=self.validation_fraction)
             if isinstance(self, ClassifierMixin):
-                y_val = self.label_binarizer_.inverse_transform(y_val)
+                y_val = self._label_binarizer.inverse_transform(y_val)
         else:
             X_val = None
             y_val = None
@@ -501,7 +504,7 @@ def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
 
         try:
             for it in range(self.max_iter):
-                X, y = shuffle(X, y, random_state=rng)
+                X, y = shuffle(X, y, random_state=self._random_state)
                 accumulated_loss = 0.0
                 for batch_slice in gen_batches(n_samples, batch_size):
                     activations[0] = X[batch_slice]
@@ -630,14 +633,14 @@ def partial_fit(self):
         """
         if self.algorithm not in _STOCHASTIC_ALGOS:
             raise AttributeError("partial_fit is only available for stochastic"
-                                 "optimization algorithms. %s is not"
+                                 " optimization algorithms. %s is not"
                                  " stochastic" % self.algorithm)
         return self._partial_fit
 
     def _partial_fit(self, X, y, classes=None):
         return self._fit(X, y, incremental=True)
 
-    def _decision_scores(self, X):
+    def _predict(self, X):
         """Predict using the trained model
 
         Parameters
@@ -668,7 +671,7 @@ def _decision_scores(self, X):
             activations.append(np.empty((X.shape[0],
                                          layer_units[i + 1])))
         # forward propagate
-        self._forward_pass(activations, with_output_activation=False)
+        self._forward_pass(activations)
         y_pred = activations[-1]
 
         return y_pred
@@ -818,9 +821,6 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):
     `loss_` : float
         The current loss computed with the loss function.
 
-    `label_binarizer_` : LabelBinarizer
-        A LabelBinarizer object trained on the training set.
-
     `coefs_` : list, length n_layers - 1
         The ith element in the list represents the weight matrix corresponding
         to layer i.
@@ -893,47 +893,25 @@ def __init__(self, hidden_layer_sizes=(100,), activation="relu",
                      validation_fraction=validation_fraction,
                      beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)
 
-        self.label_binarizer_ = LabelBinarizer()
-
     def _validate_input(self, X, y, incremental):
         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],
                          multi_output=True)
         if y.ndim == 2 and y.shape[1] == 1:
             y = column_or_1d(y, warn=True)
-            
+
         if not hasattr(self, 'classes_') or not incremental:
-            self.label_binarizer_.fit(y)
-            self.classes_ = self.label_binarizer_.classes_
+            self._label_binarizer = LabelBinarizer()
+            self._label_binarizer.fit(y)
+            self.classes_ = self._label_binarizer.classes_
         else:
-            classes = self.label_binarizer_.classes_
-            if not np.all(np.in1d(classes, self.classes_)):
+            classes = unique_labels(y)
+            if np.setdiff1d(classes, self.classes_, assume_unique=True):
                 raise ValueError("`y` has classes not in `self.classes_`."
                                  " `self.classes_` has %s. 'y' has %s." %
                                  (self.classes_, classes))
-        
-        y = self.label_binarizer_.transform(y)
-        return X, y
-
-    def decision_function(self, X):
-        """Decision function of the mlp model
 
-        Parameters
-        ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            The input data.
-
-        Returns
-        -------
-        y : array-like, shape (n_samples,) or (n_samples, n_classes)
-            The values of decision function for each class in the model.
-        """
-        check_is_fitted(self, "coefs_")
-        y_scores = self._decision_scores(X)
-
-        if self.n_outputs_ == 1:
-            return y_scores.ravel()
-        else:
-            return y_scores
+        y = self._label_binarizer.transform(y)
+        return X, y
 
     def predict(self, X):
         """Predict using the multi-layer perceptron classifier
@@ -949,10 +927,12 @@ def predict(self, X):
             The predicted classes.
         """
         check_is_fitted(self, "coefs_")
-        y_scores = self.decision_function(X)
-        y_scores = ACTIVATIONS[self.out_activation_](y_scores)
+        y_pred = self._predict(X)
+
+        if self.n_outputs_ == 1:
+            y_pred = y_pred.ravel()
 
-        return self.label_binarizer_.inverse_transform(y_scores)
+        return self._label_binarizer.inverse_transform(y_pred)
 
     @property
     def partial_fit(self):
@@ -980,17 +960,17 @@ def partial_fit(self):
         """
         if self.algorithm not in _STOCHASTIC_ALGOS:
             raise AttributeError("partial_fit is only available for stochastic"
-                                 "optimization algorithms. %s is not"
+                                 " optimization algorithms. %s is not"
                                  " stochastic" % self.algorithm)
         return self._partial_fit
 
     def _partial_fit(self, X, y, classes=None):
-        _check_partial_fit_first_call(self, classes)
-        if not hasattr(self.label_binarizer_, "classes_"):
+        if _check_partial_fit_first_call(self, classes):
+            self._label_binarizer = LabelBinarizer()
             if type_of_target(y).startswith('multilabel'):
-                self.label_binarizer_.fit(y)
+                self._label_binarizer.fit(y)
             else:
-                self.label_binarizer_.fit(classes)
+                self._label_binarizer.fit(classes)
 
         super(MLPClassifier, self)._partial_fit(X, y)
 
@@ -1028,13 +1008,16 @@ def predict_proba(self, X):
             The predicted probability of the sample for each class in the
             model, where classes are ordered as they are in `self.classes_`.
         """
-        y_scores = self.decision_function(X)
+        check_is_fitted(self, "coefs_")
+        y_pred = self._predict(X)
+
+        if self.n_outputs_ == 1:
+            y_pred = y_pred.ravel()
 
-        if y_scores.ndim == 1:
-            y_scores = logistic(y_scores)
-            return np.vstack([1 - y_scores, y_scores]).T
+        if y_pred.ndim == 1:
+            return np.vstack([1 - y_pred, y_pred]).T
         else:
-            return softmax(y_scores)
+            return y_pred
 
 
 class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):
@@ -1264,7 +1247,7 @@ def predict(self, X):
             The predicted values.
         """
         check_is_fitted(self, "coefs_")
-        y_pred = self._decision_scores(X)
+        y_pred = self._predict(X)
         if y_pred.shape[1] == 1:
             return y_pred.ravel()
         return y_pred
diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py
index 5713cb150d..3f17ff82e0 100755
--- a/sklearn/neural_network/tests/test_mlp.py
+++ b/sklearn/neural_network/tests/test_mlp.py
@@ -51,7 +51,7 @@
 
 
 def test_alpha():
-    # Test that larger alpha yields weights closer to zero"""
+    # Test that larger alpha yields weights closer to zero
     X = X_digits_binary[:100]
     y = y_digits_binary[:100]
 
@@ -71,7 +71,7 @@ def test_alpha():
 
 
 def test_fit():
-    # Test that the algorithm solution is equal to a worked out example."""
+    # Test that the algorithm solution is equal to a worked out example.
     X = np.array([[0.6, 0.8, 0.7]])
     y = np.array([0])
     mlp = MLPClassifier(algorithm='sgd', learning_rate_init=0.1, alpha=0.1,
@@ -80,7 +80,6 @@ def test_fit():
     # set weights
     mlp.coefs_ = [0] * 2
     mlp.intercepts_ = [0] * 2
-    mlp.classes_ = [0, 1]
     mlp.n_outputs_ = 1
     mlp.coefs_[0] = np.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]])
     mlp.coefs_[1] = np.array([[0.1], [0.2]])
@@ -89,8 +88,6 @@ def test_fit():
     mlp._coef_grads = [] * 2
     mlp._intercept_grads = [] * 2
 
-    mlp.label_binarizer_.y_type_ = 'binary'
-
     # Initialize parameters
     mlp.n_iter_ = 0
     mlp.learning_rate_ = 0.1
@@ -160,7 +157,8 @@ def test_fit():
     #            0.7 * -0.002244 + 0.09626) = 0.572
     #  o1 = h * W2 + b21 = 0.677 * 0.04706 +
     #             0.572 * 0.154089 + 0.9235 = 1.043
-    assert_almost_equal(mlp.decision_function(X), 1.043, decimal=3)
+    #  prob = sigmoid(o1) = 0.739
+    assert_almost_equal(mlp.predict_proba(X)[0, 1], 0.739, decimal=3)
 
 
 def test_gradient():
@@ -248,7 +246,7 @@ def test_lbfgs_classification():
 
 
 def test_lbfgs_regression():
-    # Test lbfgs on the boston dataset, a regression problems."""
+    # Test lbfgs on the boston dataset, a regression problems.
     X = Xboston
     y = yboston
     for activation in ACTIVATION_TYPES:
@@ -264,7 +262,7 @@ def test_lbfgs_regression():
 
 
 def test_learning_rate_warmstart():
-    # Test that warm_start reuses past solution."""
+    # Tests that warm_start reuse past solutions.
     X = [[3, 2], [1, 6], [5, 6], [-2, -4]]
     y = [1, 1, 1, 0]
     for learning_rate in ["invscaling", "constant"]:
@@ -285,7 +283,7 @@ def test_learning_rate_warmstart():
 
 
 def test_multilabel_classification():
-    # Test that multi-label classification works as expected."""
+    # Test that multi-label classification works as expected.
     # test fit method
     X, y = make_multilabel_classification(n_samples=50, random_state=0,
                                           return_indicator=True)
@@ -305,7 +303,7 @@ def test_multilabel_classification():
 
 
 def test_multioutput_regression():
-    # Test that multi-output regression works as expected"""
+    # Test that multi-output regression works as expected
     X, y = make_regression(n_samples=200, n_targets=5)
     mlp = MLPRegressor(algorithm='l-bfgs', hidden_layer_sizes=50, max_iter=200,
                        random_state=1)
@@ -314,7 +312,7 @@ def test_multioutput_regression():
 
 
 def test_partial_fit_classes_error():
-    # Tests that passing different classes to partial_fit raises an error"""
+    # Tests that passing different classes to partial_fit raises an error
     X = [[3, 2]]
     y = [0]
     clf = MLPClassifier(algorithm='sgd')
@@ -348,7 +346,7 @@ def test_partial_fit_unseen_classes():
     # Non regression test for bug 6994
     # Tests for labeling errors in partial fit
 
-    clf = MLPClassifier()
+    clf = MLPClassifier(random_state=0)
     clf.partial_fit([[1], [2], [3]], ["a", "b", "c"],
                     classes=["a", "b", "c", "d"])
     clf.partial_fit([[4]], ["d"])
@@ -382,7 +380,7 @@ def test_partial_fit_regression():
 
 
 def test_partial_fit_errors():
-    # Test partial_fit error handling."""
+    # Test partial_fit error handling.
     X = [[3, 2], [1, 6]]
     y = [1, 0]
 
@@ -398,7 +396,7 @@ def test_partial_fit_errors():
 
 
 def test_params_errors():
-    # Test that invalid parameters raise value error"""
+    # Test that invalid parameters raise value error
     X = [[3, 2], [1, 6]]
     y = [1, 0]
     clf = MLPClassifier
@@ -408,6 +406,17 @@ def test_params_errors():
     assert_raises(ValueError, clf(shuffle='true').fit, X, y)
     assert_raises(ValueError, clf(alpha=-1).fit, X, y)
     assert_raises(ValueError, clf(learning_rate_init=-1).fit, X, y)
+    assert_raises(ValueError, clf(momentum=2).fit, X, y)
+    assert_raises(ValueError, clf(momentum=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(nesterovs_momentum='invalid').fit, X, y)
+    assert_raises(ValueError, clf(early_stopping='invalid').fit, X, y)
+    assert_raises(ValueError, clf(validation_fraction=1).fit, X, y)
+    assert_raises(ValueError, clf(validation_fraction=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(beta_1=1).fit, X, y)
+    assert_raises(ValueError, clf(beta_1=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(beta_2=1).fit, X, y)
+    assert_raises(ValueError, clf(beta_2=-0.5).fit, X, y)
+    assert_raises(ValueError, clf(epsilon=-0.5).fit, X, y)
 
     assert_raises(ValueError, clf(algorithm='hadoken').fit, X, y)
     assert_raises(ValueError, clf(learning_rate='converge').fit, X, y)
@@ -415,7 +424,7 @@ def test_params_errors():
 
 
 def test_predict_proba_binary():
-    # Test that predict_proba works as expected for binary class."""
+    # Test that predict_proba works as expected for binary class.
     X = X_digits_binary[:50]
     y = y_digits_binary[:50]
 
@@ -437,8 +446,8 @@ def test_predict_proba_binary():
     assert_equal(roc_auc_score(y, y_proba[:, 1]), 1.0)
 
 
-def test_predict_proba_multi():
-    # Test that predict_proba works as expected for multi class."""
+def test_predict_proba_multiclass():
+    # Test that predict_proba works as expected for multi class.
     X = X_digits_multi[:10]
     y = y_digits_multi[:10]
 
@@ -458,17 +467,41 @@ def test_predict_proba_multi():
     assert_array_equal(y_log_proba, np.log(y_proba))
 
 
+def test_predict_proba_multilabel():
+    # Test that predict_proba works as expected for multilabel.
+    # Multilabel should not use softmax which makes probabilities sum to 1
+    X, Y = make_multilabel_classification(n_samples=50, random_state=0,
+                                          return_indicator=True)
+    n_samples, n_classes = Y.shape
+
+    clf = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=30,
+                        random_state=0)
+    clf.fit(X, Y)
+    y_proba = clf.predict_proba(X)
+
+    assert_equal(y_proba.shape, (n_samples, n_classes))
+    assert_array_equal(y_proba > 0.5, Y)
+
+    y_log_proba = clf.predict_log_proba(X)
+    proba_max = y_proba.argmax(axis=1)
+    proba_log_max = y_log_proba.argmax(axis=1)
+
+    assert_greater((y_proba.sum(1) - 1).dot(y_proba.sum(1) - 1), 1e-10)
+    assert_array_equal(proba_max, proba_log_max)
+    assert_array_equal(y_log_proba, np.log(y_proba))
+
+
 def test_sparse_matrices():
-    # Test that sparse and dense input matrices output the same results."""
+    # Test that sparse and dense input matrices output the same results.
     X = X_digits_binary[:50]
     y = y_digits_binary[:50]
     X_sparse = csr_matrix(X)
-    mlp = MLPClassifier(random_state=1, hidden_layer_sizes=15)
-    with ignore_warnings(category=ConvergenceWarning):
-        mlp.fit(X, y)
-        pred1 = mlp.decision_function(X)
-        mlp.fit(X_sparse, y)
-        pred2 = mlp.decision_function(X_sparse)
+    mlp = MLPClassifier(algorithm='l-bfgs', hidden_layer_sizes=15,
+                        random_state=1)
+    mlp.fit(X, y)
+    pred1 = mlp.predict(X)
+    mlp.fit(X_sparse, y)
+    pred2 = mlp.predict(X_sparse)
     assert_almost_equal(pred1, pred2)
     pred1 = mlp.predict(X)
     pred2 = mlp.predict(X_sparse)
diff --git a/sklearn/preprocessing/imputation.py b/sklearn/preprocessing/imputation.py
index ec401ee167..e414e98f42 100755
--- a/sklearn/preprocessing/imputation.py
+++ b/sklearn/preprocessing/imputation.py
@@ -309,11 +309,17 @@ def transform(self, X):
         """
         if self.axis == 0:
             check_is_fitted(self, 'statistics_')
+            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
+                            force_all_finite=False, copy=self.copy)
+            statistics = self.statistics_
+            if X.shape[1] != statistics.shape[0]:
+                raise ValueError("X has %d features per sample, expected %d"
+                                 % (X.shape[1], self.statistics_.shape[0]))
 
         # Since two different arrays can be provided in fit(X) and
         # transform(X), the imputation data need to be recomputed
         # when the imputation is done per sample
-        if self.axis == 1:
+        else:
             X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,
                             force_all_finite=False, copy=self.copy)
 
@@ -328,10 +334,6 @@ def transform(self, X):
                                              self.strategy,
                                              self.missing_values,
                                              self.axis)
-        else:
-            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,
-                            force_all_finite=False, copy=self.copy)
-            statistics = self.statistics_
 
         # Delete the invalid rows/columns
         invalid_mask = np.isnan(statistics)
diff --git a/sklearn/tests/test_base.py b/sklearn/tests/test_base.py
index ae6d9fdb13..1c5c9bb744 100755
--- a/sklearn/tests/test_base.py
+++ b/sklearn/tests/test_base.py
@@ -6,22 +6,29 @@
 import numpy as np
 import scipy.sparse as sp
 
+import sklearn
 from sklearn.utils.testing import assert_array_equal
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_equal
 from sklearn.utils.testing import assert_not_equal
 from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_no_warnings
 from sklearn.utils.testing import assert_warns_message
 
 from sklearn.base import BaseEstimator, clone, is_classifier
 from sklearn.svm import SVC
 from sklearn.pipeline import Pipeline
 from sklearn.model_selection import GridSearchCV
+
+from sklearn.tree import DecisionTreeClassifier
+from sklearn.tree import DecisionTreeRegressor
+from sklearn import datasets
 from sklearn.utils import deprecated
 
 from sklearn.base import TransformerMixin
 from sklearn.utils.mocking import MockDataFrame
+import pickle
 
 
 #############################################################################
@@ -235,8 +242,8 @@ def test_is_classifier():
     assert_true(is_classifier(svc))
     assert_true(is_classifier(GridSearchCV(svc, {'C': [0.1, 1]})))
     assert_true(is_classifier(Pipeline([('svc', svc)])))
-    assert_true(is_classifier(Pipeline([('svc_cv',
-                                         GridSearchCV(svc, {'C': [0.1, 1]}))])))
+    assert_true(is_classifier(Pipeline(
+        [('svc_cv', GridSearchCV(svc, {'C': [0.1, 1]}))])))
 
 
 def test_set_params():
@@ -253,9 +260,6 @@ def test_set_params():
 
 
 def test_score_sample_weight():
-    from sklearn.tree import DecisionTreeClassifier
-    from sklearn.tree import DecisionTreeRegressor
-    from sklearn import datasets
 
     rng = np.random.RandomState(0)
 
@@ -313,3 +317,45 @@ def transform(self, X, y=None):
     # the test
     assert_true((e.df == cloned_e.df).values.all())
     assert_equal(e.scalar_param, cloned_e.scalar_param)
+
+
+class TreeNoVersion(DecisionTreeClassifier):
+    def __getstate__(self):
+        return self.__dict__
+
+
+def test_pickle_version_warning():
+    # check that warnings are raised when unpickling in a different version
+
+    # first, check no warning when in the same version:
+    iris = datasets.load_iris()
+    tree = DecisionTreeClassifier().fit(iris.data, iris.target)
+    tree_pickle = pickle.dumps(tree)
+    assert_true(b"version" in tree_pickle)
+    assert_no_warnings(pickle.loads, tree_pickle)
+
+    # check that warning is raised on different version
+    tree_pickle_other = tree_pickle.replace(sklearn.__version__.encode(),
+                                            b"something")
+    message = ("Trying to unpickle estimator DecisionTreeClassifier from "
+               "version {0} when using version {1}. This might lead to "
+               "breaking code or invalid results. "
+               "Use at your own risk.".format("something",
+                                              sklearn.__version__))
+    assert_warns_message(UserWarning, message, pickle.loads, tree_pickle_other)
+
+    # check that not including any version also works:
+    # TreeNoVersion has no getstate, like pre-0.18
+    tree = TreeNoVersion().fit(iris.data, iris.target)
+
+    tree_pickle_noversion = pickle.dumps(tree)
+    assert_false(b"version" in tree_pickle_noversion)
+    message = message.replace("something", "pre-0.18")
+    message = message.replace("DecisionTreeClassifier", "TreeNoVersion")
+    # check we got the warning about using pre-0.18 pickle
+    assert_warns_message(UserWarning, message, pickle.loads,
+                         tree_pickle_noversion)
+
+    # check that no warning is raised for external estimators
+    TreeNoVersion.__module__ = "notsklearn"
+    assert_no_warnings(pickle.loads, tree_pickle_noversion)
diff --git a/sklearn/tests/test_cross_validation.py b/sklearn/tests/test_cross_validation.py
index 0e03cad783..4d756bdaa0 100755
--- a/sklearn/tests/test_cross_validation.py
+++ b/sklearn/tests/test_cross_validation.py
@@ -479,7 +479,7 @@ def test_stratified_shuffle_split_init():
 def test_stratified_shuffle_split_iter():
     ys = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),
           np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),
-          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),
+          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),
           np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),
           np.array([-1] * 800 + [1] * 50)
           ]
@@ -487,16 +487,22 @@ def test_stratified_shuffle_split_iter():
     for y in ys:
         sss = cval.StratifiedShuffleSplit(y, 6, test_size=0.33,
                                           random_state=0)
+        test_size = np.ceil(0.33 * len(y))
+        train_size = len(y) - test_size
         for train, test in sss:
             assert_array_equal(np.unique(y[train]), np.unique(y[test]))
             # Checks if folds keep classes proportions
-            p_train = (np.bincount(np.unique(y[train], return_inverse=True)[1])
-                       / float(len(y[train])))
-            p_test = (np.bincount(np.unique(y[test], return_inverse=True)[1])
-                      / float(len(y[test])))
+            p_train = (np.bincount(np.unique(y[train],
+                                   return_inverse=True)[1]) /
+                       float(len(y[train])))
+            p_test = (np.bincount(np.unique(y[test],
+                                  return_inverse=True)[1]) /
+                      float(len(y[test])))
             assert_array_almost_equal(p_train, p_test, 1)
-            assert_equal(y[train].size + y[test].size, y.size)
-            assert_array_equal(np.intersect1d(train, test), [])
+            assert_equal(len(train) + len(test), y.size)
+            assert_equal(len(train), train_size)
+            assert_equal(len(test), test_size)
+            assert_array_equal(np.lib.arraysetops.intersect1d(train, test), [])
 
 
 def test_stratified_shuffle_split_even():
@@ -906,10 +912,10 @@ def test_cross_val_score_with_score_func_regression():
     assert_array_almost_equal(r2_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
 
     # Mean squared error; this is a loss function, so "scores" are negative
-    mse_scores = cval.cross_val_score(reg, X, y, cv=5,
-                                      scoring="mean_squared_error")
-    expected_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
-    assert_array_almost_equal(mse_scores, expected_mse, 2)
+    neg_mse_scores = cval.cross_val_score(reg, X, y, cv=5,
+                                          scoring="neg_mean_squared_error")
+    expected_neg_mse = np.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
+    assert_array_almost_equal(neg_mse_scores, expected_neg_mse, 2)
 
     # Explained variance
     scoring = make_scorer(explained_variance_score)
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 5c031c881a..672886cca7 100755
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -835,6 +835,8 @@ def check_estimators_pickle(name, Estimator):
 
     # pickle and unpickle!
     pickled_estimator = pickle.dumps(estimator)
+    if Estimator.__module__.startswith('sklearn.'):
+        assert_true(b"version" in pickled_estimator)
     unpickled_estimator = pickle.loads(pickled_estimator)
 
     for method in result:
diff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py
index be349e1bc7..cd10e2c068 100755
--- a/sklearn/utils/extmath.py
+++ b/sklearn/utils/extmath.py
@@ -842,3 +842,23 @@ def _deterministic_vector_sign_flip(u):
     signs = np.sign(u[range(u.shape[0]), max_abs_rows])
     u *= signs[:, np.newaxis]
     return u
+
+
+def stable_cumsum(arr, rtol=1e-05, atol=1e-08):
+    """Use high precision for cumsum and check that final value matches sum
+
+    Parameters
+    ----------
+    arr : array-like
+        To be cumulatively summed as flat
+    rtol : float
+        Relative tolerance, see ``np.allclose``
+    atol : float
+        Absolute tolerance, see ``np.allclose``
+    """
+    out = np.cumsum(arr, dtype=np.float64)
+    expected = np.sum(arr, dtype=np.float64)
+    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):
+        raise RuntimeError('cumsum was found to be unstable: '
+                           'its last element does not correspond to sum')
+    return out
diff --git a/sklearn/utils/fixes.py b/sklearn/utils/fixes.py
index a10afa6d4d..79b12aae93 100755
--- a/sklearn/utils/fixes.py
+++ b/sklearn/utils/fixes.py
@@ -227,49 +227,6 @@ def combinations_with_replacement(iterable, r):
             yield tuple(pool[i] for i in indices)
 
 
-try:
-    from numpy import isclose
-except ImportError:
-    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
-        """
-        Returns a boolean array where two arrays are element-wise equal within
-        a tolerance.
-
-        This function was added to numpy v1.7.0, and the version you are
-        running has been backported from numpy v1.8.1. See its documentation
-        for more details.
-        """
-        def within_tol(x, y, atol, rtol):
-            with np.errstate(invalid='ignore'):
-                result = np.less_equal(abs(x - y), atol + rtol * abs(y))
-            if np.isscalar(a) and np.isscalar(b):
-                result = bool(result)
-            return result
-
-        x = np.array(a, copy=False, subok=True, ndmin=1)
-        y = np.array(b, copy=False, subok=True, ndmin=1)
-        xfin = np.isfinite(x)
-        yfin = np.isfinite(y)
-        if all(xfin) and all(yfin):
-            return within_tol(x, y, atol, rtol)
-        else:
-            finite = xfin & yfin
-            cond = np.zeros_like(finite, subok=True)
-            # Since we're using boolean indexing, x & y must be the same shape.
-            # Ideally, we'd just do x, y = broadcast_arrays(x, y). It's in
-            # lib.stride_tricks, though, so we can't import it here.
-            x = x * np.ones_like(cond)
-            y = y * np.ones_like(cond)
-            # Avoid subtraction with infinite/nan values...
-            cond[finite] = within_tol(x[finite], y[finite], atol, rtol)
-            # Check for equality of infinite values...
-            cond[~finite] = (x[~finite] == y[~finite])
-            if equal_nan:
-                # Make NaN == NaN
-                cond[np.isnan(x) & np.isnan(y)] = True
-            return cond
-
-
 if np_version < (1, 7):
     # Prior to 1.7.0, np.frombuffer wouldn't work for empty first arg.
     def frombuffer_empty(buf, dtype):
diff --git a/sklearn/utils/random.py b/sklearn/utils/random.py
index 34738d8653..5805f9be2c 100755
--- a/sklearn/utils/random.py
+++ b/sklearn/utils/random.py
@@ -123,7 +123,7 @@ def choice(a, size=None, replace=True, p=None, random_state=None):
         if pop_size is 0:
             raise ValueError("a must be non-empty")
 
-    if None != p:
+    if p is not None:
         p = np.array(p, dtype=np.double, ndmin=1, copy=False)
         if p.ndim != 1:
             raise ValueError("p must be 1-dimensional")
@@ -142,7 +142,7 @@ def choice(a, size=None, replace=True, p=None, random_state=None):
 
     # Actual sampling
     if replace:
-        if None != p:
+        if p is not None:
             cdf = p.cumsum()
             cdf /= cdf[-1]
             uniform_samples = random_state.random_sample(shape)
@@ -156,7 +156,7 @@ def choice(a, size=None, replace=True, p=None, random_state=None):
             raise ValueError("Cannot take a larger sample than "
                              "population when 'replace=False'")
 
-        if None != p:
+        if p is not None:
             if np.sum(p > 0) < size:
                 raise ValueError("Fewer non-zero entries in p than size")
             n_uniq = 0
diff --git a/sklearn/utils/tests/test_extmath.py b/sklearn/utils/tests/test_extmath.py
index 5847d0566a..55f96cdf15 100755
--- a/sklearn/utils/tests/test_extmath.py
+++ b/sklearn/utils/tests/test_extmath.py
@@ -17,7 +17,10 @@
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_raises
+from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import skip_if_32bit
+from sklearn.utils.testing import SkipTest
+from sklearn.utils.fixes import np_version
 
 from sklearn.utils.extmath import density
 from sklearn.utils.extmath import logsumexp
@@ -32,6 +35,7 @@
 from sklearn.utils.extmath import _incremental_mean_and_var
 from sklearn.utils.extmath import _deterministic_vector_sign_flip
 from sklearn.utils.extmath import softmax
+from sklearn.utils.extmath import stable_cumsum
 from sklearn.datasets.samples_generator import make_low_rank_matrix
 
 
@@ -643,3 +647,14 @@ def test_softmax():
     exp_X = np.exp(X)
     sum_exp_X = np.sum(exp_X, axis=1).reshape((-1, 1))
     assert_array_almost_equal(softmax(X), exp_X / sum_exp_X)
+
+
+def test_stable_cumsum():
+    if np_version < (1, 9):
+        raise SkipTest("Sum is as unstable as cumsum for numpy < 1.9")
+    assert_array_equal(stable_cumsum([1, 2, 3]), np.cumsum([1, 2, 3]))
+    r = np.random.RandomState(0).rand(100000)
+    assert_raise_message(RuntimeError,
+                         'cumsum was found to be unstable: its last element '
+                         'does not correspond to sum',
+                         stable_cumsum, r, rtol=0, atol=0)
